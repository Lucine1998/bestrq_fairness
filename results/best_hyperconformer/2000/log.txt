2023-12-01 09:54:30,895 - speechbrain.core - INFO - Beginning experiment!
2023-12-01 09:54:30,895 - speechbrain.core - INFO - Experiment folder: results/best_hyperconformer/2000
2023-12-01 09:54:33,943 - speechbrain.utils.superpowers - DEBUG - appdirs==1.4.4
attrs==23.1.0
black==19.10b0
certifi==2023.11.17
cfgv==3.4.0
charset-normalizer==3.3.2
click==8.0.4
cmake==3.27.7
distlib==0.3.7
entrypoints==0.3
filelock==3.13.1
flake8==3.7.9
fsspec==2023.10.0
huggingface-hub==0.19.4
HyperPyYAML==1.2.2
identify==2.5.32
idna==3.6
iniconfig==2.0.0
Jinja2==3.1.2
joblib==1.3.2
lit==17.0.6
MarkupSafe==2.1.3
mccabe==0.6.1
mpmath==1.3.0
networkx==3.2.1
nodeenv==1.8.0
numpy==1.26.2
nvidia-cublas-cu11==11.10.3.66
nvidia-cublas-cu12==12.1.3.1
nvidia-cuda-cupti-cu11==11.7.101
nvidia-cuda-cupti-cu12==12.1.105
nvidia-cuda-nvrtc-cu11==11.7.99
nvidia-cuda-nvrtc-cu12==12.1.105
nvidia-cuda-runtime-cu11==11.7.99
nvidia-cuda-runtime-cu12==12.1.105
nvidia-cudnn-cu11==8.5.0.96
nvidia-cudnn-cu12==8.9.2.26
nvidia-cufft-cu11==10.9.0.58
nvidia-cufft-cu12==11.0.2.54
nvidia-curand-cu11==10.2.10.91
nvidia-curand-cu12==10.3.2.106
nvidia-cusolver-cu11==11.4.0.1
nvidia-cusolver-cu12==11.4.5.107
nvidia-cusparse-cu11==11.7.4.91
nvidia-cusparse-cu12==12.1.0.106
nvidia-nccl-cu11==2.14.3
nvidia-nccl-cu12==2.18.1
nvidia-nvjitlink-cu12==12.3.101
nvidia-nvtx-cu11==11.7.91
nvidia-nvtx-cu12==12.1.105
packaging==23.2
pandas==2.1.3
pathspec==0.11.2
platformdirs==4.0.0
pluggy==1.3.0
pre-commit==3.5.0
pycodestyle==2.5.0
pyflakes==2.1.1
pytest==7.4.0
python-dateutil==2.8.2
pytz==2023.3.post1
PyYAML==6.0.1
regex==2023.10.3
requests==2.31.0
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
safetensors==0.4.1
scipy==1.11.4
sentencepiece==0.1.99
six==1.16.0
-e /gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain
sympy==1.12
tokenizers==0.15.0
toml==0.10.2
torch==2.0.1
torchaudio==2.0.2
tqdm==4.66.1
transformers==4.35.2
triton==2.0.0
typed-ast==1.5.5
typing_extensions==4.8.0
tzdata==2023.3
urllib3==2.1.0
virtualenv==20.24.7
yamllint==1.23.0

ERROR: Error [Errno 2] No such file or directory: 'git' while executing command git config --get-regexp 'remote\..*\.url'
WARNING: cannot determine version of editable source in /gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain (git command not found in path)

2023-12-01 09:54:38,505 - speechbrain.dataio.sampler - INFO - Batch quantisation in latent space
2023-12-01 09:54:38,506 - speechbrain.dataio.sampler - DEBUG - Latent bucket boundary - buckets: ['1.62', '2.19', '2.66', '3.09', '3.49', '3.88', '4.27', '4.65', '5.02', '5.41', '5.79', '6.18', '6.58', '6.99', '7.40', '7.83', '8.27', '8.72', '9.19', '9.68', '10.18', '10.70', '11.25', '11.82', '12.41', '13.04', '13.70', '14.39', '15.12', '15.90', '16.73', '17.61', '18.55', '19.57', '20.67', '21.86', '23.16', '24.59', '26.17', '27.94', '29.93', '32.21', '34.84', '37.94', '41.68', '46.34', '52.40', '60.82', '73.93', '100.00'] - length multipliers: ['1.35', '1.22', '1.16', '1.13', '1.11', '1.10', '1.09', '1.08', '1.08', '1.07', '1.07', '1.06', '1.06', '1.06', '1.06', '1.06', '1.05', '1.05', '1.05', '1.05', '1.05', '1.05', '1.05', '1.05', '1.05', '1.05', '1.05', '1.05', '1.05', '1.05', '1.05', '1.05', '1.05', '1.06', '1.06', '1.06', '1.06', '1.06', '1.07', '1.07', '1.08', '1.08', '1.09', '1.10', '1.11', '1.13', '1.16', '1.22', '1.35']
2023-12-01 09:54:38,506 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 09:54:39,193 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 0 with boundary 0.0-1.6 and batch_size 61: Num Examples 0.0, Num Full Batches 0.000, Pad Factor 0.000.
2023-12-01 09:54:39,194 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 1 with boundary 1.6-2.2 and batch_size 45: Num Examples 906.0, Num Full Batches 19.000, Pad Factor 8.810.
2023-12-01 09:54:39,194 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 2 with boundary 2.2-2.7 and batch_size 37: Num Examples 3456.0, Num Full Batches 84.000, Pad Factor 19.315.
2023-12-01 09:54:39,196 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 3 with boundary 2.7-3.1 and batch_size 32: Num Examples 3364.0, Num Full Batches 96.000, Pad Factor 14.604.
2023-12-01 09:54:39,196 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 4 with boundary 3.1-3.5 and batch_size 28: Num Examples 3292.0, Num Full Batches 108.000, Pad Factor 12.168.
2023-12-01 09:54:39,196 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 5 with boundary 3.5-3.9 and batch_size 25: Num Examples 3156.0, Num Full Batches 116.000, Pad Factor 10.443.
2023-12-01 09:54:39,196 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 6 with boundary 3.9-4.3 and batch_size 23: Num Examples 3158.0, Num Full Batches 128.000, Pad Factor 9.328.
2023-12-01 09:54:39,196 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 7 with boundary 4.3-4.6 and batch_size 21: Num Examples 3035.0, Num Full Batches 135.000, Pad Factor 8.411.
2023-12-01 09:54:39,196 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 8 with boundary 4.6-5.0 and batch_size 19: Num Examples 3010.0, Num Full Batches 145.000, Pad Factor 7.650.
2023-12-01 09:54:39,196 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 9 with boundary 5.0-5.4 and batch_size 18: Num Examples 3119.0, Num Full Batches 162.000, Pad Factor 7.284.
2023-12-01 09:54:39,196 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 10 with boundary 5.4-5.8 and batch_size 17: Num Examples 3126.0, Num Full Batches 175.000, Pad Factor 6.786.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 11 with boundary 5.8-6.2 and batch_size 16: Num Examples 3051.0, Num Full Batches 182.000, Pad Factor 6.432.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 12 with boundary 6.2-6.6 and batch_size 15: Num Examples 3091.0, Num Full Batches 197.000, Pad Factor 6.192.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 13 with boundary 6.6-7.0 and batch_size 14: Num Examples 3048.0, Num Full Batches 206.000, Pad Factor 5.893.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 14 with boundary 7.0-7.4 and batch_size 13: Num Examples 3288.0, Num Full Batches 236.000, Pad Factor 5.700.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 15 with boundary 7.4-7.8 and batch_size 12: Num Examples 3300.0, Num Full Batches 251.000, Pad Factor 5.514.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 16 with boundary 7.8-8.3 and batch_size 12: Num Examples 3438.0, Num Full Batches 276.000, Pad Factor 5.406.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 17 with boundary 8.3-8.7 and batch_size 11: Num Examples 3599.0, Num Full Batches 305.000, Pad Factor 5.299.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 18 with boundary 8.7-9.2 and batch_size 10: Num Examples 3901.0, Num Full Batches 349.000, Pad Factor 5.189.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 19 with boundary 9.2-9.7 and batch_size 10: Num Examples 4113.0, Num Full Batches 388.000, Pad Factor 5.087.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 20 with boundary 9.7-10.2 and batch_size 9: Num Examples 4692.0, Num Full Batches 465.000, Pad Factor 4.987.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 21 with boundary 10.2-10.7 and batch_size 9: Num Examples 5324.0, Num Full Batches 556.000, Pad Factor 4.978.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 22 with boundary 10.7-11.2 and batch_size 8: Num Examples 6379.0, Num Full Batches 700.000, Pad Factor 4.917.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 23 with boundary 11.2-11.8 and batch_size 8: Num Examples 8176.0, Num Full Batches 943.000, Pad Factor 4.894.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 24 with boundary 11.8-12.4 and batch_size 8: Num Examples 11428.0, Num Full Batches 1386.000, Pad Factor 4.865.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 25 with boundary 12.4-13.0 and batch_size 7: Num Examples 16056.0, Num Full Batches 2045.000, Pad Factor 4.866.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 26 with boundary 13.0-13.7 and batch_size 7: Num Examples 23408.0, Num Full Batches 3133.000, Pad Factor 4.893.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 27 with boundary 13.7-14.4 and batch_size 6: Num Examples 33287.0, Num Full Batches 4679.000, Pad Factor 4.874.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 28 with boundary 14.4-15.1 and batch_size 6: Num Examples 43150.0, Num Full Batches 6370.000, Pad Factor 4.946.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 29 with boundary 15.1-15.9 and batch_size 6: Num Examples 46934.0, Num Full Batches 7275.000, Pad Factor 4.968.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 30 with boundary 15.9-16.7 and batch_size 5: Num Examples 18846.0, Num Full Batches 3051.000, Pad Factor 5.095.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 31 with boundary 16.7-17.6 and batch_size 5: Num Examples 2283.0, Num Full Batches 385.000, Pad Factor 5.001.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 32 with boundary 17.6-18.6 and batch_size 5: Num Examples 24.0, Num Full Batches 4.000, Pad Factor 4.747.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 33 with boundary 18.6-19.6 and batch_size 5: Num Examples 23.0, Num Full Batches 4.000, Pad Factor 4.870.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 34 with boundary 19.6-20.7 and batch_size 4: Num Examples 26.0, Num Full Batches 5.000, Pad Factor 5.180.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 35 with boundary 20.7-21.9 and batch_size 4: Num Examples 15.0, Num Full Batches 3.000, Pad Factor 4.735.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 36 with boundary 21.9-23.2 and batch_size 4: Num Examples 11.0, Num Full Batches 2.000, Pad Factor 4.155.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 37 with boundary 23.2-24.6 and batch_size 4: Num Examples 7.0, Num Full Batches 1.000, Pad Factor 4.739.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 38 with boundary 24.6-26.2 and batch_size 3: Num Examples 6.0, Num Full Batches 1.000, Pad Factor 4.438.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 39 with boundary 26.2-27.9 and batch_size 3: Num Examples 4.0, Num Full Batches 1.000, Pad Factor 6.346.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 40 with boundary 27.9-29.9 and batch_size 3: Num Examples 1.0, Num Full Batches 0.000, Pad Factor 0.000.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 41 with boundary 29.9-32.2 and batch_size 3: Num Examples 0.0, Num Full Batches 0.000, Pad Factor 0.000.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 42 with boundary 32.2-34.8 and batch_size 2: Num Examples 0.0, Num Full Batches 0.000, Pad Factor 0.000.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 43 with boundary 34.8-37.9 and batch_size 2: Num Examples 0.0, Num Full Batches 0.000, Pad Factor 0.000.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 44 with boundary 37.9-41.7 and batch_size 2: Num Examples 0.0, Num Full Batches 0.000, Pad Factor 0.000.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 45 with boundary 41.7-46.3 and batch_size 2: Num Examples 0.0, Num Full Batches 0.000, Pad Factor 0.000.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 46 with boundary 46.3-52.4 and batch_size 1: Num Examples 0.0, Num Full Batches 0.000, Pad Factor 0.000.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 47 with boundary 52.4-60.8 and batch_size 1: Num Examples 0.0, Num Full Batches 0.000, Pad Factor 0.000.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 48 with boundary 60.8-73.9 and batch_size 1: Num Examples 0.0, Num Full Batches 0.000, Pad Factor 0.000.
2023-12-01 09:54:39,197 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 49 with boundary 73.9-100.0 and batch_size 1: Num Examples 0.0, Num Full Batches 0.000, Pad Factor 0.000.
2023-12-01 09:54:39,199 - speechbrain.core - INFO - Info: test_only arg overridden by command line input to: False
2023-12-01 09:54:39,199 - speechbrain.core - INFO - Info: debug arg overridden by command line input to: False
2023-12-01 09:54:39,199 - speechbrain.core - INFO - Info: debug_batches arg overridden by command line input to: 2
2023-12-01 09:54:39,199 - speechbrain.core - INFO - Info: debug_epochs arg overridden by command line input to: 2
2023-12-01 09:54:39,199 - speechbrain.core - INFO - Info: debug_persistently arg overridden by command line input to: False
2023-12-01 09:54:39,199 - speechbrain.core - INFO - Info: device arg overridden by command line input to: cuda:0
2023-12-01 09:54:39,199 - speechbrain.core - INFO - Info: data_parallel_backend arg overridden by command line input to: False
2023-12-01 09:54:39,199 - speechbrain.core - INFO - Info: distributed_backend arg overridden by command line input to: nccl
2023-12-01 09:54:39,199 - speechbrain.core - INFO - Info: find_unused_parameters arg overridden by command line input to: True
2023-12-01 09:54:39,199 - speechbrain.core - INFO - Info: jit arg overridden by command line input to: False
2023-12-01 09:54:39,199 - speechbrain.core - INFO - Info: compile arg overridden by command line input to: False
2023-12-01 09:54:39,200 - speechbrain.core - INFO - Info: auto_mix_prec arg from hparam file is used
2023-12-01 09:54:39,200 - speechbrain.core - INFO - Info: bfloat16_mix_prec arg from hparam file is used
2023-12-01 09:54:39,200 - speechbrain.core - INFO - Info: max_grad_norm arg from hparam file is used
2023-12-01 09:54:39,200 - speechbrain.core - INFO - Info: optimizer_step_limit arg from hparam file is used
2023-12-01 09:54:39,200 - speechbrain.core - INFO - Info: tqdm_colored_bar arg overridden by command line input to: False
2023-12-01 09:54:39,200 - speechbrain.core - INFO - Info: remove_vector_weight_decay arg overridden by command line input to: False
2023-12-01 09:54:39,327 - speechbrain.core - INFO - 71.3M trainable parameters in BestRQBrain
2023-12-01 09:54:39,391 - speechbrain.utils.checkpoints - INFO - Would load a checkpoint here, but none found yet.
2023-12-01 09:54:39,391 - speechbrain.utils.epoch_loop - INFO - Going into epoch 1
2023-12-01 09:54:39,393 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 09:54:40,205 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 09:56:25,894 - speechbrain.utils.train_logger - INFO - steps: 500, lr: 1.60e-05, avg_loss: 7.55
2023-12-01 09:56:36,504 - speechbrain.core - INFO - Beginning experiment!
2023-12-01 09:56:36,505 - speechbrain.core - INFO - Experiment folder: results/best_hyperconformer/2000
2023-12-01 09:56:38,014 - speechbrain.utils.superpowers - DEBUG - appdirs==1.4.4
attrs==23.1.0
black==19.10b0
certifi==2023.11.17
cfgv==3.4.0
charset-normalizer==3.3.2
click==8.0.4
cmake==3.27.7
distlib==0.3.7
entrypoints==0.3
filelock==3.13.1
flake8==3.7.9
fsspec==2023.10.0
huggingface-hub==0.19.4
HyperPyYAML==1.2.2
identify==2.5.32
idna==3.6
iniconfig==2.0.0
Jinja2==3.1.2
joblib==1.3.2
lit==17.0.6
MarkupSafe==2.1.3
mccabe==0.6.1
mpmath==1.3.0
networkx==3.2.1
nodeenv==1.8.0
numpy==1.26.2
nvidia-cublas-cu11==11.10.3.66
nvidia-cublas-cu12==12.1.3.1
nvidia-cuda-cupti-cu11==11.7.101
nvidia-cuda-cupti-cu12==12.1.105
nvidia-cuda-nvrtc-cu11==11.7.99
nvidia-cuda-nvrtc-cu12==12.1.105
nvidia-cuda-runtime-cu11==11.7.99
nvidia-cuda-runtime-cu12==12.1.105
nvidia-cudnn-cu11==8.5.0.96
nvidia-cudnn-cu12==8.9.2.26
nvidia-cufft-cu11==10.9.0.58
nvidia-cufft-cu12==11.0.2.54
nvidia-curand-cu11==10.2.10.91
nvidia-curand-cu12==10.3.2.106
nvidia-cusolver-cu11==11.4.0.1
nvidia-cusolver-cu12==11.4.5.107
nvidia-cusparse-cu11==11.7.4.91
nvidia-cusparse-cu12==12.1.0.106
nvidia-nccl-cu11==2.14.3
nvidia-nccl-cu12==2.18.1
nvidia-nvjitlink-cu12==12.3.101
nvidia-nvtx-cu11==11.7.91
nvidia-nvtx-cu12==12.1.105
packaging==23.2
pandas==2.1.3
pathspec==0.11.2
platformdirs==4.0.0
pluggy==1.3.0
pre-commit==3.5.0
pycodestyle==2.5.0
pyflakes==2.1.1
pytest==7.4.0
python-dateutil==2.8.2
pytz==2023.3.post1
PyYAML==6.0.1
regex==2023.10.3
requests==2.31.0
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
safetensors==0.4.1
scipy==1.11.4
sentencepiece==0.1.99
six==1.16.0
-e /gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain
sympy==1.12
tokenizers==0.15.0
toml==0.10.2
torch==2.0.1
torchaudio==2.0.2
tqdm==4.66.1
transformers==4.35.2
triton==2.0.0
typed-ast==1.5.5
typing_extensions==4.8.0
tzdata==2023.3
urllib3==2.1.0
virtualenv==20.24.7
yamllint==1.23.0

ERROR: Error [Errno 2] No such file or directory: 'git' while executing command git config --get-regexp 'remote\..*\.url'
WARNING: cannot determine version of editable source in /gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain (git command not found in path)

2023-12-01 09:56:42,353 - speechbrain.dataio.sampler - INFO - Batch quantisation in latent space
2023-12-01 09:56:42,354 - speechbrain.dataio.sampler - DEBUG - Latent bucket boundary - buckets: ['1.62', '2.19', '2.66', '3.09', '3.49', '3.88', '4.27', '4.65', '5.02', '5.41', '5.79', '6.18', '6.58', '6.99', '7.40', '7.83', '8.27', '8.72', '9.19', '9.68', '10.18', '10.70', '11.25', '11.82', '12.41', '13.04', '13.70', '14.39', '15.12', '15.90', '16.73', '17.61', '18.55', '19.57', '20.67', '21.86', '23.16', '24.59', '26.17', '27.94', '29.93', '32.21', '34.84', '37.94', '41.68', '46.34', '52.40', '60.82', '73.93', '100.00'] - length multipliers: ['1.35', '1.22', '1.16', '1.13', '1.11', '1.10', '1.09', '1.08', '1.08', '1.07', '1.07', '1.06', '1.06', '1.06', '1.06', '1.06', '1.05', '1.05', '1.05', '1.05', '1.05', '1.05', '1.05', '1.05', '1.05', '1.05', '1.05', '1.05', '1.05', '1.05', '1.05', '1.05', '1.05', '1.06', '1.06', '1.06', '1.06', '1.06', '1.07', '1.07', '1.08', '1.08', '1.09', '1.10', '1.11', '1.13', '1.16', '1.22', '1.35']
2023-12-01 09:56:42,354 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 09:56:43,036 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 0 with boundary 0.0-1.6 and batch_size 61: Num Examples 0.0, Num Full Batches 0.000, Pad Factor 0.000.
2023-12-01 09:56:43,036 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 1 with boundary 1.6-2.2 and batch_size 45: Num Examples 906.0, Num Full Batches 19.000, Pad Factor 8.810.
2023-12-01 09:56:43,036 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 2 with boundary 2.2-2.7 and batch_size 37: Num Examples 3456.0, Num Full Batches 84.000, Pad Factor 19.315.
2023-12-01 09:56:43,036 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 3 with boundary 2.7-3.1 and batch_size 32: Num Examples 3364.0, Num Full Batches 96.000, Pad Factor 14.604.
2023-12-01 09:56:43,036 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 4 with boundary 3.1-3.5 and batch_size 28: Num Examples 3292.0, Num Full Batches 108.000, Pad Factor 12.168.
2023-12-01 09:56:43,036 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 5 with boundary 3.5-3.9 and batch_size 25: Num Examples 3156.0, Num Full Batches 116.000, Pad Factor 10.443.
2023-12-01 09:56:43,037 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 6 with boundary 3.9-4.3 and batch_size 23: Num Examples 3158.0, Num Full Batches 128.000, Pad Factor 9.328.
2023-12-01 09:56:43,037 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 7 with boundary 4.3-4.6 and batch_size 21: Num Examples 3035.0, Num Full Batches 135.000, Pad Factor 8.411.
2023-12-01 09:56:43,037 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 8 with boundary 4.6-5.0 and batch_size 19: Num Examples 3010.0, Num Full Batches 145.000, Pad Factor 7.650.
2023-12-01 09:56:43,037 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 9 with boundary 5.0-5.4 and batch_size 18: Num Examples 3119.0, Num Full Batches 162.000, Pad Factor 7.284.
2023-12-01 09:56:43,037 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 10 with boundary 5.4-5.8 and batch_size 17: Num Examples 3126.0, Num Full Batches 175.000, Pad Factor 6.786.
2023-12-01 09:56:43,037 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 11 with boundary 5.8-6.2 and batch_size 16: Num Examples 3051.0, Num Full Batches 182.000, Pad Factor 6.432.
2023-12-01 09:56:43,037 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 12 with boundary 6.2-6.6 and batch_size 15: Num Examples 3091.0, Num Full Batches 197.000, Pad Factor 6.192.
2023-12-01 09:56:43,037 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 13 with boundary 6.6-7.0 and batch_size 14: Num Examples 3048.0, Num Full Batches 206.000, Pad Factor 5.893.
2023-12-01 09:56:43,037 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 14 with boundary 7.0-7.4 and batch_size 13: Num Examples 3288.0, Num Full Batches 236.000, Pad Factor 5.700.
2023-12-01 09:56:43,037 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 15 with boundary 7.4-7.8 and batch_size 12: Num Examples 3300.0, Num Full Batches 251.000, Pad Factor 5.514.
2023-12-01 09:56:43,037 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 16 with boundary 7.8-8.3 and batch_size 12: Num Examples 3438.0, Num Full Batches 276.000, Pad Factor 5.406.
2023-12-01 09:56:43,037 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 17 with boundary 8.3-8.7 and batch_size 11: Num Examples 3599.0, Num Full Batches 305.000, Pad Factor 5.299.
2023-12-01 09:56:43,037 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 18 with boundary 8.7-9.2 and batch_size 10: Num Examples 3901.0, Num Full Batches 349.000, Pad Factor 5.189.
2023-12-01 09:56:43,038 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 19 with boundary 9.2-9.7 and batch_size 10: Num Examples 4113.0, Num Full Batches 388.000, Pad Factor 5.087.
2023-12-01 09:56:43,038 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 20 with boundary 9.7-10.2 and batch_size 9: Num Examples 4692.0, Num Full Batches 465.000, Pad Factor 4.987.
2023-12-01 09:56:43,038 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 21 with boundary 10.2-10.7 and batch_size 9: Num Examples 5324.0, Num Full Batches 556.000, Pad Factor 4.978.
2023-12-01 09:56:43,038 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 22 with boundary 10.7-11.2 and batch_size 8: Num Examples 6379.0, Num Full Batches 700.000, Pad Factor 4.917.
2023-12-01 09:56:43,038 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 23 with boundary 11.2-11.8 and batch_size 8: Num Examples 8176.0, Num Full Batches 943.000, Pad Factor 4.894.
2023-12-01 09:56:43,038 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 24 with boundary 11.8-12.4 and batch_size 8: Num Examples 11428.0, Num Full Batches 1386.000, Pad Factor 4.865.
2023-12-01 09:56:43,038 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 25 with boundary 12.4-13.0 and batch_size 7: Num Examples 16056.0, Num Full Batches 2045.000, Pad Factor 4.866.
2023-12-01 09:56:43,038 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 26 with boundary 13.0-13.7 and batch_size 7: Num Examples 23408.0, Num Full Batches 3133.000, Pad Factor 4.893.
2023-12-01 09:56:43,038 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 27 with boundary 13.7-14.4 and batch_size 6: Num Examples 33287.0, Num Full Batches 4679.000, Pad Factor 4.874.
2023-12-01 09:56:43,038 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 28 with boundary 14.4-15.1 and batch_size 6: Num Examples 43150.0, Num Full Batches 6370.000, Pad Factor 4.946.
2023-12-01 09:56:43,038 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 29 with boundary 15.1-15.9 and batch_size 6: Num Examples 46934.0, Num Full Batches 7275.000, Pad Factor 4.968.
2023-12-01 09:56:43,038 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 30 with boundary 15.9-16.7 and batch_size 5: Num Examples 18846.0, Num Full Batches 3051.000, Pad Factor 5.095.
2023-12-01 09:56:43,038 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 31 with boundary 16.7-17.6 and batch_size 5: Num Examples 2283.0, Num Full Batches 385.000, Pad Factor 5.001.
2023-12-01 09:56:43,038 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 32 with boundary 17.6-18.6 and batch_size 5: Num Examples 24.0, Num Full Batches 4.000, Pad Factor 4.747.
2023-12-01 09:56:43,038 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 33 with boundary 18.6-19.6 and batch_size 5: Num Examples 23.0, Num Full Batches 4.000, Pad Factor 4.870.
2023-12-01 09:56:43,038 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 34 with boundary 19.6-20.7 and batch_size 4: Num Examples 26.0, Num Full Batches 5.000, Pad Factor 5.180.
2023-12-01 09:56:43,038 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 35 with boundary 20.7-21.9 and batch_size 4: Num Examples 15.0, Num Full Batches 3.000, Pad Factor 4.735.
2023-12-01 09:56:43,038 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 36 with boundary 21.9-23.2 and batch_size 4: Num Examples 11.0, Num Full Batches 2.000, Pad Factor 4.155.
2023-12-01 09:56:43,038 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 37 with boundary 23.2-24.6 and batch_size 4: Num Examples 7.0, Num Full Batches 1.000, Pad Factor 4.739.
2023-12-01 09:56:43,038 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 38 with boundary 24.6-26.2 and batch_size 3: Num Examples 6.0, Num Full Batches 1.000, Pad Factor 4.438.
2023-12-01 09:56:43,038 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 39 with boundary 26.2-27.9 and batch_size 3: Num Examples 4.0, Num Full Batches 1.000, Pad Factor 6.346.
2023-12-01 09:56:43,038 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 40 with boundary 27.9-29.9 and batch_size 3: Num Examples 1.0, Num Full Batches 0.000, Pad Factor 0.000.
2023-12-01 09:56:43,038 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 41 with boundary 29.9-32.2 and batch_size 3: Num Examples 0.0, Num Full Batches 0.000, Pad Factor 0.000.
2023-12-01 09:56:43,038 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 42 with boundary 32.2-34.8 and batch_size 2: Num Examples 0.0, Num Full Batches 0.000, Pad Factor 0.000.
2023-12-01 09:56:43,038 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 43 with boundary 34.8-37.9 and batch_size 2: Num Examples 0.0, Num Full Batches 0.000, Pad Factor 0.000.
2023-12-01 09:56:43,038 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 44 with boundary 37.9-41.7 and batch_size 2: Num Examples 0.0, Num Full Batches 0.000, Pad Factor 0.000.
2023-12-01 09:56:43,038 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 45 with boundary 41.7-46.3 and batch_size 2: Num Examples 0.0, Num Full Batches 0.000, Pad Factor 0.000.
2023-12-01 09:56:43,038 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 46 with boundary 46.3-52.4 and batch_size 1: Num Examples 0.0, Num Full Batches 0.000, Pad Factor 0.000.
2023-12-01 09:56:43,039 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 47 with boundary 52.4-60.8 and batch_size 1: Num Examples 0.0, Num Full Batches 0.000, Pad Factor 0.000.
2023-12-01 09:56:43,039 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 48 with boundary 60.8-73.9 and batch_size 1: Num Examples 0.0, Num Full Batches 0.000, Pad Factor 0.000.
2023-12-01 09:56:43,039 - speechbrain.dataio.sampler - DEBUG - DynamicBatchSampler: Bucket 49 with boundary 73.9-100.0 and batch_size 1: Num Examples 0.0, Num Full Batches 0.000, Pad Factor 0.000.
2023-12-01 09:56:43,041 - speechbrain.core - INFO - Info: test_only arg overridden by command line input to: False
2023-12-01 09:56:43,041 - speechbrain.core - INFO - Info: debug arg overridden by command line input to: False
2023-12-01 09:56:43,041 - speechbrain.core - INFO - Info: debug_batches arg overridden by command line input to: 2
2023-12-01 09:56:43,041 - speechbrain.core - INFO - Info: debug_epochs arg overridden by command line input to: 2
2023-12-01 09:56:43,041 - speechbrain.core - INFO - Info: debug_persistently arg overridden by command line input to: False
2023-12-01 09:56:43,041 - speechbrain.core - INFO - Info: device arg overridden by command line input to: cuda:0
2023-12-01 09:56:43,041 - speechbrain.core - INFO - Info: data_parallel_backend arg overridden by command line input to: False
2023-12-01 09:56:43,041 - speechbrain.core - INFO - Info: distributed_backend arg overridden by command line input to: nccl
2023-12-01 09:56:43,041 - speechbrain.core - INFO - Info: find_unused_parameters arg overridden by command line input to: True
2023-12-01 09:56:43,041 - speechbrain.core - INFO - Info: jit arg overridden by command line input to: False
2023-12-01 09:56:43,041 - speechbrain.core - INFO - Info: compile arg overridden by command line input to: False
2023-12-01 09:56:43,041 - speechbrain.core - INFO - Info: auto_mix_prec arg from hparam file is used
2023-12-01 09:56:43,041 - speechbrain.core - INFO - Info: bfloat16_mix_prec arg from hparam file is used
2023-12-01 09:56:43,041 - speechbrain.core - INFO - Info: max_grad_norm arg from hparam file is used
2023-12-01 09:56:43,041 - speechbrain.core - INFO - Info: optimizer_step_limit arg from hparam file is used
2023-12-01 09:56:43,041 - speechbrain.core - INFO - Info: tqdm_colored_bar arg overridden by command line input to: False
2023-12-01 09:56:43,041 - speechbrain.core - INFO - Info: remove_vector_weight_decay arg overridden by command line input to: False
2023-12-01 09:56:43,134 - speechbrain.core - INFO - 46.5M trainable parameters in BestRQBrain
2023-12-01 09:56:43,338 - speechbrain.utils.checkpoints - INFO - Would load a checkpoint here, but none found yet.
2023-12-01 09:56:43,338 - speechbrain.utils.epoch_loop - INFO - Going into epoch 1
2023-12-01 09:56:43,339 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 09:56:44,145 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 09:58:07,111 - speechbrain.utils.train_logger - INFO - steps: 1000, lr: 3.20e-05, avg_loss: 6.90, run_time: 1.01e+02
2023-12-01 09:58:07,385 - speechbrain.utils.train_logger - INFO - steps: 500, lr: 1.60e-05, avg_loss: 7.52
2023-12-01 09:59:27,161 - speechbrain.utils.train_logger - INFO - steps: 1000, lr: 3.20e-05, avg_loss: 6.80, run_time: 79.76
2023-12-01 09:59:48,363 - speechbrain.utils.train_logger - INFO - steps: 1500, lr: 4.80e-05, avg_loss: 6.51, run_time: 1.01e+02
2023-12-01 10:00:47,014 - speechbrain.utils.train_logger - INFO - steps: 1500, lr: 4.80e-05, avg_loss: 6.38, run_time: 79.86
2023-12-01 10:01:29,652 - speechbrain.utils.train_logger - INFO - steps: 2000, lr: 6.40e-05, avg_loss: 6.17, run_time: 1.01e+02
2023-12-01 10:02:07,017 - speechbrain.utils.train_logger - INFO - steps: 2000, lr: 6.40e-05, avg_loss: 6.07, run_time: 80.01
2023-12-01 10:03:10,917 - speechbrain.utils.train_logger - INFO - steps: 2500, lr: 8.00e-05, avg_loss: 5.90, run_time: 1.01e+02
2023-12-01 10:03:27,086 - speechbrain.utils.train_logger - INFO - steps: 2500, lr: 8.00e-05, avg_loss: 5.83, run_time: 80.07
2023-12-01 10:04:47,474 - speechbrain.utils.train_logger - INFO - steps: 3000, lr: 9.60e-05, avg_loss: 5.63, run_time: 80.38
2023-12-01 10:04:52,094 - speechbrain.utils.train_logger - INFO - steps: 3000, lr: 9.60e-05, avg_loss: 5.69, run_time: 1.01e+02
2023-12-01 10:06:07,689 - speechbrain.utils.train_logger - INFO - steps: 3500, lr: 1.12e-04, avg_loss: 5.47, run_time: 80.21
2023-12-01 10:06:33,181 - speechbrain.utils.train_logger - INFO - steps: 3500, lr: 1.12e-04, avg_loss: 5.51, run_time: 1.01e+02
2023-12-01 10:07:27,744 - speechbrain.utils.train_logger - INFO - steps: 4000, lr: 1.28e-04, avg_loss: 5.34, run_time: 80.07
2023-12-01 10:08:14,484 - speechbrain.utils.train_logger - INFO - steps: 4000, lr: 1.28e-04, avg_loss: 5.37, run_time: 1.01e+02
2023-12-01 10:08:47,804 - speechbrain.utils.train_logger - INFO - steps: 4500, lr: 1.44e-04, avg_loss: 5.22, run_time: 80.06
2023-12-01 10:09:55,607 - speechbrain.utils.train_logger - INFO - steps: 4500, lr: 1.44e-04, avg_loss: 5.25, run_time: 1.01e+02
2023-12-01 10:10:00,682 - speechbrain.utils.train_logger - INFO - epoch: 1, steps: 4832, lr: 1.55e-04 - train loss: 5.15 - valid loss: 4.87, valid accuracy: 1.46e-01
2023-12-01 10:10:01,623 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+10-10-00+00
2023-12-01 10:10:01,625 - speechbrain.utils.epoch_loop - INFO - Going into epoch 2
2023-12-01 10:10:01,627 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 10:10:02,514 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 10:10:30,785 - speechbrain.utils.train_logger - INFO - steps: 5000, lr: 1.60e-04, avg_loss: 4.24, run_time: 1.03e+02
2023-12-01 10:11:27,107 - speechbrain.utils.train_logger - INFO - epoch: 1, steps: 4832, lr: 1.55e-04 - train loss: 5.18 - valid loss: 4.96, valid accuracy: 1.29e-01
2023-12-01 10:11:28,331 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+10-11-27+00
2023-12-01 10:11:28,334 - speechbrain.utils.epoch_loop - INFO - Going into epoch 2
2023-12-01 10:11:28,336 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 10:11:29,208 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 10:11:51,368 - speechbrain.utils.train_logger - INFO - steps: 5500, lr: 1.76e-04, avg_loss: 4.21, run_time: 80.58
2023-12-01 10:12:04,773 - speechbrain.utils.train_logger - INFO - steps: 5000, lr: 1.60e-04, avg_loss: 4.22, run_time: 1.29e+02
2023-12-01 10:13:11,491 - speechbrain.utils.train_logger - INFO - steps: 6000, lr: 1.92e-04, avg_loss: 4.18, run_time: 80.12
2023-12-01 10:13:45,996 - speechbrain.utils.train_logger - INFO - steps: 5500, lr: 1.76e-04, avg_loss: 4.20, run_time: 1.01e+02
2023-12-01 10:14:31,903 - speechbrain.utils.train_logger - INFO - steps: 6500, lr: 2.08e-04, avg_loss: 4.16, run_time: 80.42
2023-12-01 10:15:27,503 - speechbrain.utils.train_logger - INFO - steps: 6000, lr: 1.92e-04, avg_loss: 4.17, run_time: 1.02e+02
2023-12-01 10:15:52,114 - speechbrain.utils.train_logger - INFO - steps: 7000, lr: 2.24e-04, avg_loss: 4.15, run_time: 80.21
2023-12-01 10:17:08,807 - speechbrain.utils.train_logger - INFO - steps: 6500, lr: 2.08e-04, avg_loss: 4.15, run_time: 1.01e+02
2023-12-01 10:17:12,095 - speechbrain.utils.train_logger - INFO - steps: 7500, lr: 2.40e-04, avg_loss: 4.13, run_time: 79.98
2023-12-01 10:18:32,167 - speechbrain.utils.train_logger - INFO - steps: 8000, lr: 2.56e-04, avg_loss: 4.12, run_time: 80.06
2023-12-01 10:18:50,118 - speechbrain.utils.train_logger - INFO - steps: 7000, lr: 2.24e-04, avg_loss: 4.14, run_time: 1.01e+02
2023-12-01 10:19:52,577 - speechbrain.utils.train_logger - INFO - steps: 8500, lr: 2.72e-04, avg_loss: 4.11, run_time: 80.42
2023-12-01 10:20:31,402 - speechbrain.utils.train_logger - INFO - steps: 7500, lr: 2.40e-04, avg_loss: 4.12, run_time: 1.01e+02
2023-12-01 10:21:12,935 - speechbrain.utils.train_logger - INFO - steps: 9000, lr: 2.88e-04, avg_loss: 4.09, run_time: 80.36
2023-12-01 10:22:12,520 - speechbrain.utils.train_logger - INFO - steps: 8000, lr: 2.56e-04, avg_loss: 4.11, run_time: 1.01e+02
2023-12-01 10:22:32,933 - speechbrain.utils.train_logger - INFO - steps: 9500, lr: 3.04e-04, avg_loss: 4.08, run_time: 80.00
2023-12-01 10:23:18,496 - speechbrain.utils.train_logger - INFO - epoch: 2, steps: 9664, lr: 3.09e-04 - train loss: 4.08 - valid loss: 4.46, valid accuracy: 1.69e-01
2023-12-01 10:23:19,150 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+10-23-18+00
2023-12-01 10:23:19,156 - speechbrain.utils.epoch_loop - INFO - Going into epoch 3
2023-12-01 10:23:19,158 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 10:23:20,123 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 10:23:54,219 - speechbrain.utils.train_logger - INFO - steps: 8500, lr: 2.72e-04, avg_loss: 4.10, run_time: 1.02e+02
2023-12-01 10:24:15,492 - speechbrain.utils.train_logger - INFO - steps: 10000, lr: 3.20e-04, avg_loss: 3.95, run_time: 1.03e+02
2023-12-01 10:25:35,957 - speechbrain.utils.train_logger - INFO - steps: 9000, lr: 2.88e-04, avg_loss: 4.09, run_time: 1.02e+02
2023-12-01 10:25:36,238 - speechbrain.utils.train_logger - INFO - steps: 10500, lr: 3.36e-04, avg_loss: 3.94, run_time: 80.75
2023-12-01 10:26:56,564 - speechbrain.utils.train_logger - INFO - steps: 11000, lr: 3.52e-04, avg_loss: 3.93, run_time: 80.30
2023-12-01 10:27:17,737 - speechbrain.utils.train_logger - INFO - steps: 9500, lr: 3.04e-04, avg_loss: 4.08, run_time: 1.02e+02
2023-12-01 10:28:14,931 - speechbrain.utils.train_logger - INFO - epoch: 2, steps: 9664, lr: 3.09e-04 - train loss: 4.07 - valid loss: 4.59, valid accuracy: 1.47e-01
2023-12-01 10:28:15,873 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+10-28-14+00
2023-12-01 10:28:15,881 - speechbrain.utils.epoch_loop - INFO - Going into epoch 3
2023-12-01 10:28:15,883 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 10:28:16,702 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 10:28:16,714 - speechbrain.utils.train_logger - INFO - steps: 11500, lr: 3.68e-04, avg_loss: 3.93, run_time: 80.18
2023-12-01 10:29:26,584 - speechbrain.utils.train_logger - INFO - steps: 10000, lr: 3.20e-04, avg_loss: 3.96, run_time: 1.29e+02
2023-12-01 10:29:36,832 - speechbrain.utils.train_logger - INFO - steps: 12000, lr: 3.84e-04, avg_loss: 3.93, run_time: 80.12
2023-12-01 10:30:56,806 - speechbrain.utils.train_logger - INFO - steps: 12500, lr: 4.00e-04, avg_loss: 3.92, run_time: 79.98
2023-12-01 10:31:07,919 - speechbrain.utils.train_logger - INFO - steps: 10500, lr: 3.36e-04, avg_loss: 3.95, run_time: 1.01e+02
2023-12-01 10:32:16,953 - speechbrain.utils.train_logger - INFO - steps: 13000, lr: 4.16e-04, avg_loss: 3.91, run_time: 80.13
2023-12-01 10:32:49,212 - speechbrain.utils.train_logger - INFO - steps: 11000, lr: 3.52e-04, avg_loss: 3.94, run_time: 1.01e+02
2023-12-01 10:33:37,233 - speechbrain.utils.train_logger - INFO - steps: 13500, lr: 4.32e-04, avg_loss: 3.91, run_time: 80.30
2023-12-01 10:34:30,955 - speechbrain.utils.train_logger - INFO - steps: 11500, lr: 3.68e-04, avg_loss: 3.93, run_time: 1.02e+02
2023-12-01 10:34:57,324 - speechbrain.utils.train_logger - INFO - steps: 14000, lr: 4.48e-04, avg_loss: 3.90, run_time: 80.09
2023-12-01 10:36:12,827 - speechbrain.utils.train_logger - INFO - steps: 12000, lr: 3.84e-04, avg_loss: 3.93, run_time: 1.02e+02
2023-12-01 10:36:35,843 - speechbrain.utils.train_logger - INFO - epoch: 3, steps: 14496, lr: 4.64e-04 - train loss: 3.90 - valid loss: 4.29, valid accuracy: 1.79e-01
2023-12-01 10:36:36,557 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+10-36-35+00
2023-12-01 10:36:36,570 - speechbrain.utils.epoch_loop - INFO - Going into epoch 4
2023-12-01 10:36:36,571 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 10:36:37,494 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 10:36:39,323 - speechbrain.utils.train_logger - INFO - steps: 14500, lr: 4.64e-04, avg_loss: 3.55, run_time: 1.02e+02
2023-12-01 10:37:54,475 - speechbrain.utils.train_logger - INFO - steps: 12500, lr: 4.00e-04, avg_loss: 3.92, run_time: 1.02e+02
2023-12-01 10:37:59,628 - speechbrain.utils.train_logger - INFO - steps: 15000, lr: 4.80e-04, avg_loss: 3.85, run_time: 80.30
2023-12-01 10:39:19,759 - speechbrain.utils.train_logger - INFO - steps: 15500, lr: 4.96e-04, avg_loss: 3.83, run_time: 80.12
2023-12-01 10:39:36,018 - speechbrain.utils.train_logger - INFO - steps: 13000, lr: 4.16e-04, avg_loss: 3.92, run_time: 1.02e+02
2023-12-01 10:40:39,904 - speechbrain.utils.train_logger - INFO - steps: 16000, lr: 5.12e-04, avg_loss: 3.83, run_time: 80.14
2023-12-01 10:41:17,743 - speechbrain.utils.train_logger - INFO - steps: 13500, lr: 4.32e-04, avg_loss: 3.91, run_time: 1.02e+02
2023-12-01 10:41:59,942 - speechbrain.utils.train_logger - INFO - steps: 16500, lr: 5.28e-04, avg_loss: 3.83, run_time: 80.05
2023-12-01 10:42:59,273 - speechbrain.utils.train_logger - INFO - steps: 14000, lr: 4.48e-04, avg_loss: 3.91, run_time: 1.02e+02
2023-12-01 10:43:19,798 - speechbrain.utils.train_logger - INFO - steps: 17000, lr: 5.44e-04, avg_loss: 3.82, run_time: 79.86
2023-12-01 10:44:39,870 - speechbrain.utils.train_logger - INFO - steps: 17500, lr: 5.60e-04, avg_loss: 3.82, run_time: 80.06
2023-12-01 10:45:03,795 - speechbrain.utils.train_logger - INFO - epoch: 3, steps: 14496, lr: 4.64e-04 - train loss: 3.90 - valid loss: 4.46, valid accuracy: 1.57e-01
2023-12-01 10:45:04,788 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+10-45-03+00
2023-12-01 10:45:04,803 - speechbrain.utils.epoch_loop - INFO - Going into epoch 4
2023-12-01 10:45:04,805 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 10:45:05,548 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 10:45:07,796 - speechbrain.utils.train_logger - INFO - steps: 14500, lr: 4.64e-04, avg_loss: 3.64, run_time: 1.29e+02
2023-12-01 10:45:59,640 - speechbrain.utils.train_logger - INFO - steps: 18000, lr: 5.76e-04, avg_loss: 3.82, run_time: 79.78
2023-12-01 10:46:48,966 - speechbrain.utils.train_logger - INFO - steps: 15000, lr: 4.80e-04, avg_loss: 3.84, run_time: 1.01e+02
2023-12-01 10:47:19,348 - speechbrain.utils.train_logger - INFO - steps: 18500, lr: 5.92e-04, avg_loss: 3.81, run_time: 79.71
2023-12-01 10:48:30,139 - speechbrain.utils.train_logger - INFO - steps: 15500, lr: 4.96e-04, avg_loss: 3.83, run_time: 1.01e+02
2023-12-01 10:48:39,274 - speechbrain.utils.train_logger - INFO - steps: 19000, lr: 6.08e-04, avg_loss: 3.81, run_time: 79.93
2023-12-01 10:49:51,135 - speechbrain.utils.train_logger - INFO - epoch: 4, steps: 19328, lr: 6.18e-04 - train loss: 3.81 - valid loss: 4.23, valid accuracy: 1.82e-01
2023-12-01 10:49:51,917 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+10-49-51+00
2023-12-01 10:49:51,939 - speechbrain.utils.epoch_loop - INFO - Going into epoch 5
2023-12-01 10:49:51,941 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 10:49:52,731 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 10:50:11,266 - speechbrain.utils.train_logger - INFO - steps: 16000, lr: 5.12e-04, avg_loss: 3.83, run_time: 1.01e+02
2023-12-01 10:50:21,970 - speechbrain.utils.train_logger - INFO - steps: 19500, lr: 6.24e-04, avg_loss: 3.79, run_time: 1.03e+02
2023-12-01 10:51:42,185 - speechbrain.utils.train_logger - INFO - steps: 20000, lr: 6.40e-04, avg_loss: 3.77, run_time: 80.19
2023-12-01 10:51:52,300 - speechbrain.utils.train_logger - INFO - steps: 16500, lr: 5.28e-04, avg_loss: 3.83, run_time: 1.01e+02
2023-12-01 10:53:02,514 - speechbrain.utils.train_logger - INFO - steps: 20500, lr: 6.56e-04, avg_loss: 3.77, run_time: 80.36
2023-12-01 10:53:33,417 - speechbrain.utils.train_logger - INFO - steps: 17000, lr: 5.44e-04, avg_loss: 3.83, run_time: 1.01e+02
2023-12-01 10:54:23,117 - speechbrain.utils.train_logger - INFO - steps: 21000, lr: 6.72e-04, avg_loss: 3.77, run_time: 80.60
2023-12-01 10:55:15,182 - speechbrain.utils.train_logger - INFO - steps: 17500, lr: 5.60e-04, avg_loss: 3.82, run_time: 1.02e+02
2023-12-01 10:55:43,208 - speechbrain.utils.train_logger - INFO - steps: 21500, lr: 6.88e-04, avg_loss: 3.76, run_time: 80.09
2023-12-01 10:56:57,207 - speechbrain.utils.train_logger - INFO - steps: 18000, lr: 5.76e-04, avg_loss: 3.82, run_time: 1.02e+02
2023-12-01 10:57:04,191 - speechbrain.utils.train_logger - INFO - steps: 22000, lr: 7.04e-04, avg_loss: 3.76, run_time: 80.98
2023-12-01 10:58:24,545 - speechbrain.utils.train_logger - INFO - steps: 22500, lr: 7.20e-04, avg_loss: 3.76, run_time: 80.34
2023-12-01 10:58:38,061 - speechbrain.utils.train_logger - INFO - steps: 18500, lr: 5.92e-04, avg_loss: 3.81, run_time: 1.01e+02
2023-12-01 10:59:44,403 - speechbrain.utils.train_logger - INFO - steps: 23000, lr: 7.36e-04, avg_loss: 3.75, run_time: 79.85
2023-12-01 11:00:19,585 - speechbrain.utils.train_logger - INFO - steps: 19000, lr: 6.08e-04, avg_loss: 3.81, run_time: 1.02e+02
2023-12-01 11:01:04,574 - speechbrain.utils.train_logger - INFO - steps: 23500, lr: 7.52e-04, avg_loss: 3.75, run_time: 80.20
2023-12-01 11:01:50,463 - speechbrain.utils.train_logger - INFO - epoch: 4, steps: 19328, lr: 6.18e-04 - train loss: 3.81 - valid loss: 4.39, valid accuracy: 1.62e-01
2023-12-01 11:01:51,370 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+11-01-50+00
2023-12-01 11:01:51,395 - speechbrain.utils.epoch_loop - INFO - Going into epoch 5
2023-12-01 11:01:51,397 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 11:01:52,214 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 11:02:24,597 - speechbrain.utils.train_logger - INFO - steps: 24000, lr: 7.68e-04, avg_loss: 3.75, run_time: 80.02
2023-12-01 11:02:28,427 - speechbrain.utils.train_logger - INFO - steps: 19500, lr: 6.24e-04, avg_loss: 3.76, run_time: 1.29e+02
2023-12-01 11:03:09,262 - speechbrain.utils.train_logger - INFO - epoch: 5, steps: 24160, lr: 7.73e-04 - train loss: 3.75 - valid loss: 4.24, valid accuracy: 1.85e-01
2023-12-01 11:03:09,916 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+11-03-09+00
2023-12-01 11:03:09,950 - speechbrain.utils.epoch_loop - INFO - Going into epoch 6
2023-12-01 11:03:09,951 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 11:03:10,694 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 11:04:06,545 - speechbrain.utils.train_logger - INFO - steps: 24500, lr: 7.84e-04, avg_loss: 3.74, run_time: 1.02e+02
2023-12-01 11:04:09,801 - speechbrain.utils.train_logger - INFO - steps: 20000, lr: 6.40e-04, avg_loss: 3.76, run_time: 1.01e+02
2023-12-01 11:05:26,684 - speechbrain.utils.train_logger - INFO - steps: 25000, lr: 8.00e-04, avg_loss: 3.73, run_time: 80.10
2023-12-01 11:05:50,956 - speechbrain.utils.train_logger - INFO - steps: 20500, lr: 6.56e-04, avg_loss: 3.77, run_time: 1.01e+02
2023-12-01 11:06:46,803 - speechbrain.utils.train_logger - INFO - steps: 25500, lr: 7.92e-04, avg_loss: 3.73, run_time: 80.14
2023-12-01 11:07:32,206 - speechbrain.utils.train_logger - INFO - steps: 21000, lr: 6.72e-04, avg_loss: 3.77, run_time: 1.01e+02
2023-12-01 11:08:07,194 - speechbrain.utils.train_logger - INFO - steps: 26000, lr: 7.84e-04, avg_loss: 3.72, run_time: 80.40
2023-12-01 11:09:13,413 - speechbrain.utils.train_logger - INFO - steps: 21500, lr: 6.88e-04, avg_loss: 3.77, run_time: 1.01e+02
2023-12-01 11:09:27,196 - speechbrain.utils.train_logger - INFO - steps: 26500, lr: 7.77e-04, avg_loss: 3.72, run_time: 80.00
2023-12-01 11:10:47,329 - speechbrain.utils.train_logger - INFO - steps: 27000, lr: 7.70e-04, avg_loss: 3.72, run_time: 80.04
2023-12-01 11:10:55,075 - speechbrain.utils.train_logger - INFO - steps: 22000, lr: 7.04e-04, avg_loss: 3.76, run_time: 1.02e+02
2023-12-01 11:12:07,849 - speechbrain.utils.train_logger - INFO - steps: 27500, lr: 7.63e-04, avg_loss: 3.72, run_time: 80.57
2023-12-01 11:12:36,412 - speechbrain.utils.train_logger - INFO - steps: 22500, lr: 7.20e-04, avg_loss: 3.76, run_time: 1.01e+02
2023-12-01 11:13:28,243 - speechbrain.utils.train_logger - INFO - steps: 28000, lr: 7.56e-04, avg_loss: 3.71, run_time: 80.43
2023-12-01 11:14:17,846 - speechbrain.utils.train_logger - INFO - steps: 23000, lr: 7.36e-04, avg_loss: 3.75, run_time: 1.01e+02
2023-12-01 11:14:48,759 - speechbrain.utils.train_logger - INFO - steps: 28500, lr: 7.49e-04, avg_loss: 3.71, run_time: 80.53
2023-12-01 11:15:59,768 - speechbrain.utils.train_logger - INFO - steps: 23500, lr: 7.52e-04, avg_loss: 3.75, run_time: 1.02e+02
2023-12-01 11:16:27,231 - speechbrain.utils.train_logger - INFO - epoch: 6, steps: 28992, lr: 7.43e-04 - train loss: 3.71 - valid loss: 4.15, valid accuracy: 1.89e-01
2023-12-01 11:16:27,908 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+11-16-27+00
2023-12-01 11:16:27,953 - speechbrain.utils.epoch_loop - INFO - Going into epoch 7
2023-12-01 11:16:27,954 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 11:16:28,683 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 11:16:31,559 - speechbrain.utils.train_logger - INFO - steps: 29000, lr: 7.43e-04, avg_loss: 3.76, run_time: 1.03e+02
2023-12-01 11:17:41,540 - speechbrain.utils.train_logger - INFO - steps: 24000, lr: 7.68e-04, avg_loss: 3.75, run_time: 1.02e+02
2023-12-01 11:17:51,652 - speechbrain.utils.train_logger - INFO - steps: 29500, lr: 7.36e-04, avg_loss: 3.66, run_time: 80.09
2023-12-01 11:18:37,654 - speechbrain.utils.train_logger - INFO - epoch: 5, steps: 24160, lr: 7.73e-04 - train loss: 3.75 - valid loss: 4.34, valid accuracy: 1.68e-01
2023-12-01 11:18:38,881 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+11-18-37+00
2023-12-01 11:18:38,996 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+10-10-00+00
2023-12-01 11:18:38,996 - speechbrain.utils.epoch_loop - INFO - Going into epoch 6
2023-12-01 11:18:38,999 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 11:18:39,928 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 11:19:50,321 - speechbrain.utils.train_logger - INFO - steps: 24500, lr: 7.84e-04, avg_loss: 3.74, run_time: 1.29e+02
2023-12-01 11:21:31,593 - speechbrain.utils.train_logger - INFO - steps: 25000, lr: 8.00e-04, avg_loss: 3.73, run_time: 1.01e+02
2023-12-01 11:23:13,172 - speechbrain.utils.train_logger - INFO - steps: 25500, lr: 7.92e-04, avg_loss: 3.73, run_time: 1.02e+02
2023-12-01 11:24:54,821 - speechbrain.utils.train_logger - INFO - steps: 26000, lr: 7.84e-04, avg_loss: 3.72, run_time: 1.02e+02
2023-12-01 11:26:36,784 - speechbrain.utils.train_logger - INFO - steps: 26500, lr: 7.77e-04, avg_loss: 3.72, run_time: 1.02e+02
2023-12-01 11:28:19,081 - speechbrain.utils.train_logger - INFO - steps: 27000, lr: 7.70e-04, avg_loss: 3.72, run_time: 1.02e+02
2023-12-01 11:30:01,112 - speechbrain.utils.train_logger - INFO - steps: 27500, lr: 7.63e-04, avg_loss: 3.72, run_time: 1.02e+02
2023-12-01 11:31:43,209 - speechbrain.utils.train_logger - INFO - steps: 28000, lr: 7.56e-04, avg_loss: 3.72, run_time: 1.02e+02
2023-12-01 11:33:25,339 - speechbrain.utils.train_logger - INFO - steps: 28500, lr: 7.49e-04, avg_loss: 3.71, run_time: 1.02e+02
2023-12-01 11:35:29,452 - speechbrain.utils.train_logger - INFO - epoch: 6, steps: 28992, lr: 7.43e-04 - train loss: 3.71 - valid loss: 4.30, valid accuracy: 1.71e-01
2023-12-01 11:35:30,906 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+11-35-29+00
2023-12-01 11:35:30,994 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+10-11-27+00
2023-12-01 11:35:30,994 - speechbrain.utils.epoch_loop - INFO - Going into epoch 7
2023-12-01 11:35:30,996 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 11:35:31,968 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 11:35:34,811 - speechbrain.utils.train_logger - INFO - steps: 29000, lr: 7.43e-04, avg_loss: 3.72, run_time: 1.29e+02
2023-12-01 11:37:17,025 - speechbrain.utils.train_logger - INFO - steps: 29500, lr: 7.36e-04, avg_loss: 3.66, run_time: 1.02e+02
2023-12-01 11:38:58,705 - speechbrain.utils.train_logger - INFO - steps: 30000, lr: 7.30e-04, avg_loss: 3.65, run_time: 1.02e+02
2023-12-01 11:40:40,109 - speechbrain.utils.train_logger - INFO - steps: 30500, lr: 7.24e-04, avg_loss: 3.65, run_time: 1.01e+02
2023-12-01 11:42:21,824 - speechbrain.utils.train_logger - INFO - steps: 31000, lr: 7.18e-04, avg_loss: 3.65, run_time: 1.02e+02
2023-12-01 11:44:03,405 - speechbrain.utils.train_logger - INFO - steps: 31500, lr: 7.13e-04, avg_loss: 3.66, run_time: 1.02e+02
2023-12-01 11:45:45,268 - speechbrain.utils.train_logger - INFO - steps: 32000, lr: 7.07e-04, avg_loss: 3.65, run_time: 1.02e+02
2023-12-01 11:47:27,021 - speechbrain.utils.train_logger - INFO - steps: 32500, lr: 7.02e-04, avg_loss: 3.65, run_time: 1.02e+02
2023-12-01 11:49:08,884 - speechbrain.utils.train_logger - INFO - steps: 33000, lr: 6.96e-04, avg_loss: 3.65, run_time: 1.02e+02
2023-12-01 11:50:50,667 - speechbrain.utils.train_logger - INFO - steps: 33500, lr: 6.91e-04, avg_loss: 3.65, run_time: 1.02e+02
2023-12-01 11:52:20,713 - speechbrain.utils.train_logger - INFO - epoch: 7, steps: 33824, lr: 6.88e-04 - train loss: 3.64 - valid loss: 4.24, valid accuracy: 1.74e-01
2023-12-01 11:52:21,617 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+11-52-20+00
2023-12-01 11:52:21,734 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+10-23-18+00
2023-12-01 11:52:21,734 - speechbrain.utils.epoch_loop - INFO - Going into epoch 8
2023-12-01 11:52:21,736 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 11:52:22,537 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 11:52:59,981 - speechbrain.utils.train_logger - INFO - steps: 34000, lr: 6.86e-04, avg_loss: 3.60, run_time: 1.29e+02
2023-12-01 11:54:41,187 - speechbrain.utils.train_logger - INFO - steps: 34500, lr: 6.81e-04, avg_loss: 3.62, run_time: 1.01e+02
2023-12-01 11:56:22,762 - speechbrain.utils.train_logger - INFO - steps: 35000, lr: 6.76e-04, avg_loss: 3.62, run_time: 1.02e+02
2023-12-01 11:58:04,182 - speechbrain.utils.train_logger - INFO - steps: 35500, lr: 6.71e-04, avg_loss: 3.61, run_time: 1.01e+02
2023-12-01 11:59:46,595 - speechbrain.utils.train_logger - INFO - steps: 36000, lr: 6.67e-04, avg_loss: 3.61, run_time: 1.02e+02
2023-12-01 12:01:28,267 - speechbrain.utils.train_logger - INFO - steps: 36500, lr: 6.62e-04, avg_loss: 3.62, run_time: 1.02e+02
2023-12-01 12:03:10,106 - speechbrain.utils.train_logger - INFO - steps: 37000, lr: 6.58e-04, avg_loss: 3.61, run_time: 1.02e+02
2023-12-01 12:04:52,128 - speechbrain.utils.train_logger - INFO - steps: 37500, lr: 6.53e-04, avg_loss: 3.61, run_time: 1.02e+02
2023-12-01 12:06:33,750 - speechbrain.utils.train_logger - INFO - steps: 38000, lr: 6.49e-04, avg_loss: 3.61, run_time: 1.02e+02
2023-12-01 12:08:15,446 - speechbrain.utils.train_logger - INFO - steps: 38500, lr: 6.45e-04, avg_loss: 3.61, run_time: 1.02e+02
2023-12-01 12:09:11,091 - speechbrain.utils.train_logger - INFO - epoch: 8, steps: 38656, lr: 6.43e-04 - train loss: 3.61 - valid loss: 4.28, valid accuracy: 1.72e-01
2023-12-01 12:09:12,076 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+12-09-11+00
2023-12-01 12:09:12,198 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+10-28-14+00
2023-12-01 12:09:12,199 - speechbrain.utils.epoch_loop - INFO - Going into epoch 9
2023-12-01 12:09:12,201 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 12:09:12,989 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 12:10:24,636 - speechbrain.utils.train_logger - INFO - steps: 39000, lr: 6.41e-04, avg_loss: 3.60, run_time: 1.29e+02
2023-12-01 12:12:06,591 - speechbrain.utils.train_logger - INFO - steps: 39500, lr: 6.36e-04, avg_loss: 3.59, run_time: 1.02e+02
2023-12-01 12:13:48,446 - speechbrain.utils.train_logger - INFO - steps: 40000, lr: 6.32e-04, avg_loss: 3.59, run_time: 1.02e+02
2023-12-01 12:15:30,377 - speechbrain.utils.train_logger - INFO - steps: 40500, lr: 6.29e-04, avg_loss: 3.59, run_time: 1.02e+02
2023-12-01 12:17:12,140 - speechbrain.utils.train_logger - INFO - steps: 41000, lr: 6.25e-04, avg_loss: 3.59, run_time: 1.02e+02
2023-12-01 12:18:53,779 - speechbrain.utils.train_logger - INFO - steps: 41500, lr: 6.21e-04, avg_loss: 3.59, run_time: 1.02e+02
2023-12-01 12:20:35,468 - speechbrain.utils.train_logger - INFO - steps: 42000, lr: 6.17e-04, avg_loss: 3.59, run_time: 1.02e+02
2023-12-01 12:22:16,923 - speechbrain.utils.train_logger - INFO - steps: 42500, lr: 6.14e-04, avg_loss: 3.58, run_time: 1.01e+02
2023-12-01 12:23:58,395 - speechbrain.utils.train_logger - INFO - steps: 43000, lr: 6.10e-04, avg_loss: 3.58, run_time: 1.01e+02
2023-12-01 12:26:01,830 - speechbrain.utils.train_logger - INFO - epoch: 9, steps: 43488, lr: 6.07e-04 - train loss: 3.58 - valid loss: 4.24, valid accuracy: 1.77e-01
2023-12-01 12:26:02,783 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+12-26-01+00
2023-12-01 12:26:02,964 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+10-36-35+00
2023-12-01 12:26:02,964 - speechbrain.utils.epoch_loop - INFO - Going into epoch 10
2023-12-01 12:26:02,966 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 12:26:03,796 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 12:26:07,543 - speechbrain.utils.train_logger - INFO - steps: 43500, lr: 6.06e-04, avg_loss: 3.61, run_time: 1.29e+02
2023-12-01 12:27:49,348 - speechbrain.utils.train_logger - INFO - steps: 44000, lr: 6.03e-04, avg_loss: 3.56, run_time: 1.02e+02
2023-12-01 12:29:31,470 - speechbrain.utils.train_logger - INFO - steps: 44500, lr: 6.00e-04, avg_loss: 3.56, run_time: 1.02e+02
2023-12-01 12:31:13,242 - speechbrain.utils.train_logger - INFO - steps: 45000, lr: 5.96e-04, avg_loss: 3.56, run_time: 1.02e+02
2023-12-01 12:32:54,922 - speechbrain.utils.train_logger - INFO - steps: 45500, lr: 5.93e-04, avg_loss: 3.55, run_time: 1.02e+02
2023-12-01 12:34:36,787 - speechbrain.utils.train_logger - INFO - steps: 46000, lr: 5.90e-04, avg_loss: 3.55, run_time: 1.02e+02
2023-12-01 12:36:18,841 - speechbrain.utils.train_logger - INFO - steps: 46500, lr: 5.87e-04, avg_loss: 3.55, run_time: 1.02e+02
2023-12-01 12:38:01,330 - speechbrain.utils.train_logger - INFO - steps: 47000, lr: 5.83e-04, avg_loss: 3.55, run_time: 1.02e+02
2023-12-01 12:39:43,668 - speechbrain.utils.train_logger - INFO - steps: 47500, lr: 5.80e-04, avg_loss: 3.55, run_time: 1.02e+02
2023-12-01 12:41:26,423 - speechbrain.utils.train_logger - INFO - steps: 48000, lr: 5.77e-04, avg_loss: 3.55, run_time: 1.03e+02
2023-12-01 12:42:57,390 - speechbrain.utils.train_logger - INFO - epoch: 10, steps: 48320, lr: 5.75e-04 - train loss: 3.55 - valid loss: 4.21, valid accuracy: 1.78e-01
2023-12-01 12:42:58,832 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+12-42-57+00
2023-12-01 12:42:58,943 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+10-45-03+00
2023-12-01 12:42:58,943 - speechbrain.utils.epoch_loop - INFO - Going into epoch 11
2023-12-01 12:42:58,945 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 12:42:59,803 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 12:43:38,121 - speechbrain.utils.train_logger - INFO - steps: 48500, lr: 5.74e-04, avg_loss: 3.52, run_time: 1.32e+02
2023-12-01 12:45:20,759 - speechbrain.utils.train_logger - INFO - steps: 49000, lr: 5.71e-04, avg_loss: 3.53, run_time: 1.03e+02
2023-12-01 12:47:03,032 - speechbrain.utils.train_logger - INFO - steps: 49500, lr: 5.69e-04, avg_loss: 3.53, run_time: 1.02e+02
2023-12-01 12:48:45,328 - speechbrain.utils.train_logger - INFO - steps: 50000, lr: 5.66e-04, avg_loss: 3.53, run_time: 1.02e+02
2023-12-01 12:50:27,484 - speechbrain.utils.train_logger - INFO - steps: 50500, lr: 5.63e-04, avg_loss: 3.53, run_time: 1.02e+02
2023-12-01 12:52:09,997 - speechbrain.utils.train_logger - INFO - steps: 51000, lr: 5.60e-04, avg_loss: 3.53, run_time: 1.03e+02
2023-12-01 12:53:52,276 - speechbrain.utils.train_logger - INFO - steps: 51500, lr: 5.57e-04, avg_loss: 3.53, run_time: 1.02e+02
2023-12-01 12:55:34,882 - speechbrain.utils.train_logger - INFO - steps: 52000, lr: 5.55e-04, avg_loss: 3.53, run_time: 1.03e+02
2023-12-01 12:57:17,356 - speechbrain.utils.train_logger - INFO - steps: 52500, lr: 5.52e-04, avg_loss: 3.53, run_time: 1.02e+02
2023-12-01 12:59:00,010 - speechbrain.utils.train_logger - INFO - steps: 53000, lr: 5.49e-04, avg_loss: 3.53, run_time: 1.03e+02
2023-12-01 12:59:57,085 - speechbrain.utils.train_logger - INFO - epoch: 11, steps: 53152, lr: 5.49e-04 - train loss: 3.53 - valid loss: 4.22, valid accuracy: 1.79e-01
2023-12-01 12:59:58,002 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+12-59-57+00
2023-12-01 12:59:58,122 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+10-49-51+00
2023-12-01 12:59:58,122 - speechbrain.utils.epoch_loop - INFO - Going into epoch 12
2023-12-01 12:59:58,124 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 12:59:59,104 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 13:01:11,684 - speechbrain.utils.train_logger - INFO - steps: 53500, lr: 5.47e-04, avg_loss: 3.51, run_time: 1.32e+02
2023-12-01 13:02:54,060 - speechbrain.utils.train_logger - INFO - steps: 54000, lr: 5.44e-04, avg_loss: 3.52, run_time: 1.02e+02
2023-12-01 13:04:37,325 - speechbrain.utils.train_logger - INFO - steps: 54500, lr: 5.42e-04, avg_loss: 3.52, run_time: 1.03e+02
2023-12-01 13:06:20,096 - speechbrain.utils.train_logger - INFO - steps: 55000, lr: 5.39e-04, avg_loss: 3.52, run_time: 1.03e+02
2023-12-01 13:08:03,493 - speechbrain.utils.train_logger - INFO - steps: 55500, lr: 5.37e-04, avg_loss: 3.52, run_time: 1.03e+02
2023-12-01 13:09:46,731 - speechbrain.utils.train_logger - INFO - steps: 56000, lr: 5.35e-04, avg_loss: 3.52, run_time: 1.03e+02
2023-12-01 13:11:30,061 - speechbrain.utils.train_logger - INFO - steps: 56500, lr: 5.32e-04, avg_loss: 3.52, run_time: 1.03e+02
2023-12-01 13:13:13,132 - speechbrain.utils.train_logger - INFO - steps: 57000, lr: 5.30e-04, avg_loss: 3.52, run_time: 1.03e+02
2023-12-01 13:14:56,121 - speechbrain.utils.train_logger - INFO - steps: 57500, lr: 5.28e-04, avg_loss: 3.52, run_time: 1.03e+02
2023-12-01 13:17:01,710 - speechbrain.utils.train_logger - INFO - epoch: 12, steps: 57984, lr: 5.25e-04 - train loss: 3.52 - valid loss: 4.16, valid accuracy: 1.81e-01
2023-12-01 13:17:02,969 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+13-17-01+00
2023-12-01 13:17:03,137 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+11-01-50+00
2023-12-01 13:17:03,137 - speechbrain.utils.epoch_loop - INFO - Going into epoch 13
2023-12-01 13:17:03,140 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 13:17:04,047 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 13:17:09,009 - speechbrain.utils.train_logger - INFO - steps: 58000, lr: 5.25e-04, avg_loss: 3.48, run_time: 1.33e+02
2023-12-01 13:18:51,498 - speechbrain.utils.train_logger - INFO - steps: 58500, lr: 5.23e-04, avg_loss: 3.50, run_time: 1.02e+02
2023-12-01 13:20:34,644 - speechbrain.utils.train_logger - INFO - steps: 59000, lr: 5.21e-04, avg_loss: 3.50, run_time: 1.03e+02
2023-12-01 13:22:17,433 - speechbrain.utils.train_logger - INFO - steps: 59500, lr: 5.19e-04, avg_loss: 3.50, run_time: 1.03e+02
2023-12-01 13:24:00,271 - speechbrain.utils.train_logger - INFO - steps: 60000, lr: 5.16e-04, avg_loss: 3.50, run_time: 1.03e+02
2023-12-01 13:25:43,137 - speechbrain.utils.train_logger - INFO - steps: 60500, lr: 5.14e-04, avg_loss: 3.50, run_time: 1.03e+02
2023-12-01 13:27:25,731 - speechbrain.utils.train_logger - INFO - steps: 61000, lr: 5.12e-04, avg_loss: 3.50, run_time: 1.03e+02
2023-12-01 13:29:08,636 - speechbrain.utils.train_logger - INFO - steps: 61500, lr: 5.10e-04, avg_loss: 3.50, run_time: 1.03e+02
2023-12-01 13:30:51,075 - speechbrain.utils.train_logger - INFO - steps: 62000, lr: 5.08e-04, avg_loss: 3.51, run_time: 1.02e+02
2023-12-01 13:32:33,989 - speechbrain.utils.train_logger - INFO - steps: 62500, lr: 5.06e-04, avg_loss: 3.50, run_time: 1.03e+02
2023-12-01 13:34:03,036 - speechbrain.utils.train_logger - INFO - epoch: 13, steps: 62816, lr: 5.05e-04 - train loss: 3.50 - valid loss: 4.17, valid accuracy: 1.81e-01
2023-12-01 13:34:03,978 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+13-34-03+00
2023-12-01 13:34:04,160 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+11-03-09+00
2023-12-01 13:34:04,161 - speechbrain.utils.epoch_loop - INFO - Going into epoch 14
2023-12-01 13:34:04,163 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 13:34:04,983 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 13:34:44,586 - speechbrain.utils.train_logger - INFO - steps: 63000, lr: 5.04e-04, avg_loss: 3.49, run_time: 1.31e+02
2023-12-01 13:36:27,499 - speechbrain.utils.train_logger - INFO - steps: 63500, lr: 5.02e-04, avg_loss: 3.49, run_time: 1.03e+02
2023-12-01 13:38:10,266 - speechbrain.utils.train_logger - INFO - steps: 64000, lr: 5.00e-04, avg_loss: 3.49, run_time: 1.03e+02
2023-12-01 13:39:52,959 - speechbrain.utils.train_logger - INFO - steps: 64500, lr: 4.98e-04, avg_loss: 3.49, run_time: 1.03e+02
2023-12-01 13:41:36,181 - speechbrain.utils.train_logger - INFO - steps: 65000, lr: 4.96e-04, avg_loss: 3.49, run_time: 1.03e+02
2023-12-01 13:43:19,169 - speechbrain.utils.train_logger - INFO - steps: 65500, lr: 4.94e-04, avg_loss: 3.49, run_time: 1.03e+02
2023-12-01 13:45:01,791 - speechbrain.utils.train_logger - INFO - steps: 66000, lr: 4.92e-04, avg_loss: 3.49, run_time: 1.03e+02
2023-12-01 13:46:44,479 - speechbrain.utils.train_logger - INFO - steps: 66500, lr: 4.91e-04, avg_loss: 3.49, run_time: 1.03e+02
2023-12-01 13:48:27,643 - speechbrain.utils.train_logger - INFO - steps: 67000, lr: 4.89e-04, avg_loss: 3.49, run_time: 1.03e+02
2023-12-01 13:50:11,058 - speechbrain.utils.train_logger - INFO - steps: 67500, lr: 4.87e-04, avg_loss: 3.49, run_time: 1.03e+02
2023-12-01 13:51:06,037 - speechbrain.utils.train_logger - INFO - epoch: 14, steps: 67648, lr: 4.86e-04 - train loss: 3.49 - valid loss: 4.16, valid accuracy: 1.83e-01
2023-12-01 13:51:07,408 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+13-51-06+00
2023-12-01 13:51:07,624 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+11-16-27+00
2023-12-01 13:51:07,625 - speechbrain.utils.epoch_loop - INFO - Going into epoch 15
2023-12-01 13:51:07,627 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 13:51:08,434 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 13:52:22,820 - speechbrain.utils.train_logger - INFO - steps: 68000, lr: 4.85e-04, avg_loss: 3.48, run_time: 1.32e+02
2023-12-01 13:54:05,208 - speechbrain.utils.train_logger - INFO - steps: 68500, lr: 4.83e-04, avg_loss: 3.49, run_time: 1.02e+02
2023-12-01 13:55:51,273 - speechbrain.utils.train_logger - INFO - steps: 69000, lr: 4.82e-04, avg_loss: 3.49, run_time: 1.06e+02
2023-12-01 13:57:34,484 - speechbrain.utils.train_logger - INFO - steps: 69500, lr: 4.80e-04, avg_loss: 3.48, run_time: 1.03e+02
2023-12-01 13:59:18,223 - speechbrain.utils.train_logger - INFO - steps: 70000, lr: 4.78e-04, avg_loss: 3.48, run_time: 1.04e+02
2023-12-01 14:01:01,844 - speechbrain.utils.train_logger - INFO - steps: 70500, lr: 4.76e-04, avg_loss: 3.48, run_time: 1.04e+02
2023-12-01 14:02:45,632 - speechbrain.utils.train_logger - INFO - steps: 71000, lr: 4.75e-04, avg_loss: 3.48, run_time: 1.04e+02
2023-12-01 14:04:29,154 - speechbrain.utils.train_logger - INFO - steps: 71500, lr: 4.73e-04, avg_loss: 3.48, run_time: 1.04e+02
2023-12-01 14:06:12,492 - speechbrain.utils.train_logger - INFO - steps: 72000, lr: 4.71e-04, avg_loss: 3.48, run_time: 1.03e+02
2023-12-01 14:08:16,077 - speechbrain.utils.train_logger - INFO - epoch: 15, steps: 72480, lr: 4.70e-04 - train loss: 3.48 - valid loss: 4.20, valid accuracy: 1.80e-01
2023-12-01 14:08:17,218 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+14-08-16+00
2023-12-01 14:08:17,402 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+11-18-37+00
2023-12-01 14:08:17,402 - speechbrain.utils.epoch_loop - INFO - Going into epoch 16
2023-12-01 14:08:17,406 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 14:08:18,188 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 14:08:24,047 - speechbrain.utils.train_logger - INFO - steps: 72500, lr: 4.70e-04, avg_loss: 3.42, run_time: 1.32e+02
2023-12-01 14:10:06,752 - speechbrain.utils.train_logger - INFO - steps: 73000, lr: 4.68e-04, avg_loss: 3.46, run_time: 1.03e+02
2023-12-01 14:11:49,508 - speechbrain.utils.train_logger - INFO - steps: 73500, lr: 4.67e-04, avg_loss: 3.47, run_time: 1.03e+02
2023-12-01 14:13:32,296 - speechbrain.utils.train_logger - INFO - steps: 74000, lr: 4.65e-04, avg_loss: 3.47, run_time: 1.03e+02
2023-12-01 14:15:15,526 - speechbrain.utils.train_logger - INFO - steps: 74500, lr: 4.63e-04, avg_loss: 3.47, run_time: 1.03e+02
2023-12-01 14:16:58,912 - speechbrain.utils.train_logger - INFO - steps: 75000, lr: 4.62e-04, avg_loss: 3.47, run_time: 1.03e+02
2023-12-01 14:18:42,073 - speechbrain.utils.train_logger - INFO - steps: 75500, lr: 4.60e-04, avg_loss: 3.47, run_time: 1.03e+02
2023-12-01 14:20:25,229 - speechbrain.utils.train_logger - INFO - steps: 76000, lr: 4.59e-04, avg_loss: 3.47, run_time: 1.03e+02
2023-12-01 14:22:08,363 - speechbrain.utils.train_logger - INFO - steps: 76500, lr: 4.57e-04, avg_loss: 3.47, run_time: 1.03e+02
2023-12-01 14:23:51,288 - speechbrain.utils.train_logger - INFO - steps: 77000, lr: 4.56e-04, avg_loss: 3.47, run_time: 1.03e+02
2023-12-01 14:25:20,628 - speechbrain.utils.train_logger - INFO - epoch: 16, steps: 77312, lr: 4.55e-04 - train loss: 3.47 - valid loss: 4.15, valid accuracy: 1.85e-01
2023-12-01 14:25:21,614 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+14-25-20+00
2023-12-01 14:25:21,788 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+11-35-29+00
2023-12-01 14:25:21,788 - speechbrain.utils.epoch_loop - INFO - Going into epoch 17
2023-12-01 14:25:21,790 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 14:25:22,761 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 14:26:02,927 - speechbrain.utils.train_logger - INFO - steps: 77500, lr: 4.54e-04, avg_loss: 3.47, run_time: 1.32e+02
2023-12-01 14:27:46,165 - speechbrain.utils.train_logger - INFO - steps: 78000, lr: 4.53e-04, avg_loss: 3.47, run_time: 1.03e+02
2023-12-01 14:29:29,233 - speechbrain.utils.train_logger - INFO - steps: 78500, lr: 4.51e-04, avg_loss: 3.46, run_time: 1.03e+02
2023-12-01 14:31:12,030 - speechbrain.utils.train_logger - INFO - steps: 79000, lr: 4.50e-04, avg_loss: 3.46, run_time: 1.03e+02
2023-12-01 14:32:55,385 - speechbrain.utils.train_logger - INFO - steps: 79500, lr: 4.49e-04, avg_loss: 3.46, run_time: 1.03e+02
2023-12-01 14:34:38,340 - speechbrain.utils.train_logger - INFO - steps: 80000, lr: 4.47e-04, avg_loss: 3.46, run_time: 1.03e+02
2023-12-01 14:36:21,320 - speechbrain.utils.train_logger - INFO - steps: 80500, lr: 4.46e-04, avg_loss: 3.46, run_time: 1.03e+02
2023-12-01 14:38:04,858 - speechbrain.utils.train_logger - INFO - steps: 81000, lr: 4.44e-04, avg_loss: 3.46, run_time: 1.04e+02
2023-12-01 14:39:47,647 - speechbrain.utils.train_logger - INFO - steps: 81500, lr: 4.43e-04, avg_loss: 3.46, run_time: 1.03e+02
2023-12-01 14:41:34,201 - speechbrain.utils.train_logger - INFO - steps: 82000, lr: 4.42e-04, avg_loss: 3.46, run_time: 1.06e+02
2023-12-01 14:42:27,611 - speechbrain.utils.train_logger - INFO - epoch: 17, steps: 82144, lr: 4.41e-04 - train loss: 3.46 - valid loss: 4.20, valid accuracy: 1.81e-01
2023-12-01 14:42:28,655 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+14-42-27+00
2023-12-01 14:42:28,847 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+11-52-20+00
2023-12-01 14:42:28,847 - speechbrain.utils.epoch_loop - INFO - Going into epoch 18
2023-12-01 14:42:28,849 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 14:42:29,775 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 14:43:44,595 - speechbrain.utils.train_logger - INFO - steps: 82500, lr: 4.40e-04, avg_loss: 3.46, run_time: 1.30e+02
2023-12-01 14:45:27,786 - speechbrain.utils.train_logger - INFO - steps: 83000, lr: 4.39e-04, avg_loss: 3.46, run_time: 1.03e+02
2023-12-01 14:47:10,433 - speechbrain.utils.train_logger - INFO - steps: 83500, lr: 4.38e-04, avg_loss: 3.46, run_time: 1.03e+02
2023-12-01 14:48:53,446 - speechbrain.utils.train_logger - INFO - steps: 84000, lr: 4.36e-04, avg_loss: 3.45, run_time: 1.03e+02
2023-12-01 14:50:36,752 - speechbrain.utils.train_logger - INFO - steps: 84500, lr: 4.35e-04, avg_loss: 3.45, run_time: 1.03e+02
2023-12-01 14:52:20,265 - speechbrain.utils.train_logger - INFO - steps: 85000, lr: 4.34e-04, avg_loss: 3.45, run_time: 1.04e+02
2023-12-01 14:54:04,386 - speechbrain.utils.train_logger - INFO - steps: 85500, lr: 4.33e-04, avg_loss: 3.45, run_time: 1.04e+02
2023-12-01 14:55:51,087 - speechbrain.utils.train_logger - INFO - steps: 86000, lr: 4.31e-04, avg_loss: 3.45, run_time: 1.07e+02
2023-12-01 14:57:38,219 - speechbrain.utils.train_logger - INFO - steps: 86500, lr: 4.30e-04, avg_loss: 3.45, run_time: 1.07e+02
2023-12-01 14:59:44,476 - speechbrain.utils.train_logger - INFO - epoch: 18, steps: 86976, lr: 4.29e-04 - train loss: 3.46 - valid loss: 4.13, valid accuracy: 1.87e-01
2023-12-01 14:59:45,429 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+14-59-44+00
2023-12-01 14:59:45,618 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+12-09-11+00
2023-12-01 14:59:45,618 - speechbrain.utils.epoch_loop - INFO - Going into epoch 19
2023-12-01 14:59:45,620 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 14:59:46,397 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 14:59:53,207 - speechbrain.utils.train_logger - INFO - steps: 87000, lr: 4.29e-04, avg_loss: 3.46, run_time: 1.35e+02
2023-12-01 15:01:36,401 - speechbrain.utils.train_logger - INFO - steps: 87500, lr: 4.28e-04, avg_loss: 3.45, run_time: 1.03e+02
2023-12-01 15:03:19,508 - speechbrain.utils.train_logger - INFO - steps: 88000, lr: 4.26e-04, avg_loss: 3.44, run_time: 1.03e+02
2023-12-01 15:05:04,006 - speechbrain.utils.train_logger - INFO - steps: 88500, lr: 4.25e-04, avg_loss: 3.45, run_time: 1.05e+02
2023-12-01 15:06:47,251 - speechbrain.utils.train_logger - INFO - steps: 89000, lr: 4.24e-04, avg_loss: 3.45, run_time: 1.03e+02
2023-12-01 15:08:34,666 - speechbrain.utils.train_logger - INFO - steps: 89500, lr: 4.23e-04, avg_loss: 3.44, run_time: 1.07e+02
2023-12-01 15:10:17,535 - speechbrain.utils.train_logger - INFO - steps: 90000, lr: 4.22e-04, avg_loss: 3.44, run_time: 1.03e+02
2023-12-01 15:12:00,265 - speechbrain.utils.train_logger - INFO - steps: 90500, lr: 4.20e-04, avg_loss: 3.44, run_time: 1.03e+02
2023-12-01 15:13:44,195 - speechbrain.utils.train_logger - INFO - steps: 91000, lr: 4.19e-04, avg_loss: 3.44, run_time: 1.04e+02
2023-12-01 15:15:27,713 - speechbrain.utils.train_logger - INFO - steps: 91500, lr: 4.18e-04, avg_loss: 3.45, run_time: 1.04e+02
2023-12-01 15:16:57,732 - speechbrain.utils.train_logger - INFO - epoch: 19, steps: 91808, lr: 4.17e-04 - train loss: 3.45 - valid loss: 4.14, valid accuracy: 1.86e-01
2023-12-01 15:16:59,212 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+15-16-57+00
2023-12-01 15:16:59,434 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+12-26-01+00
2023-12-01 15:16:59,434 - speechbrain.utils.epoch_loop - INFO - Going into epoch 20
2023-12-01 15:16:59,436 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 15:17:00,176 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 15:17:41,580 - speechbrain.utils.train_logger - INFO - steps: 92000, lr: 4.17e-04, avg_loss: 3.42, run_time: 1.34e+02
2023-12-01 15:19:29,008 - speechbrain.utils.train_logger - INFO - steps: 92500, lr: 4.16e-04, avg_loss: 3.43, run_time: 1.07e+02
2023-12-01 15:21:12,513 - speechbrain.utils.train_logger - INFO - steps: 93000, lr: 4.15e-04, avg_loss: 3.43, run_time: 1.04e+02
2023-12-01 15:22:59,634 - speechbrain.utils.train_logger - INFO - steps: 93500, lr: 4.14e-04, avg_loss: 3.43, run_time: 1.07e+02
2023-12-01 15:24:47,238 - speechbrain.utils.train_logger - INFO - steps: 94000, lr: 4.13e-04, avg_loss: 3.44, run_time: 1.08e+02
2023-12-01 15:26:30,834 - speechbrain.utils.train_logger - INFO - steps: 94500, lr: 4.11e-04, avg_loss: 3.44, run_time: 1.04e+02
2023-12-01 15:28:13,964 - speechbrain.utils.train_logger - INFO - steps: 95000, lr: 4.10e-04, avg_loss: 3.44, run_time: 1.03e+02
2023-12-01 15:29:57,802 - speechbrain.utils.train_logger - INFO - steps: 95500, lr: 4.09e-04, avg_loss: 3.44, run_time: 1.04e+02
2023-12-01 15:31:43,807 - speechbrain.utils.train_logger - INFO - steps: 96000, lr: 4.08e-04, avg_loss: 3.44, run_time: 1.06e+02
2023-12-01 15:33:26,752 - speechbrain.utils.train_logger - INFO - steps: 96500, lr: 4.07e-04, avg_loss: 3.44, run_time: 1.03e+02
2023-12-01 15:34:21,211 - speechbrain.utils.train_logger - INFO - epoch: 20, steps: 96640, lr: 4.07e-04 - train loss: 3.44 - valid loss: 4.13, valid accuracy: 1.85e-01
2023-12-01 15:34:22,277 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+15-34-21+00
2023-12-01 15:34:22,515 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+12-42-57+00
2023-12-01 15:34:22,515 - speechbrain.utils.epoch_loop - INFO - Going into epoch 21
2023-12-01 15:34:22,517 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 15:34:23,264 - speechbrain.dataio.sampler - INFO - DynamicBatchSampler: Generating dynamic batches
2023-12-01 15:35:38,801 - speechbrain.utils.train_logger - INFO - steps: 97000, lr: 4.06e-04, avg_loss: 3.45, run_time: 1.32e+02
2023-12-01 15:37:21,522 - speechbrain.utils.train_logger - INFO - steps: 97500, lr: 4.05e-04, avg_loss: 3.43, run_time: 1.03e+02
2023-12-01 15:39:03,877 - speechbrain.utils.train_logger - INFO - steps: 98000, lr: 4.04e-04, avg_loss: 3.43, run_time: 1.02e+02
2023-12-01 15:40:46,740 - speechbrain.utils.train_logger - INFO - steps: 98500, lr: 4.03e-04, avg_loss: 3.42, run_time: 1.03e+02
2023-12-01 15:42:33,312 - speechbrain.utils.train_logger - INFO - steps: 99000, lr: 4.02e-04, avg_loss: 3.43, run_time: 1.07e+02
2023-12-01 15:44:16,192 - speechbrain.utils.train_logger - INFO - steps: 99500, lr: 4.01e-04, avg_loss: 3.43, run_time: 1.03e+02
2023-12-01 15:45:59,013 - speechbrain.utils.train_logger - INFO - steps: 100000, lr: 4.00e-04, avg_loss: 3.43, run_time: 1.03e+02
2023-12-01 15:47:41,863 - speechbrain.utils.train_logger - INFO - steps: 100500, lr: 3.99e-04, avg_loss: 3.43, run_time: 1.03e+02
2023-12-01 15:49:24,957 - speechbrain.utils.train_logger - INFO - steps: 101000, lr: 3.98e-04, avg_loss: 3.43, run_time: 1.03e+02
2023-12-01 15:51:27,361 - speechbrain.utils.train_logger - INFO - epoch: 21, steps: 101472, lr: 3.97e-04 - train loss: 3.43 - valid loss: 4.12, valid accuracy: 1.87e-01
2023-12-01 15:51:28,364 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+15-51-27+00
2023-12-01 15:51:28,544 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/best_hyperconformer/2000/save/CKPT+2023-12-01+12-59-57+00
