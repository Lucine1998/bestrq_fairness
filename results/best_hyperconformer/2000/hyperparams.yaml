# Generated 2023-12-01 from:
# /gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/bestrq/hparams/best_hyperbranchformer.yaml
# yamllint disable
# ############################################################################
# Model: SSL: Best-RQ HyperConformer
# Encoder: HyperConformer Encoder (w/Random Projection Quantizer)
# loss: Cross entropy
# Training: Librispeech 960h
# Authors:  Ryan Whetten
# ############################################################################
# Seed needs to be set at top of yaml, before objects with parameters are made

seed: 2000
__set_seed: !apply:torch.manual_seed [2000]

project_name: bestrq
experiment_name: hyper-branchformer-base
data_folder: /gpfsscratch/rech/nkp/uaj64gk/corpus/LibriSpeech/
output_folder: results/best_hyperconformer/2000
output_wer_folder: results/best_hyperconformer/2000/

ckpt_interval_min: 15
save_folder: results/best_hyperconformer/2000/save
# Logging file for every N optimizer steps (many lines)
train_steps_log: results/best_hyperconformer/2000/train_steps_log.txt
# Logging file per epoch
train_stage_log: results/best_hyperconformer/2000/train_stage_log.txt

train_splits: [train-clean-100, train-clean-360, train-other-500]
dev_splits: [dev-clean]
test_splits: [test-clean]
train_csv: results/bestrq/csv/train.csv
valid_csv: results/bestrq/csv/dev-clean.csv
skip_prep: true

avoid_if_longer_than: 60.0
avoid_if_shorter_than: 2.0
log_interval: 500 # Logging every N optimizer steps
auto_mix_prec: false
bfloat16_mix_prec: false
max_grad_norm: 1.

# Training parameters
# To make Transformers converge, the global bath size should be large enough.
# The global batch size is computed as batch_size * n_gpus * gradient_accumulation.
# Empirically, we found that this value should be >= 128.
# Please, set your parameters accordingly.

number_of_epochs: 21
optimizer_step_limit: 400000
# grad_accumulation_factor: 2


seconds_per_batch: 100 # Fits in a 32GB GPUs (V100)
train_num_buckets: 50

train_dataloader_options:
  num_workers: 4

test_dataloader_options:
  batch_size: 8   # DynamicBatching not used at testing time
  num_workers: 4

# Training parameters (based on Section 4.1.1 of Best-RQ paper)
lr: 0.0008
warmup: 25000

# Feature parameters
sample_rate: 16000
n_fft: 400
n_mels: 80
hop_length: 10
pad_to_divisible_by: 4

####################### Model parameters ###########################
# Transformer
d_model: 512
nhead: 8 # table 1 https://arxiv.org/pdf/2010.10504.pdf
num_encoder_layers: 12 # section 4.1.1
num_decoder_layers: 0
d_ffn: 2048
transformer_dropout: 0.1
activation: &id001 !name:torch.nn.GELU
output_neurons: 5000
encoder_layerdrop: 0.05
# specify 'hypermixing' for usage of multi-head HyperMixer instead of MultiHeadAttention
#Â You can also specify RelPosMHAXL for conformer
attention_type: hypermixing

# option 1) 'conformer' for HyperConformer; 
# option 2) 'transformer' for vanilla HyperMixer
# option 3) 'branchformer': HyperBranchformer
encoder_module: branchformer

# 15% masking
mask_length: 4 # for 400ms
mask_prob: 0.15
noise_mean: 0
noise_std: 0.1

# quantizer parameter
p_input: 320
cb_dim: 16
cb_vocab: 8192


############################## models ################################

CNN: &id003 !new:speechbrain.lobes.models.convolution.ConvolutionFrontEnd
  input_shape: (8, 10, 80)
  num_blocks: 2
  num_layers_per_block: 1
  out_channels: (128, 32)
  kernel_sizes: (3, 3)
  strides: (2, 2)
  residuals: (False, False)

Transformer: &id002 !new:speechbrain.lobes.models.transformer.TransformerASR.TransformerASR

  input_size: 640
  tgt_vocab: 5000
  d_model: 512
  nhead: 8
  num_encoder_layers: 12
  num_decoder_layers: 0
  d_ffn: 2048
  dropout: 0.1
  activation: *id001
  encoder_module: branchformer
  attention_type: hypermixing
  normalize_before: true
  causal: false
  layerdrop_prob: 0.05

# We must call an encoder wrapper so the decoder isn't run (we don't have any)
wrapper: &id004 !new:speechbrain.lobes.models.transformer.TransformerASR.EncoderWrapper
  transformer: *id002
Quantizer: &id005 !new:.quantiser.RandomProjectionQuantizer
    # projection
  input_dim: 320
    # codebook
  cb_dim: 16
  cb_vocab: 8192

linear: &id006 !new:speechbrain.nnet.linear.Linear

  input_size: 512
  n_neurons: 8192

modules:
  CNN: *id003
  wrapper: *id004
  Quantizer: *id005
  normalize: &id007 !new:speechbrain.processing.features.InputNormalization
    norm_type: global
    update_until_epoch: 4

############################## running ################################

# define two optimizers here for two-stage training
  linear: *id006
model: &id008 !new:torch.nn.ModuleList
- [*id003, *id004, *id005, *id006]
compute_features: !new:speechbrain.lobes.features.Fbank
  sample_rate: 16000
  n_fft: 400
  n_mels: 80

normalize: *id007
optimizer: !name:torch.optim.Adam
  lr: 0.0008
  betas: (0.9, 0.98)
  eps: 0.000000001

noam_annealing: &id009 !new:speechbrain.nnet.schedulers.NoamScheduler
  lr_initial: 0.0008
  n_warmup_steps: 25000

epoch_counter: &id010 !new:speechbrain.utils.epoch_loop.EpochCounter
  limit: 21

train_steps_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
  save_file: results/best_hyperconformer/2000/train_steps_log.txt

train_stage_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
  save_file: results/best_hyperconformer/2000/train_stage_log.txt

checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
  checkpoints_dir: results/best_hyperconformer/2000/save
  recoverables:
    model: *id008
    noam_scheduler: *id009
    normalizer: *id007
    counter: *id010
