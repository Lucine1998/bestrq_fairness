# Generated 2023-12-05 from:
# /gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/bestrq/wav2vec2/hparams/wav2vec2_base.yaml
# yamllint disable
# ################################
# Model: wav2vec2
# Authors: Rudolf A Braun 2022, Guillermo Cambara 2022, Titouan Parcollet 2022
# ################################

# data_folder: !PLACEHOLDER
data_folder: /corpus/LibriSpeech/
output_folder: results/wav2vec2-base
save_folder: results/wav2vec2-base/save
# Logging file for every N optimizer steps (many lines)
train_steps_log: results/wav2vec2-base/train_steps_log.txt
# Logging file per epoch
train_stage_log: results/wav2vec2-base/train_stage_log.txt

train_splits: [train-clean-100, train-clean-360, train-other-500]
dev_splits: [dev-clean]
test_splits: [test-clean]
train_csv: results/bestrq/csv/train.csv
valid_csv: results/bestrq/csv/dev-clean.csv
skip_prep: true

# train_csv: !ref <output_folder>/train.csv
# valid_csv: !ref <output_folder>/dev-clean.csv
# skip_prep: False

# avoid_if_longer_than: 30.0
# avoid_if_shorter_than: 1.5
avoid_if_longer_than: 60.0
avoid_if_shorter_than: 2.0
log_interval: 500 # Logging every N optimizer steps
auto_mix_prec: false
bfloat16_mix_prec: false
max_grad_norm: 100.
# grad_accumulation_factor: 4


# The training will either stops at number_of_epochs or optimizer_step_limit
# I.e. the first that is reached.
number_of_epochs: 42
optimizer_step_limit: 400000

# # Dynamic Batching parameters
# train_num_buckets: 70
# seconds_per_batch: 200 # Fits in a 32GB GPUs (V100)

seconds_per_batch: 100 # Fits in a 11GB GPUs
train_num_buckets: 50

train_dataloader_options:
  num_workers: 4

test_dataloader_options:
  batch_size: 8  # DynamicBatching not used at testing time
  num_workers: 4

# Training parameters
lr: 0.0005
warmup: 30000
# This is equivalent to optimizer_step_limit - warmup
# Necessary to do to have a linear warmup and linear decay directly
# If cooldown < optimizer_step_limit - warmup then a third step with a slower
# decay is applied in the middle (see the implementation of the scheduler)
cooldown: 370000

# Loss parameters
diversity_loss_weight: 0.1
mask_prob: 0.65
mask_length: 10
num_negatives: 100

# Model parameters
embedding_dim: 768
extractor_dim: 512
final_dim: 256
encoder_layerdrop: 0.05
latentextractor_kernels: &id001 [11, 3, 3, 3, 3, 3, 3]
latentextractor_strides: &id002 [5, 2, 2, 2, 2, 2, 2]

optimizer: !name:torch.optim.AdamW
  lr: 0.0005
  weight_decay: 0.01
  eps: 0.000001

epoch_counter: &id009 !new:speechbrain.utils.epoch_loop.EpochCounter

  limit: 42

extractor: &id004 !new:speechbrain.lobes.models.wav2vec.W2VLatentExtractor
  kernel_sizes: *id001
  strides: *id002
  out_channels: [512, 512, 512, 512, 512, 512, 512]

encoder: &id003 !new:speechbrain.lobes.models.transformer.Transformer.TransformerEncoder
  d_model: 768
  num_layers: 12
  nhead: 8
  d_ffn: 3072
  dropout: 0.1
  layerdrop_prob: 0.05
  normalize_before: true
  activation: !name:torch.nn.GELU

encoder_wrapper: &id005 !new:speechbrain.lobes.models.wav2vec.EncoderWrapper
  in_dim: 512
  embedding_dim: 768
  latent_encoder: *id003
  dropout_encoder_input: 0.1

target_quantiser: &id007 !new:speechbrain.lobes.models.wav2vec.W2VTargetQuantiser

  in_dim: 512
  out_dim: 256

feat_proj: &id006 !new:torch.nn.Linear
  in_features: 768
  out_features: 256

modules:
  latent_extractor: *id004
  latent_encoder: *id005
  feat_proj: *id006
  target_quantiser: *id007
loss: !new:speechbrain.nnet.losses.ContrastiveLoss
  logit_temp: 0.1

lr_scheduler: &id008 !new:speechbrain.nnet.schedulers.WarmCoolDecayLRSchedule
  lr: 0.0005
  warmup: 30000
  cooldown: 370000
  total_steps: 400000

checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
  checkpoints_dir: results/wav2vec2-base/save
  recoverables:
    latent_extractor: *id004
    latent_encoder: *id005
    feat_proj: *id006
    target_quantiser: *id007
    scheduler: *id008
    counter: *id009
train_steps_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
  save_file: results/wav2vec2-base/train_steps_log.txt

train_stage_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
  save_file: results/wav2vec2-base/train_stage_log.txt
