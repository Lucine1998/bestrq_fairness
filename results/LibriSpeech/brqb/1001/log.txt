2023-12-06 11:23:58,373 - speechbrain.core - INFO - Beginning experiment!
2023-12-06 11:23:58,373 - speechbrain.core - INFO - Experiment folder: results/LibriSpeech/brqb/1001
2023-12-06 11:23:59,228 - speechbrain.utils.superpowers - DEBUG - appdirs==1.4.4
attrs==23.1.0
black==19.10b0
certifi==2023.11.17
cfgv==3.4.0
charset-normalizer==3.3.2
click==8.0.4
cmake==3.27.7
distlib==0.3.7
entrypoints==0.3
filelock==3.13.1
flake8==3.7.9
fsspec==2023.10.0
huggingface-hub==0.19.4
HyperPyYAML==1.2.2
hypothesis==6.91.0
identify==2.5.32
idna==3.6
iniconfig==2.0.0
Jinja2==3.1.2
joblib==1.3.2
kenlm==0.2.0
lit==17.0.6
MarkupSafe==2.1.3
mccabe==0.6.1
mpmath==1.3.0
networkx==3.2.1
nodeenv==1.8.0
numpy==1.26.2
nvidia-cublas-cu11==11.10.3.66
nvidia-cublas-cu12==12.1.3.1
nvidia-cuda-cupti-cu11==11.7.101
nvidia-cuda-cupti-cu12==12.1.105
nvidia-cuda-nvrtc-cu11==11.7.99
nvidia-cuda-nvrtc-cu12==12.1.105
nvidia-cuda-runtime-cu11==11.7.99
nvidia-cuda-runtime-cu12==12.1.105
nvidia-cudnn-cu11==8.5.0.96
nvidia-cudnn-cu12==8.9.2.26
nvidia-cufft-cu11==10.9.0.58
nvidia-cufft-cu12==11.0.2.54
nvidia-curand-cu11==10.2.10.91
nvidia-curand-cu12==10.3.2.106
nvidia-cusolver-cu11==11.4.0.1
nvidia-cusolver-cu12==11.4.5.107
nvidia-cusparse-cu11==11.7.4.91
nvidia-cusparse-cu12==12.1.0.106
nvidia-nccl-cu11==2.14.3
nvidia-nccl-cu12==2.18.1
nvidia-nvjitlink-cu12==12.3.101
nvidia-nvtx-cu11==11.7.91
nvidia-nvtx-cu12==12.1.105
packaging==23.2
pandas==2.1.3
pathspec==0.11.2
platformdirs==4.0.0
pluggy==1.3.0
pre-commit==3.5.0
pycodestyle==2.5.0
pyctcdecode==0.5.0
pyflakes==2.1.1
pygtrie==2.5.0
pytest==7.4.0
python-dateutil==2.8.2
pytz==2023.3.post1
PyYAML==6.0.1
regex==2023.10.3
requests==2.31.0
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
safetensors==0.4.1
scipy==1.11.4
sentencepiece==0.1.99
six==1.16.0
sortedcontainers==2.4.0
-e git+https://github.com/whettenr/speechbrain.git@b545d914c7e1677cae6d8fe030a00f7594b78475#egg=speechbrain
sympy==1.12
tokenizers==0.15.0
toml==0.10.2
torch==2.0.1
torchaudio==2.0.2
tqdm==4.66.1
transformers==4.35.2
triton==2.0.0
typed-ast==1.5.5
typing_extensions==4.8.0
tzdata==2023.3
urllib3==2.1.0
virtualenv==20.24.7
yamllint==1.23.0


2023-12-06 11:23:59,241 - speechbrain.utils.superpowers - DEBUG - 3d4f47c


2023-12-06 11:24:00,209 - speechbrain.dataio.encoder - DEBUG - Would load categorical encoding from results/LibriSpeech/brqb/1001/save/label_encoder.txt, but file doesn't exist yet.
2023-12-06 11:24:00,722 - speechbrain.dataio.encoder - INFO - Moving label 'T' from index 0, because '<blank>' was put at its place.
2023-12-06 11:24:00,724 - speechbrain.dataio.encoder - INFO - Load called, but CTCTextEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.
2023-12-06 11:24:00,725 - speechbrain.dataio.encoder - DEBUG - Loaded categorical encoding from results/LibriSpeech/brqb/1001/save/label_encoder.txt
2023-12-06 11:24:00,725 - speechbrain.core - INFO - Info: auto_mix_prec arg from hparam file is used
2023-12-06 11:24:00,725 - speechbrain.core - INFO - Info: ckpt_interval_minutes arg from hparam file is used
2023-12-06 11:24:02,808 - speechbrain.core - INFO - 42.1M trainable parameters in ASR
2023-12-06 11:24:02,808 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results/LibriSpeech/brqb/1001/save.
2023-12-06 11:24:02,852 - speechbrain.pretrained.fetching - INFO - Destination model.ckpt: local file in /gpfswork/rech/nkp/uaj64gk/bestrqexp/jz/brqb/CKPT+2023-12-05+22-58-16+00/model.ckpt.
2023-12-06 11:24:02,853 - speechbrain.utils.parameter_transfer - INFO - Set local path in self.paths[weighted_ssl_model] = results/LibriSpeech/brqb/1001/save/weighted_ssl_model.ckpt
2023-12-06 11:24:02,855 - speechbrain.pretrained.fetching - INFO - Destination normalizer.ckpt: local file in /gpfswork/rech/nkp/uaj64gk/bestrqexp/jz/brqb/CKPT+2023-12-05+22-58-16+00/normalizer.ckpt.
2023-12-06 11:24:02,856 - speechbrain.utils.parameter_transfer - INFO - Set local path in self.paths[normalize] = results/LibriSpeech/brqb/1001/save/normalize.ckpt
2023-12-06 11:24:02,861 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: weighted_ssl_model, normalize
2023-12-06 11:24:02,861 - speechbrain.utils.parameter_transfer - INFO - Redirecting (loading from local path): results/LibriSpeech/brqb/1001/save/weighted_ssl_model.ckpt -> results/LibriSpeech/brqb/1001/save/weighted_ssl_model.ckpt
2023-12-06 11:24:02,861 - speechbrain.utils.parameter_transfer - INFO - Redirecting (loading from local path): results/LibriSpeech/brqb/1001/save/normalize.ckpt -> results/LibriSpeech/brqb/1001/save/normalize.ckpt
2023-12-06 11:24:04,677 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to ModuleList(
  (0): ConvolutionFrontEnd(
    (convblock_0): ConvBlock(
      (convs): Sequential(
        (conv_0): Conv2d(
          (conv): Conv2d(1, 128, kernel_size=(3, 3), stride=(2, 2))
        )
        (norm_0): LayerNorm(
          (norm): LayerNorm((40, 128), eps=1e-05, elementwise_affine=True)
        )
        (act_0): LeakyReLU(negative_slope=0.01)
        (dropout_0): Dropout(p=0.1, inplace=False)
      )
    )
    (convblock_1): ConvBlock(
      (convs): Sequential(
        (conv_0): Conv2d(
          (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(2, 2))
        )
        (norm_0): LayerNorm(
          (norm): LayerNorm((20, 32), eps=1e-05, elementwise_affine=True)
        )
        (act_0): LeakyReLU(negative_slope=0.01)
        (dropout_0): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (1): WeightedEncoderWrapper(
    (transformer): TransformerASR(
      (positional_encoding): RelPosEncXL()
      (positional_encoding_decoder): PositionalEncoding()
      (encoder): ConformerEncoder(
        (layers): ModuleList(
          (0-11): 12 x ConformerEncoderLayer(
            (mha_layer): RelPosMHAXL(
              (dropout_att): Dropout(p=0.1, inplace=False)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
              (linear_pos): Linear(in_features=512, out_features=512, bias=False)
            )
            (convolution_module): ConvolutionModule(
              (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (bottleneck): Sequential(
                (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
                (1): GLU(dim=1)
              )
              (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
              (after_conv): Sequential(
                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (1): Swish(
                  (sigmoid): Sigmoid()
                )
                (2): Linear(in_features=512, out_features=512, bias=True)
                (3): Dropout(p=0.1, inplace=False)
              )
            )
            (ffn_module1): Sequential(
              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (1): PositionalwiseFeedForward(
                (ffn): Sequential(
                  (0): Linear(in_features=512, out_features=2048, bias=True)
                  (1): Swish(
                    (sigmoid): Sigmoid()
                  )
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (2): Dropout(p=0.1, inplace=False)
            )
            (ffn_module2): Sequential(
              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (1): PositionalwiseFeedForward(
                (ffn): Sequential(
                  (0): Linear(in_features=512, out_features=2048, bias=True)
                  (1): Swish(
                    (sigmoid): Sigmoid()
                  )
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (2): Dropout(p=0.1, inplace=False)
            )
            (norm1): LayerNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
            (norm2): LayerNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
            (drop): Dropout(p=0.1, inplace=False)
          )
        )
        (norm): LayerNorm(
          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        )
      )
      (custom_src_module): ModuleList(
        (layers): ModuleList(
          (0): Linear(
            (w): Linear(in_features=640, out_features=512, bias=True)
          )
          (1): Dropout(p=0.1, inplace=False)
        )
      )
      (custom_tgt_module): ModuleList(
        (layers): ModuleList(
          (0): NormalizedEmbedding(
            (emb): Embedding(
              (Embedding): Embedding(5000, 512)
            )
          )
        )
      )
    )
  )
  (2): RandomProjectionQuantizer()
  (3): Linear(
    (w): Linear(in_features=512, out_features=8192, bias=True)
  )
) loading from results/LibriSpeech/brqb/1001/save/weighted_ssl_model.ckpt, the transferred parameters did not have parameters for the key: 1.weights
2023-12-06 11:24:04,680 - speechbrain.utils.checkpoints - INFO - Would load a checkpoint here, but none found yet.
2023-12-06 11:24:04,680 - speechbrain.utils.epoch_loop - INFO - Going into epoch 1
2023-12-06 11:25:16,284 - speechbrain.core - ERROR - Exception:
Traceback (most recent call last):
  File "/gpfswork/rech/nkp/uaj64gk/bestrqexp/benchmark/benchmarks/MP3S/LibriSpeech/LSTM/train_brq.py", line 376, in <module>
    asr_brain.fit(
  File "/gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain/speechbrain/core.py", line 1366, in fit
    self._fit_train(train_set=train_set, epoch=epoch, enable=enable)
  File "/gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain/speechbrain/core.py", line 1193, in _fit_train
    loss = self.fit_batch(batch)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfswork/rech/nkp/uaj64gk/bestrqexp/benchmark/benchmarks/MP3S/LibriSpeech/LSTM/train_brq.py", line 90, in fit_batch
    predictions = self.compute_forward(batch, sb.Stage.TRAIN)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfswork/rech/nkp/uaj64gk/bestrqexp/benchmark/benchmarks/MP3S/LibriSpeech/LSTM/train_brq.py", line 36, in compute_forward
    feats = self.modules.weighted_ssl_model[1](feats, wav_lens)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/linkhome/rech/genzjn01/uaj64gk/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain/speechbrain/lobes/models/transformer/TransformerASR.py", line 438, in forward
    _, hidden_states = self.transformer.encode(x, wav_lens, pad_idx)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain/speechbrain/lobes/models/transformer/TransformerASR.py", line 337, in encode
    encoder_out, _, hidden_state_lst = self.encoder(
                                       ^^^^^^^^^^^^^
  File "/linkhome/rech/genzjn01/uaj64gk/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain/speechbrain/lobes/models/transformer/Conformer.py", line 415, in forward
    output, attention = enc_layer(
                        ^^^^^^^^^^
  File "/linkhome/rech/genzjn01/uaj64gk/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain/speechbrain/lobes/models/transformer/Conformer.py", line 253, in forward
    x = x + 0.5 * self.ffn_module1(x)
                  ^^^^^^^^^^^^^^^^^^^
  File "/linkhome/rech/genzjn01/uaj64gk/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/linkhome/rech/genzjn01/uaj64gk/.local/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/linkhome/rech/genzjn01/uaj64gk/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain/speechbrain/nnet/attention.py", line 836, in forward
    x = self.ffn(x)
        ^^^^^^^^^^^
  File "/linkhome/rech/genzjn01/uaj64gk/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1494, in _call_impl
    def _call_impl(self, *args, **kwargs):
    
KeyboardInterrupt
2023-12-06 11:28:51,814 - speechbrain.core - INFO - Beginning experiment!
2023-12-06 11:28:51,821 - speechbrain.core - INFO - Experiment folder: results/LibriSpeech/brqb/1001
2023-12-06 11:28:57,108 - speechbrain.utils.superpowers - DEBUG - appdirs==1.4.4
attrs==23.1.0
black==19.10b0
certifi==2023.11.17
cfgv==3.4.0
charset-normalizer==3.3.2
click==8.0.4
cmake==3.27.7
distlib==0.3.7
entrypoints==0.3
filelock==3.13.1
flake8==3.7.9
fsspec==2023.10.0
huggingface-hub==0.19.4
HyperPyYAML==1.2.2
hypothesis==6.91.0
identify==2.5.32
idna==3.6
iniconfig==2.0.0
Jinja2==3.1.2
joblib==1.3.2
kenlm==0.2.0
lit==17.0.6
MarkupSafe==2.1.3
mccabe==0.6.1
mpmath==1.3.0
networkx==3.2.1
nodeenv==1.8.0
numpy==1.26.2
nvidia-cublas-cu11==11.10.3.66
nvidia-cublas-cu12==12.1.3.1
nvidia-cuda-cupti-cu11==11.7.101
nvidia-cuda-cupti-cu12==12.1.105
nvidia-cuda-nvrtc-cu11==11.7.99
nvidia-cuda-nvrtc-cu12==12.1.105
nvidia-cuda-runtime-cu11==11.7.99
nvidia-cuda-runtime-cu12==12.1.105
nvidia-cudnn-cu11==8.5.0.96
nvidia-cudnn-cu12==8.9.2.26
nvidia-cufft-cu11==10.9.0.58
nvidia-cufft-cu12==11.0.2.54
nvidia-curand-cu11==10.2.10.91
nvidia-curand-cu12==10.3.2.106
nvidia-cusolver-cu11==11.4.0.1
nvidia-cusolver-cu12==11.4.5.107
nvidia-cusparse-cu11==11.7.4.91
nvidia-cusparse-cu12==12.1.0.106
nvidia-nccl-cu11==2.14.3
nvidia-nccl-cu12==2.18.1
nvidia-nvjitlink-cu12==12.3.101
nvidia-nvtx-cu11==11.7.91
nvidia-nvtx-cu12==12.1.105
packaging==23.2
pandas==2.1.3
pathspec==0.11.2
platformdirs==4.0.0
pluggy==1.3.0
pre-commit==3.5.0
pycodestyle==2.5.0
pyctcdecode==0.5.0
pyflakes==2.1.1
pygtrie==2.5.0
pytest==7.4.0
python-dateutil==2.8.2
pytz==2023.3.post1
PyYAML==6.0.1
regex==2023.10.3
requests==2.31.0
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
safetensors==0.4.1
scipy==1.11.4
sentencepiece==0.1.99
six==1.16.0
sortedcontainers==2.4.0
-e /gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain
sympy==1.12
tokenizers==0.15.0
toml==0.10.2
torch==2.0.1
torchaudio==2.0.2
tqdm==4.66.1
transformers==4.35.2
triton==2.0.0
typed-ast==1.5.5
typing_extensions==4.8.0
tzdata==2023.3
urllib3==2.1.0
virtualenv==20.24.7
yamllint==1.23.0

ERROR: Error [Errno 2] No such file or directory: 'git' while executing command git config --get-regexp 'remote\..*\.url'
WARNING: cannot determine version of editable source in /gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain (git command not found in path)

2023-12-06 11:28:57,795 - speechbrain.dataio.encoder - DEBUG - Would load categorical encoding from results/LibriSpeech/brqb/1001/save/label_encoder.txt, but file doesn't exist yet.
2023-12-06 11:28:58,596 - speechbrain.dataio.encoder - INFO - Moving label 'T' from index 0, because '<blank>' was put at its place.
2023-12-06 11:28:58,597 - speechbrain.dataio.encoder - INFO - Load called, but CTCTextEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.
2023-12-06 11:28:58,598 - speechbrain.dataio.encoder - DEBUG - Loaded categorical encoding from results/LibriSpeech/brqb/1001/save/label_encoder.txt
2023-12-06 11:28:58,598 - speechbrain.core - INFO - Info: auto_mix_prec arg from hparam file is used
2023-12-06 11:28:58,598 - speechbrain.core - INFO - Info: ckpt_interval_minutes arg from hparam file is used
2023-12-06 11:29:03,146 - speechbrain.core - INFO - 42.1M trainable parameters in ASR
2023-12-06 11:29:03,298 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results/LibriSpeech/brqb/1001/save.
2023-12-06 11:29:03,331 - speechbrain.pretrained.fetching - INFO - Destination model.ckpt: local file in /gpfswork/rech/nkp/uaj64gk/bestrqexp/jz/brqb/CKPT+2023-12-05+22-58-16+00/model.ckpt.
2023-12-06 11:29:03,333 - speechbrain.utils.parameter_transfer - INFO - Set local path in self.paths[weighted_ssl_model] = results/LibriSpeech/brqb/1001/save/weighted_ssl_model.ckpt
2023-12-06 11:29:03,333 - speechbrain.pretrained.fetching - INFO - Destination normalizer.ckpt: local file in /gpfswork/rech/nkp/uaj64gk/bestrqexp/jz/brqb/CKPT+2023-12-05+22-58-16+00/normalizer.ckpt.
2023-12-06 11:29:03,335 - speechbrain.utils.parameter_transfer - INFO - Set local path in self.paths[normalize] = results/LibriSpeech/brqb/1001/save/normalize.ckpt
2023-12-06 11:29:03,336 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: weighted_ssl_model, normalize
2023-12-06 11:29:03,336 - speechbrain.utils.parameter_transfer - INFO - Redirecting (loading from local path): results/LibriSpeech/brqb/1001/save/weighted_ssl_model.ckpt -> results/LibriSpeech/brqb/1001/save/weighted_ssl_model.ckpt
2023-12-06 11:29:03,336 - speechbrain.utils.parameter_transfer - INFO - Redirecting (loading from local path): results/LibriSpeech/brqb/1001/save/normalize.ckpt -> results/LibriSpeech/brqb/1001/save/normalize.ckpt
2023-12-06 11:29:05,034 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to ModuleList(
  (0): ConvolutionFrontEnd(
    (convblock_0): ConvBlock(
      (convs): Sequential(
        (conv_0): Conv2d(
          (conv): Conv2d(1, 128, kernel_size=(3, 3), stride=(2, 2))
        )
        (norm_0): LayerNorm(
          (norm): LayerNorm((40, 128), eps=1e-05, elementwise_affine=True)
        )
        (act_0): LeakyReLU(negative_slope=0.01)
        (dropout_0): Dropout(p=0.1, inplace=False)
      )
    )
    (convblock_1): ConvBlock(
      (convs): Sequential(
        (conv_0): Conv2d(
          (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(2, 2))
        )
        (norm_0): LayerNorm(
          (norm): LayerNorm((20, 32), eps=1e-05, elementwise_affine=True)
        )
        (act_0): LeakyReLU(negative_slope=0.01)
        (dropout_0): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (1): WeightedEncoderWrapper(
    (transformer): TransformerASR(
      (positional_encoding): RelPosEncXL()
      (positional_encoding_decoder): PositionalEncoding()
      (encoder): ConformerEncoder(
        (layers): ModuleList(
          (0-11): 12 x ConformerEncoderLayer(
            (mha_layer): RelPosMHAXL(
              (dropout_att): Dropout(p=0.1, inplace=False)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
              (linear_pos): Linear(in_features=512, out_features=512, bias=False)
            )
            (convolution_module): ConvolutionModule(
              (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (bottleneck): Sequential(
                (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
                (1): GLU(dim=1)
              )
              (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
              (after_conv): Sequential(
                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (1): Swish(
                  (sigmoid): Sigmoid()
                )
                (2): Linear(in_features=512, out_features=512, bias=True)
                (3): Dropout(p=0.1, inplace=False)
              )
            )
            (ffn_module1): Sequential(
              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (1): PositionalwiseFeedForward(
                (ffn): Sequential(
                  (0): Linear(in_features=512, out_features=2048, bias=True)
                  (1): Swish(
                    (sigmoid): Sigmoid()
                  )
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (2): Dropout(p=0.1, inplace=False)
            )
            (ffn_module2): Sequential(
              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (1): PositionalwiseFeedForward(
                (ffn): Sequential(
                  (0): Linear(in_features=512, out_features=2048, bias=True)
                  (1): Swish(
                    (sigmoid): Sigmoid()
                  )
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (2): Dropout(p=0.1, inplace=False)
            )
            (norm1): LayerNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
            (norm2): LayerNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
            (drop): Dropout(p=0.1, inplace=False)
          )
        )
        (norm): LayerNorm(
          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        )
      )
      (custom_src_module): ModuleList(
        (layers): ModuleList(
          (0): Linear(
            (w): Linear(in_features=640, out_features=512, bias=True)
          )
          (1): Dropout(p=0.1, inplace=False)
        )
      )
      (custom_tgt_module): ModuleList(
        (layers): ModuleList(
          (0): NormalizedEmbedding(
            (emb): Embedding(
              (Embedding): Embedding(5000, 512)
            )
          )
        )
      )
    )
  )
  (2): RandomProjectionQuantizer()
  (3): Linear(
    (w): Linear(in_features=512, out_features=8192, bias=True)
  )
) loading from results/LibriSpeech/brqb/1001/save/weighted_ssl_model.ckpt, the transferred parameters did not have parameters for the key: 1.weights
2023-12-06 11:29:05,041 - speechbrain.utils.checkpoints - INFO - Would load a checkpoint here, but none found yet.
2023-12-06 11:29:05,042 - speechbrain.utils.epoch_loop - INFO - Going into epoch 1
2023-12-06 11:52:26,840 - speechbrain.utils.train_logger - INFO - epoch: 1, lr_model: 2.00e-04 - train loss: 5.42e-01 - valid loss: 3.33e-01, valid CER: 8.08, valid WER: 27.91
2023-12-06 11:52:29,297 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+11-52-26+00
2023-12-06 11:52:29,301 - speechbrain.utils.epoch_loop - INFO - Going into epoch 2
2023-12-06 12:15:29,356 - speechbrain.utils.train_logger - INFO - epoch: 2, lr_model: 2.00e-04 - train loss: 2.77e-01 - valid loss: 2.83e-01, valid CER: 6.60, valid WER: 23.03
2023-12-06 12:16:37,109 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+12-15-29+00
2023-12-06 12:16:37,310 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+11-52-26+00
2023-12-06 12:16:37,310 - speechbrain.utils.epoch_loop - INFO - Going into epoch 3
2023-12-06 12:39:18,029 - speechbrain.utils.train_logger - INFO - epoch: 3, lr_model: 2.00e-04 - train loss: 2.18e-01 - valid loss: 2.62e-01, valid CER: 6.17, valid WER: 21.57
2023-12-06 12:39:18,958 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+12-39-18+00
2023-12-06 12:39:18,997 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+12-15-29+00
2023-12-06 12:39:18,997 - speechbrain.utils.epoch_loop - INFO - Going into epoch 4
2023-12-06 13:01:54,141 - speechbrain.utils.train_logger - INFO - epoch: 4, lr_model: 2.00e-04 - train loss: 1.81e-01 - valid loss: 2.56e-01, valid CER: 5.77, valid WER: 20.06
2023-12-06 13:01:55,079 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+13-01-54+00
2023-12-06 13:01:55,096 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+12-39-18+00
2023-12-06 13:01:55,096 - speechbrain.utils.epoch_loop - INFO - Going into epoch 5
2023-12-06 13:24:30,087 - speechbrain.utils.train_logger - INFO - epoch: 5, lr_model: 2.00e-04 - train loss: 1.57e-01 - valid loss: 2.55e-01, valid CER: 5.94, valid WER: 20.40
2023-12-06 13:24:31,422 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+13-24-30+00
2023-12-06 13:24:31,437 - speechbrain.utils.epoch_loop - INFO - Going into epoch 6
2023-12-06 13:47:05,641 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.0002 to 0.00016
2023-12-06 13:47:05,641 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.01 to 0.009
2023-12-06 13:47:05,641 - speechbrain.utils.train_logger - INFO - epoch: 6, lr_model: 2.00e-04 - train loss: 1.38e-01 - valid loss: 2.66e-01, valid CER: 5.86, valid WER: 19.98
2023-12-06 13:47:06,928 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+13-47-05+00
2023-12-06 13:47:06,974 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+13-24-30+00
2023-12-06 13:47:06,978 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+13-01-54+00
2023-12-06 13:47:06,978 - speechbrain.utils.epoch_loop - INFO - Going into epoch 7
2023-12-06 14:10:03,416 - speechbrain.utils.train_logger - INFO - epoch: 7, lr_model: 1.60e-04 - train loss: 1.13e-01 - valid loss: 2.49e-01, valid CER: 5.34, valid WER: 18.52
2023-12-06 14:10:04,926 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+14-10-03+00
2023-12-06 14:10:04,974 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+13-47-05+00
2023-12-06 14:10:04,974 - speechbrain.utils.epoch_loop - INFO - Going into epoch 8
2023-12-06 14:33:01,579 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.00016 to 0.00013
2023-12-06 14:33:01,580 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.009 to 0.0081
2023-12-06 14:33:01,580 - speechbrain.utils.train_logger - INFO - epoch: 8, lr_model: 1.60e-04 - train loss: 9.97e-02 - valid loss: 2.56e-01, valid CER: 5.37, valid WER: 18.51
2023-12-06 14:33:02,717 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+14-33-01+00
2023-12-06 14:33:02,747 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+14-10-03+00
2023-12-06 14:33:02,747 - speechbrain.utils.epoch_loop - INFO - Going into epoch 9
2023-12-06 14:55:59,980 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.00013 to 0.0001
2023-12-06 14:55:59,981 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.0081 to 0.0073
2023-12-06 14:55:59,981 - speechbrain.utils.train_logger - INFO - epoch: 9, lr_model: 1.28e-04 - train loss: 8.24e-02 - valid loss: 2.59e-01, valid CER: 5.15, valid WER: 17.70
2023-12-06 14:56:00,885 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+14-55-59+00
2023-12-06 14:56:00,920 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+14-33-01+00
2023-12-06 14:56:00,921 - speechbrain.utils.epoch_loop - INFO - Going into epoch 10
2023-12-06 15:19:03,060 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.0001 to 8.2e-05
2023-12-06 15:19:03,060 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.0073 to 0.0066
2023-12-06 15:19:03,060 - speechbrain.utils.train_logger - INFO - epoch: 10, lr_model: 1.02e-04 - train loss: 6.56e-02 - valid loss: 2.67e-01, valid CER: 4.99, valid WER: 17.16
2023-12-06 15:19:04,248 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+15-19-03+00
2023-12-06 15:19:04,333 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+14-55-59+00
2023-12-06 15:19:04,333 - speechbrain.utils.epoch_loop - INFO - Going into epoch 11
2023-12-06 15:42:00,541 - speechbrain.nnet.schedulers - INFO - Changing lr from 8.2e-05 to 6.6e-05
2023-12-06 15:42:00,559 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.0066 to 0.0059
2023-12-06 15:42:00,560 - speechbrain.utils.train_logger - INFO - epoch: 11, lr_model: 8.19e-05 - train loss: 5.30e-02 - valid loss: 2.71e-01, valid CER: 4.87, valid WER: 16.70
2023-12-06 15:42:53,396 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+15-42-00+00
2023-12-06 15:42:53,947 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+15-19-03+00
2023-12-06 15:42:53,947 - speechbrain.utils.epoch_loop - INFO - Going into epoch 12
2023-12-06 16:05:36,851 - speechbrain.nnet.schedulers - INFO - Changing lr from 6.6e-05 to 5.2e-05
2023-12-06 16:05:36,873 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.0059 to 0.0053
2023-12-06 16:05:36,874 - speechbrain.utils.train_logger - INFO - epoch: 12, lr_model: 6.55e-05 - train loss: 4.41e-02 - valid loss: 2.70e-01, valid CER: 4.87, valid WER: 16.60
2023-12-06 16:05:38,133 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+16-05-36+00
2023-12-06 16:05:38,311 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+15-42-00+00
2023-12-06 16:05:38,311 - speechbrain.utils.epoch_loop - INFO - Going into epoch 13
2023-12-06 16:28:16,926 - speechbrain.nnet.schedulers - INFO - Changing lr from 5.2e-05 to 4.2e-05
2023-12-06 16:28:16,937 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.0053 to 0.0048
2023-12-06 16:28:16,938 - speechbrain.utils.train_logger - INFO - epoch: 13, lr_model: 5.24e-05 - train loss: 3.70e-02 - valid loss: 2.80e-01, valid CER: 4.76, valid WER: 16.24
2023-12-06 16:28:18,007 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+16-28-16+00
2023-12-06 16:28:18,070 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+16-05-36+00
2023-12-06 16:28:18,070 - speechbrain.utils.epoch_loop - INFO - Going into epoch 14
2023-12-06 16:50:54,394 - speechbrain.nnet.schedulers - INFO - Changing lr from 4.2e-05 to 3.4e-05
2023-12-06 16:50:54,419 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.0048 to 0.0043
2023-12-06 16:50:54,420 - speechbrain.utils.train_logger - INFO - epoch: 14, lr_model: 4.19e-05 - train loss: 3.11e-02 - valid loss: 2.84e-01, valid CER: 4.76, valid WER: 16.36
2023-12-06 16:50:57,398 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+16-50-54+00
2023-12-06 16:50:57,434 - speechbrain.utils.epoch_loop - INFO - Going into epoch 15
2023-12-06 17:13:28,523 - speechbrain.nnet.schedulers - INFO - Changing lr from 3.4e-05 to 2.7e-05
2023-12-06 17:13:28,544 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.0043 to 0.0039
2023-12-06 17:13:28,591 - speechbrain.utils.train_logger - INFO - epoch: 15, lr_model: 3.36e-05 - train loss: 2.68e-02 - valid loss: 2.85e-01, valid CER: 4.71, valid WER: 16.02
2023-12-06 17:13:30,711 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+17-13-28+00
2023-12-06 17:13:30,777 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+16-28-16+00
2023-12-06 17:13:30,806 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+16-50-54+00
2023-12-06 17:13:30,806 - speechbrain.utils.epoch_loop - INFO - Going into epoch 16
2023-12-06 17:35:58,173 - speechbrain.nnet.schedulers - INFO - Changing lr from 2.7e-05 to 2.1e-05
2023-12-06 17:35:58,195 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.0039 to 0.0035
2023-12-06 17:35:58,195 - speechbrain.utils.train_logger - INFO - epoch: 16, lr_model: 2.68e-05 - train loss: 2.35e-02 - valid loss: 2.90e-01, valid CER: 4.69, valid WER: 15.91
2023-12-06 17:35:59,306 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+17-35-58+00
2023-12-06 17:35:59,352 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+17-13-28+00
2023-12-06 17:35:59,352 - speechbrain.utils.epoch_loop - INFO - Going into epoch 17
2023-12-06 17:58:25,028 - speechbrain.nnet.schedulers - INFO - Changing lr from 2.1e-05 to 1.7e-05
2023-12-06 17:58:25,047 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.0035 to 0.0031
2023-12-06 17:58:25,088 - speechbrain.utils.train_logger - INFO - epoch: 17, lr_model: 2.15e-05 - train loss: 2.08e-02 - valid loss: 2.95e-01, valid CER: 4.67, valid WER: 15.91
2023-12-06 17:58:28,510 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+17-58-25+00
2023-12-06 17:58:28,566 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+17-35-58+00
2023-12-06 17:58:28,566 - speechbrain.utils.epoch_loop - INFO - Going into epoch 18
2023-12-06 18:20:57,391 - speechbrain.nnet.schedulers - INFO - Changing lr from 1.7e-05 to 1.4e-05
2023-12-06 18:20:57,405 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.0031 to 0.0028
2023-12-06 18:20:57,405 - speechbrain.utils.train_logger - INFO - epoch: 18, lr_model: 1.72e-05 - train loss: 1.90e-02 - valid loss: 2.97e-01, valid CER: 4.66, valid WER: 15.79
2023-12-06 18:20:58,433 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+18-20-57+00
2023-12-06 18:20:58,498 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+17-58-25+00
2023-12-06 18:20:58,498 - speechbrain.utils.epoch_loop - INFO - Going into epoch 19
2023-12-06 18:43:24,996 - speechbrain.nnet.schedulers - INFO - Changing lr from 1.4e-05 to 1.1e-05
2023-12-06 18:43:25,014 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.0028 to 0.0025
2023-12-06 18:43:25,015 - speechbrain.utils.train_logger - INFO - epoch: 19, lr_model: 1.37e-05 - train loss: 1.75e-02 - valid loss: 3.00e-01, valid CER: 4.67, valid WER: 15.86
2023-12-06 18:43:41,698 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+18-43-25+00
2023-12-06 18:43:41,758 - speechbrain.utils.epoch_loop - INFO - Going into epoch 20
2023-12-06 19:06:11,882 - speechbrain.nnet.schedulers - INFO - Changing lr from 1.1e-05 to 8.8e-06
2023-12-06 19:06:11,910 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.0025 to 0.0023
2023-12-06 19:06:11,958 - speechbrain.utils.train_logger - INFO - epoch: 20, lr_model: 1.10e-05 - train loss: 1.61e-02 - valid loss: 3.02e-01, valid CER: 4.65, valid WER: 15.78
2023-12-06 19:06:12,988 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+19-06-11+00
2023-12-06 19:06:13,098 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+18-43-25+00
2023-12-06 19:06:13,158 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+18-20-57+00
2023-12-06 19:06:13,159 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+19-06-11+00
2023-12-06 19:06:13,970 - speechbrain.dataio.encoder - DEBUG - Loaded categorical encoding from results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+19-06-11+00/tokenizer.ckpt
2023-12-06 19:06:13,972 - root - DEBUG - SaveableDataLoader was requested to load a checkpoint, but the DataLoader has already been iterated. The DataLoader file will be ignored. This is normal in evaluation, when a checkpoint is loaded just to retrieve the best model.
2023-12-06 19:07:18,508 - speechbrain.utils.train_logger - INFO - Epoch loaded: 20 - test loss: 3.12e-01, test CER: 4.44, test WER: 15.11
2023-12-06 19:07:40,864 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+19-06-11+00
2023-12-06 19:07:43,194 - speechbrain.dataio.encoder - DEBUG - Loaded categorical encoding from results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+19-06-11+00/tokenizer.ckpt
2023-12-06 19:07:43,195 - root - DEBUG - SaveableDataLoader was requested to load a checkpoint, but the DataLoader has already been iterated. The DataLoader file will be ignored. This is normal in evaluation, when a checkpoint is loaded just to retrieve the best model.
2023-12-06 19:08:48,685 - speechbrain.utils.train_logger - INFO - Epoch loaded: 20 - test loss: 8.79e-01, test CER: 12.32, test WER: 34.06
2023-12-07 09:39:31,869 - speechbrain.core - INFO - Beginning experiment!
2023-12-07 09:39:31,888 - speechbrain.core - INFO - Experiment folder: results/LibriSpeech/brqb/1001
2023-12-07 09:39:32,492 - speechbrain.utils.superpowers - DEBUG - appdirs==1.4.4
attrs==23.1.0
black==19.10b0
certifi==2023.11.17
cfgv==3.4.0
charset-normalizer==3.3.2
click==8.0.4
cmake==3.27.7
distlib==0.3.7
entrypoints==0.3
filelock==3.13.1
flake8==3.7.9
fsspec==2023.10.0
huggingface-hub==0.19.4
HyperPyYAML==1.2.2
hypothesis==6.91.0
identify==2.5.32
idna==3.6
iniconfig==2.0.0
Jinja2==3.1.2
joblib==1.3.2
kenlm==0.2.0
lit==17.0.6
MarkupSafe==2.1.3
mccabe==0.6.1
mpmath==1.3.0
networkx==3.2.1
nodeenv==1.8.0
numpy==1.26.2
nvidia-cublas-cu11==11.10.3.66
nvidia-cublas-cu12==12.1.3.1
nvidia-cuda-cupti-cu11==11.7.101
nvidia-cuda-cupti-cu12==12.1.105
nvidia-cuda-nvrtc-cu11==11.7.99
nvidia-cuda-nvrtc-cu12==12.1.105
nvidia-cuda-runtime-cu11==11.7.99
nvidia-cuda-runtime-cu12==12.1.105
nvidia-cudnn-cu11==8.5.0.96
nvidia-cudnn-cu12==8.9.2.26
nvidia-cufft-cu11==10.9.0.58
nvidia-cufft-cu12==11.0.2.54
nvidia-curand-cu11==10.2.10.91
nvidia-curand-cu12==10.3.2.106
nvidia-cusolver-cu11==11.4.0.1
nvidia-cusolver-cu12==11.4.5.107
nvidia-cusparse-cu11==11.7.4.91
nvidia-cusparse-cu12==12.1.0.106
nvidia-nccl-cu11==2.14.3
nvidia-nccl-cu12==2.18.1
nvidia-nvjitlink-cu12==12.3.101
nvidia-nvtx-cu11==11.7.91
nvidia-nvtx-cu12==12.1.105
packaging==23.2
pandas==2.1.3
pathspec==0.11.2
platformdirs==4.0.0
pluggy==1.3.0
pre-commit==3.5.0
pycodestyle==2.5.0
pyctcdecode==0.5.0
pyflakes==2.1.1
pygtrie==2.5.0
pytest==7.4.0
python-dateutil==2.8.2
pytz==2023.3.post1
PyYAML==6.0.1
regex==2023.10.3
requests==2.31.0
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
safetensors==0.4.1
scipy==1.11.4
sentencepiece==0.1.99
six==1.16.0
sortedcontainers==2.4.0
-e git+https://github.com/whettenr/speechbrain.git@b545d914c7e1677cae6d8fe030a00f7594b78475#egg=speechbrain
sympy==1.12
tokenizers==0.15.0
toml==0.10.2
torch==2.0.1
torchaudio==2.0.2
tqdm==4.66.1
transformers==4.35.2
triton==2.0.0
typed-ast==1.5.5
typing_extensions==4.8.0
tzdata==2023.3
urllib3==2.1.0
virtualenv==20.24.7
yamllint==1.23.0


2023-12-07 09:39:32,506 - speechbrain.utils.superpowers - DEBUG - a32be66


2023-12-07 09:39:32,892 - speechbrain.dataio.encoder - DEBUG - Loaded categorical encoding from results/LibriSpeech/brqb/1001/save/label_encoder.txt
2023-12-07 09:39:32,892 - speechbrain.dataio.encoder - INFO - Load called, but CTCTextEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.
2023-12-07 09:39:32,892 - speechbrain.dataio.encoder - DEBUG - Loaded categorical encoding from results/LibriSpeech/brqb/1001/save/label_encoder.txt
2023-12-07 09:40:48,737 - pyctcdecode.decoder - WARNING - Unigrams not provided and cannot be automatically determined from LM file (only arpa format). Decoding accuracy might be reduced.
2023-12-07 09:40:48,738 - pyctcdecode.alphabet - INFO - Alphabet determined to be of regular style.
2023-12-07 09:40:48,738 - pyctcdecode.language_model - WARNING - No known unigrams provided, decoding results might be a lot worse.
2023-12-07 09:40:48,738 - speechbrain.core - INFO - Info: auto_mix_prec arg from hparam file is used
2023-12-07 09:40:48,738 - speechbrain.core - INFO - Info: ckpt_interval_minutes arg from hparam file is used
2023-12-07 09:40:51,619 - speechbrain.core - INFO - 42.1M trainable parameters in ASR
2023-12-07 09:40:51,620 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results/LibriSpeech/brqb/1001/save.
2023-12-07 09:40:51,661 - speechbrain.pretrained.fetching - INFO - Fetch model.ckpt: Using existing file/symlink in results/LibriSpeech/brqb/1001/save/weighted_ssl_model.ckpt.
2023-12-07 09:40:51,661 - speechbrain.utils.parameter_transfer - INFO - Set local path in self.paths[weighted_ssl_model] = results/LibriSpeech/brqb/1001/save/weighted_ssl_model.ckpt
2023-12-07 09:40:51,665 - speechbrain.pretrained.fetching - INFO - Fetch normalizer.ckpt: Using existing file/symlink in results/LibriSpeech/brqb/1001/save/normalize.ckpt.
2023-12-07 09:40:51,665 - speechbrain.utils.parameter_transfer - INFO - Set local path in self.paths[normalize] = results/LibriSpeech/brqb/1001/save/normalize.ckpt
2023-12-07 09:40:51,665 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: weighted_ssl_model, normalize
2023-12-07 09:40:51,665 - speechbrain.utils.parameter_transfer - INFO - Redirecting (loading from local path): results/LibriSpeech/brqb/1001/save/weighted_ssl_model.ckpt -> results/LibriSpeech/brqb/1001/save/weighted_ssl_model.ckpt
2023-12-07 09:40:51,665 - speechbrain.utils.parameter_transfer - INFO - Redirecting (loading from local path): results/LibriSpeech/brqb/1001/save/normalize.ckpt -> results/LibriSpeech/brqb/1001/save/normalize.ckpt
2023-12-07 09:40:53,624 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to ModuleList(
  (0): ConvolutionFrontEnd(
    (convblock_0): ConvBlock(
      (convs): Sequential(
        (conv_0): Conv2d(
          (conv): Conv2d(1, 128, kernel_size=(3, 3), stride=(2, 2))
        )
        (norm_0): LayerNorm(
          (norm): LayerNorm((40, 128), eps=1e-05, elementwise_affine=True)
        )
        (act_0): LeakyReLU(negative_slope=0.01)
        (dropout_0): Dropout(p=0.1, inplace=False)
      )
    )
    (convblock_1): ConvBlock(
      (convs): Sequential(
        (conv_0): Conv2d(
          (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(2, 2))
        )
        (norm_0): LayerNorm(
          (norm): LayerNorm((20, 32), eps=1e-05, elementwise_affine=True)
        )
        (act_0): LeakyReLU(negative_slope=0.01)
        (dropout_0): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (1): WeightedEncoderWrapper(
    (transformer): TransformerASR(
      (positional_encoding): RelPosEncXL()
      (positional_encoding_decoder): PositionalEncoding()
      (encoder): ConformerEncoder(
        (layers): ModuleList(
          (0-11): 12 x ConformerEncoderLayer(
            (mha_layer): RelPosMHAXL(
              (dropout_att): Dropout(p=0.1, inplace=False)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
              (linear_pos): Linear(in_features=512, out_features=512, bias=False)
            )
            (convolution_module): ConvolutionModule(
              (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (bottleneck): Sequential(
                (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
                (1): GLU(dim=1)
              )
              (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
              (after_conv): Sequential(
                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (1): Swish(
                  (sigmoid): Sigmoid()
                )
                (2): Linear(in_features=512, out_features=512, bias=True)
                (3): Dropout(p=0.1, inplace=False)
              )
            )
            (ffn_module1): Sequential(
              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (1): PositionalwiseFeedForward(
                (ffn): Sequential(
                  (0): Linear(in_features=512, out_features=2048, bias=True)
                  (1): Swish(
                    (sigmoid): Sigmoid()
                  )
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (2): Dropout(p=0.1, inplace=False)
            )
            (ffn_module2): Sequential(
              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (1): PositionalwiseFeedForward(
                (ffn): Sequential(
                  (0): Linear(in_features=512, out_features=2048, bias=True)
                  (1): Swish(
                    (sigmoid): Sigmoid()
                  )
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (2): Dropout(p=0.1, inplace=False)
            )
            (norm1): LayerNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
            (norm2): LayerNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
            (drop): Dropout(p=0.1, inplace=False)
          )
        )
        (norm): LayerNorm(
          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        )
      )
      (custom_src_module): ModuleList(
        (layers): ModuleList(
          (0): Linear(
            (w): Linear(in_features=640, out_features=512, bias=True)
          )
          (1): Dropout(p=0.1, inplace=False)
        )
      )
      (custom_tgt_module): ModuleList(
        (layers): ModuleList(
          (0): NormalizedEmbedding(
            (emb): Embedding(
              (Embedding): Embedding(5000, 512)
            )
          )
        )
      )
    )
  )
  (2): RandomProjectionQuantizer()
  (3): Linear(
    (w): Linear(in_features=512, out_features=8192, bias=True)
  )
) loading from results/LibriSpeech/brqb/1001/save/weighted_ssl_model.ckpt, the transferred parameters did not have parameters for the key: 1.weights
2023-12-07 09:40:53,629 - speechbrain.core - INFO - Test only mode, skipping training and validation stages.
2023-12-07 09:40:53,661 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+19-06-11+00
2023-12-07 09:40:55,710 - speechbrain.dataio.encoder - DEBUG - Loaded categorical encoding from results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+19-06-11+00/tokenizer.ckpt
2023-12-07 09:42:42,678 - speechbrain.utils.train_logger - INFO - Epoch loaded: 20 - test loss: 3.12e-01, test CER: 3.41, test WER: 9.76
2023-12-07 09:42:42,728 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+19-06-11+00
2023-12-07 09:42:44,019 - speechbrain.dataio.encoder - DEBUG - Loaded categorical encoding from results/LibriSpeech/brqb/1001/save/CKPT+2023-12-06+19-06-11+00/tokenizer.ckpt
2023-12-07 09:44:57,548 - speechbrain.utils.train_logger - INFO - Epoch loaded: 20 - test loss: 8.79e-01, test CER: 10.49, test WER: 24.74
2023-12-07 10:23:11,441 - speechbrain.core - INFO - Beginning experiment!
2023-12-07 10:23:11,460 - speechbrain.core - INFO - Experiment folder: results/LibriSpeech/brqb/1001
2023-12-07 10:23:12,405 - speechbrain.utils.superpowers - DEBUG - appdirs==1.4.4
attrs==23.1.0
black==19.10b0
certifi==2023.11.17
cfgv==3.4.0
charset-normalizer==3.3.2
click==8.0.4
cmake==3.27.7
distlib==0.3.7
entrypoints==0.3
filelock==3.13.1
flake8==3.7.9
fsspec==2023.10.0
huggingface-hub==0.19.4
HyperPyYAML==1.2.2
hypothesis==6.91.0
identify==2.5.32
idna==3.6
iniconfig==2.0.0
Jinja2==3.1.2
joblib==1.3.2
kenlm==0.2.0
lit==17.0.6
MarkupSafe==2.1.3
mccabe==0.6.1
mpmath==1.3.0
networkx==3.2.1
nodeenv==1.8.0
numpy==1.26.2
nvidia-cublas-cu11==11.10.3.66
nvidia-cublas-cu12==12.1.3.1
nvidia-cuda-cupti-cu11==11.7.101
nvidia-cuda-cupti-cu12==12.1.105
nvidia-cuda-nvrtc-cu11==11.7.99
nvidia-cuda-nvrtc-cu12==12.1.105
nvidia-cuda-runtime-cu11==11.7.99
nvidia-cuda-runtime-cu12==12.1.105
nvidia-cudnn-cu11==8.5.0.96
nvidia-cudnn-cu12==8.9.2.26
nvidia-cufft-cu11==10.9.0.58
nvidia-cufft-cu12==11.0.2.54
nvidia-curand-cu11==10.2.10.91
nvidia-curand-cu12==10.3.2.106
nvidia-cusolver-cu11==11.4.0.1
nvidia-cusolver-cu12==11.4.5.107
nvidia-cusparse-cu11==11.7.4.91
nvidia-cusparse-cu12==12.1.0.106
nvidia-nccl-cu11==2.14.3
nvidia-nccl-cu12==2.18.1
nvidia-nvjitlink-cu12==12.3.101
nvidia-nvtx-cu11==11.7.91
nvidia-nvtx-cu12==12.1.105
packaging==23.2
pandas==2.1.3
pathspec==0.11.2
platformdirs==4.0.0
pluggy==1.3.0
pre-commit==3.5.0
pycodestyle==2.5.0
pyctcdecode==0.5.0
pyflakes==2.1.1
pygtrie==2.5.0
pytest==7.4.0
python-dateutil==2.8.2
pytz==2023.3.post1
PyYAML==6.0.1
regex==2023.10.3
requests==2.31.0
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
safetensors==0.4.1
scipy==1.11.4
sentencepiece==0.1.99
six==1.16.0
sortedcontainers==2.4.0
-e git+https://github.com/whettenr/speechbrain.git@b545d914c7e1677cae6d8fe030a00f7594b78475#egg=speechbrain
sympy==1.12
tokenizers==0.15.0
toml==0.10.2
torch==2.0.1
torchaudio==2.0.2
tqdm==4.66.1
transformers==4.35.2
triton==2.0.0
typed-ast==1.5.5
typing_extensions==4.8.0
tzdata==2023.3
urllib3==2.1.0
virtualenv==20.24.7
yamllint==1.23.0


2023-12-07 10:23:12,421 - speechbrain.utils.superpowers - DEBUG - c007bdb


2023-12-07 10:23:13,061 - speechbrain.dataio.encoder - DEBUG - Loaded categorical encoding from results/LibriSpeech/brqb/1001/save/label_encoder.txt
2023-12-07 10:23:13,061 - speechbrain.dataio.encoder - INFO - Load called, but CTCTextEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.
2023-12-07 10:23:13,062 - speechbrain.dataio.encoder - DEBUG - Loaded categorical encoding from results/LibriSpeech/brqb/1001/save/label_encoder.txt
2023-12-07 10:24:39,491 - speechbrain.core - ERROR - Exception:
Traceback (most recent call last):
  File "/gpfswork/rech/nkp/uaj64gk/bestrqexp/benchmark/benchmarks/MP3S/LibriSpeech/LSTM/train_brq.py", line 349, in <module>
    decoder = build_ctcdecoder(
              ^^^^^^^^^^^^^^^^^
  File "/linkhome/rech/genzjn01/uaj64gk/.local/lib/python3.11/site-packages/pyctcdecode/decoder.py", line 907, in build_ctcdecoder
    kenlm_model = None if kenlm_model_path is None else kenlm.Model(kenlm_model_path)
                                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
2023-12-07 12:15:56,010 - speechbrain.core - INFO - Beginning experiment!
2023-12-07 12:15:56,029 - speechbrain.core - INFO - Experiment folder: results/LibriSpeech/brqb/1001
2023-12-07 12:15:59,414 - speechbrain.utils.superpowers - DEBUG - appdirs==1.4.4
attrs==23.1.0
black==19.10b0
certifi==2023.11.17
cfgv==3.4.0
charset-normalizer==3.3.2
click==8.0.4
cmake==3.27.7
distlib==0.3.7
entrypoints==0.3
filelock==3.13.1
flake8==3.7.9
fsspec==2023.10.0
huggingface-hub==0.19.4
HyperPyYAML==1.2.2
hypothesis==6.91.0
identify==2.5.32
idna==3.6
iniconfig==2.0.0
Jinja2==3.1.2
joblib==1.3.2
kenlm==0.2.0
lit==17.0.6
MarkupSafe==2.1.3
mccabe==0.6.1
mpmath==1.3.0
networkx==3.2.1
nodeenv==1.8.0
numpy==1.26.2
nvidia-cublas-cu11==11.10.3.66
nvidia-cublas-cu12==12.1.3.1
nvidia-cuda-cupti-cu11==11.7.101
nvidia-cuda-cupti-cu12==12.1.105
nvidia-cuda-nvrtc-cu11==11.7.99
nvidia-cuda-nvrtc-cu12==12.1.105
nvidia-cuda-runtime-cu11==11.7.99
nvidia-cuda-runtime-cu12==12.1.105
nvidia-cudnn-cu11==8.5.0.96
nvidia-cudnn-cu12==8.9.2.26
nvidia-cufft-cu11==10.9.0.58
nvidia-cufft-cu12==11.0.2.54
nvidia-curand-cu11==10.2.10.91
nvidia-curand-cu12==10.3.2.106
nvidia-cusolver-cu11==11.4.0.1
nvidia-cusolver-cu12==11.4.5.107
nvidia-cusparse-cu11==11.7.4.91
nvidia-cusparse-cu12==12.1.0.106
nvidia-nccl-cu11==2.14.3
nvidia-nccl-cu12==2.18.1
nvidia-nvjitlink-cu12==12.3.101
nvidia-nvtx-cu11==11.7.91
nvidia-nvtx-cu12==12.1.105
packaging==23.2
pandas==2.1.3
pathspec==0.11.2
platformdirs==4.0.0
pluggy==1.3.0
pre-commit==3.5.0
pycodestyle==2.5.0
pyctcdecode==0.5.0
pyflakes==2.1.1
pygtrie==2.5.0
pytest==7.4.0
python-dateutil==2.8.2
pytz==2023.3.post1
PyYAML==6.0.1
regex==2023.10.3
requests==2.31.0
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
safetensors==0.4.1
scipy==1.11.4
sentencepiece==0.1.99
six==1.16.0
sortedcontainers==2.4.0
-e git+https://github.com/whettenr/speechbrain.git@b545d914c7e1677cae6d8fe030a00f7594b78475#egg=speechbrain
sympy==1.12
tokenizers==0.15.0
toml==0.10.2
torch==2.0.1
torchaudio==2.0.2
tqdm==4.66.1
transformers==4.35.2
triton==2.0.0
typed-ast==1.5.5
typing_extensions==4.8.0
tzdata==2023.3
urllib3==2.1.0
virtualenv==20.24.7
yamllint==1.23.0


2023-12-07 12:15:59,525 - speechbrain.utils.superpowers - DEBUG - a97c3dd


2023-12-07 12:16:00,046 - speechbrain.dataio.encoder - DEBUG - Loaded categorical encoding from results/LibriSpeech/brqb/1001/save/label_encoder.txt
2023-12-07 12:16:00,046 - speechbrain.dataio.encoder - INFO - Load called, but CTCTextEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.
2023-12-07 12:16:00,046 - speechbrain.dataio.encoder - DEBUG - Loaded categorical encoding from results/LibriSpeech/brqb/1001/save/label_encoder.txt
2023-12-07 12:16:00,046 - speechbrain.core - INFO - Info: auto_mix_prec arg from hparam file is used
2023-12-07 12:16:00,046 - speechbrain.core - INFO - Info: ckpt_interval_minutes arg from hparam file is used
2023-12-07 12:16:08,333 - speechbrain.core - INFO - 46.3M trainable parameters in ASR
2023-12-07 12:16:08,334 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results/LibriSpeech/brqb/1001/save.
2023-12-07 12:16:08,355 - speechbrain.pretrained.fetching - INFO - Fetch model.ckpt: Using existing file/symlink in results/LibriSpeech/brqb/1001/save/weighted_ssl_model.ckpt.
2023-12-07 12:16:08,356 - speechbrain.utils.parameter_transfer - INFO - Set local path in self.paths[weighted_ssl_model] = results/LibriSpeech/brqb/1001/save/weighted_ssl_model.ckpt
2023-12-07 12:16:08,357 - speechbrain.pretrained.fetching - INFO - Fetch normalizer.ckpt: Using existing file/symlink in results/LibriSpeech/brqb/1001/save/normalize.ckpt.
2023-12-07 12:16:08,358 - speechbrain.utils.parameter_transfer - INFO - Set local path in self.paths[normalize] = results/LibriSpeech/brqb/1001/save/normalize.ckpt
2023-12-07 12:16:08,358 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: weighted_ssl_model, normalize
2023-12-07 12:16:08,358 - speechbrain.utils.parameter_transfer - INFO - Redirecting (loading from local path): results/LibriSpeech/brqb/1001/save/weighted_ssl_model.ckpt -> results/LibriSpeech/brqb/1001/save/weighted_ssl_model.ckpt
2023-12-07 12:16:08,358 - speechbrain.utils.parameter_transfer - INFO - Redirecting (loading from local path): results/LibriSpeech/brqb/1001/save/normalize.ckpt -> results/LibriSpeech/brqb/1001/save/normalize.ckpt
2023-12-07 12:16:10,302 - speechbrain.core - ERROR - Exception:
Traceback (most recent call last):
  File "/gpfswork/rech/nkp/uaj64gk/bestrqexp/benchmark/benchmarks/MP3S/LibriSpeech/LSTM/train_brq.py", line 371, in <module>
    hparams["pretrainer"].load_collected(asr_brain.device)
  File "/gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain/speechbrain/utils/parameter_transfer.py", line 314, in load_collected
    self._call_load_hooks(paramfiles, device)
  File "/gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain/speechbrain/utils/parameter_transfer.py", line 331, in _call_load_hooks
    default_hook(obj, loadpath, device=device)
  File "/gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain/speechbrain/utils/checkpoints.py", line 150, in torch_parameter_transfer
    incompatible_keys = obj.load_state_dict(
                        ^^^^^^^^^^^^^^^^^^^^
  File "/linkhome/rech/genzjn01/uaj64gk/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2041, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for ModuleList:
	size mismatch for 1.transformer.positional_encoding.inv_freq: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for 1.transformer.positional_encoding_decoder.pe: copying a param with shape torch.Size([1, 2500, 512]) from checkpoint, the shape in current model is torch.Size([1, 2500, 768]).
	size mismatch for 1.transformer.encoder.layers.0.mha_layer.in_proj_weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([2304, 768]).
	size mismatch for 1.transformer.encoder.layers.0.mha_layer.pos_bias_u: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.0.mha_layer.pos_bias_v: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.0.mha_layer.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.0.mha_layer.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.0.mha_layer.linear_pos.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.0.convolution_module.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.0.convolution_module.layer_norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.0.convolution_module.bottleneck.0.weight: copying a param with shape torch.Size([1024, 512, 1]) from checkpoint, the shape in current model is torch.Size([1536, 768, 1]).
	size mismatch for 1.transformer.encoder.layers.0.convolution_module.bottleneck.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([1536]).
	size mismatch for 1.transformer.encoder.layers.0.convolution_module.conv.weight: copying a param with shape torch.Size([512, 1, 31]) from checkpoint, the shape in current model is torch.Size([768, 1, 31]).
	size mismatch for 1.transformer.encoder.layers.0.convolution_module.conv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.0.convolution_module.after_conv.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.0.convolution_module.after_conv.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.0.convolution_module.after_conv.2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.0.convolution_module.after_conv.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.0.ffn_module1.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.0.ffn_module1.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.0.ffn_module1.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.0.ffn_module1.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.0.ffn_module1.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.0.ffn_module2.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.0.ffn_module2.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.0.ffn_module2.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.0.ffn_module2.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.0.ffn_module2.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.0.norm1.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.0.norm1.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.0.norm2.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.0.norm2.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.1.mha_layer.in_proj_weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([2304, 768]).
	size mismatch for 1.transformer.encoder.layers.1.mha_layer.pos_bias_u: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.1.mha_layer.pos_bias_v: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.1.mha_layer.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.1.mha_layer.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.1.mha_layer.linear_pos.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.1.convolution_module.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.1.convolution_module.layer_norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.1.convolution_module.bottleneck.0.weight: copying a param with shape torch.Size([1024, 512, 1]) from checkpoint, the shape in current model is torch.Size([1536, 768, 1]).
	size mismatch for 1.transformer.encoder.layers.1.convolution_module.bottleneck.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([1536]).
	size mismatch for 1.transformer.encoder.layers.1.convolution_module.conv.weight: copying a param with shape torch.Size([512, 1, 31]) from checkpoint, the shape in current model is torch.Size([768, 1, 31]).
	size mismatch for 1.transformer.encoder.layers.1.convolution_module.conv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.1.convolution_module.after_conv.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.1.convolution_module.after_conv.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.1.convolution_module.after_conv.2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.1.convolution_module.after_conv.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.1.ffn_module1.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.1.ffn_module1.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.1.ffn_module1.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.1.ffn_module1.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.1.ffn_module1.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.1.ffn_module2.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.1.ffn_module2.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.1.ffn_module2.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.1.ffn_module2.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.1.ffn_module2.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.1.norm1.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.1.norm1.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.1.norm2.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.1.norm2.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.2.mha_layer.in_proj_weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([2304, 768]).
	size mismatch for 1.transformer.encoder.layers.2.mha_layer.pos_bias_u: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.2.mha_layer.pos_bias_v: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.2.mha_layer.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.2.mha_layer.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.2.mha_layer.linear_pos.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.2.convolution_module.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.2.convolution_module.layer_norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.2.convolution_module.bottleneck.0.weight: copying a param with shape torch.Size([1024, 512, 1]) from checkpoint, the shape in current model is torch.Size([1536, 768, 1]).
	size mismatch for 1.transformer.encoder.layers.2.convolution_module.bottleneck.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([1536]).
	size mismatch for 1.transformer.encoder.layers.2.convolution_module.conv.weight: copying a param with shape torch.Size([512, 1, 31]) from checkpoint, the shape in current model is torch.Size([768, 1, 31]).
	size mismatch for 1.transformer.encoder.layers.2.convolution_module.conv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.2.convolution_module.after_conv.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.2.convolution_module.after_conv.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.2.convolution_module.after_conv.2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.2.convolution_module.after_conv.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.2.ffn_module1.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.2.ffn_module1.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.2.ffn_module1.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.2.ffn_module1.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.2.ffn_module1.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.2.ffn_module2.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.2.ffn_module2.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.2.ffn_module2.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.2.ffn_module2.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.2.ffn_module2.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.2.norm1.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.2.norm1.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.2.norm2.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.2.norm2.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.3.mha_layer.in_proj_weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([2304, 768]).
	size mismatch for 1.transformer.encoder.layers.3.mha_layer.pos_bias_u: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.3.mha_layer.pos_bias_v: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.3.mha_layer.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.3.mha_layer.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.3.mha_layer.linear_pos.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.3.convolution_module.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.3.convolution_module.layer_norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.3.convolution_module.bottleneck.0.weight: copying a param with shape torch.Size([1024, 512, 1]) from checkpoint, the shape in current model is torch.Size([1536, 768, 1]).
	size mismatch for 1.transformer.encoder.layers.3.convolution_module.bottleneck.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([1536]).
	size mismatch for 1.transformer.encoder.layers.3.convolution_module.conv.weight: copying a param with shape torch.Size([512, 1, 31]) from checkpoint, the shape in current model is torch.Size([768, 1, 31]).
	size mismatch for 1.transformer.encoder.layers.3.convolution_module.conv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.3.convolution_module.after_conv.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.3.convolution_module.after_conv.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.3.convolution_module.after_conv.2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.3.convolution_module.after_conv.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.3.ffn_module1.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.3.ffn_module1.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.3.ffn_module1.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.3.ffn_module1.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.3.ffn_module1.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.3.ffn_module2.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.3.ffn_module2.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.3.ffn_module2.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.3.ffn_module2.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.3.ffn_module2.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.3.norm1.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.3.norm1.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.3.norm2.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.3.norm2.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.4.mha_layer.in_proj_weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([2304, 768]).
	size mismatch for 1.transformer.encoder.layers.4.mha_layer.pos_bias_u: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.4.mha_layer.pos_bias_v: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.4.mha_layer.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.4.mha_layer.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.4.mha_layer.linear_pos.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.4.convolution_module.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.4.convolution_module.layer_norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.4.convolution_module.bottleneck.0.weight: copying a param with shape torch.Size([1024, 512, 1]) from checkpoint, the shape in current model is torch.Size([1536, 768, 1]).
	size mismatch for 1.transformer.encoder.layers.4.convolution_module.bottleneck.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([1536]).
	size mismatch for 1.transformer.encoder.layers.4.convolution_module.conv.weight: copying a param with shape torch.Size([512, 1, 31]) from checkpoint, the shape in current model is torch.Size([768, 1, 31]).
	size mismatch for 1.transformer.encoder.layers.4.convolution_module.conv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.4.convolution_module.after_conv.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.4.convolution_module.after_conv.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.4.convolution_module.after_conv.2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.4.convolution_module.after_conv.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.4.ffn_module1.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.4.ffn_module1.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.4.ffn_module1.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.4.ffn_module1.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.4.ffn_module1.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.4.ffn_module2.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.4.ffn_module2.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.4.ffn_module2.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.4.ffn_module2.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.4.ffn_module2.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.4.norm1.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.4.norm1.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.4.norm2.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.4.norm2.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.5.mha_layer.in_proj_weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([2304, 768]).
	size mismatch for 1.transformer.encoder.layers.5.mha_layer.pos_bias_u: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.5.mha_layer.pos_bias_v: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.5.mha_layer.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.5.mha_layer.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.5.mha_layer.linear_pos.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.5.convolution_module.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.5.convolution_module.layer_norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.5.convolution_module.bottleneck.0.weight: copying a param with shape torch.Size([1024, 512, 1]) from checkpoint, the shape in current model is torch.Size([1536, 768, 1]).
	size mismatch for 1.transformer.encoder.layers.5.convolution_module.bottleneck.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([1536]).
	size mismatch for 1.transformer.encoder.layers.5.convolution_module.conv.weight: copying a param with shape torch.Size([512, 1, 31]) from checkpoint, the shape in current model is torch.Size([768, 1, 31]).
	size mismatch for 1.transformer.encoder.layers.5.convolution_module.conv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.5.convolution_module.after_conv.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.5.convolution_module.after_conv.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.5.convolution_module.after_conv.2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.5.convolution_module.after_conv.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.5.ffn_module1.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.5.ffn_module1.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.5.ffn_module1.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.5.ffn_module1.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.5.ffn_module1.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.5.ffn_module2.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.5.ffn_module2.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.5.ffn_module2.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.5.ffn_module2.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.5.ffn_module2.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.5.norm1.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.5.norm1.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.5.norm2.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.5.norm2.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.6.mha_layer.in_proj_weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([2304, 768]).
	size mismatch for 1.transformer.encoder.layers.6.mha_layer.pos_bias_u: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.6.mha_layer.pos_bias_v: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.6.mha_layer.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.6.mha_layer.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.6.mha_layer.linear_pos.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.6.convolution_module.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.6.convolution_module.layer_norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.6.convolution_module.bottleneck.0.weight: copying a param with shape torch.Size([1024, 512, 1]) from checkpoint, the shape in current model is torch.Size([1536, 768, 1]).
	size mismatch for 1.transformer.encoder.layers.6.convolution_module.bottleneck.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([1536]).
	size mismatch for 1.transformer.encoder.layers.6.convolution_module.conv.weight: copying a param with shape torch.Size([512, 1, 31]) from checkpoint, the shape in current model is torch.Size([768, 1, 31]).
	size mismatch for 1.transformer.encoder.layers.6.convolution_module.conv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.6.convolution_module.after_conv.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.6.convolution_module.after_conv.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.6.convolution_module.after_conv.2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.6.convolution_module.after_conv.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.6.ffn_module1.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.6.ffn_module1.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.6.ffn_module1.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.6.ffn_module1.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.6.ffn_module1.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.6.ffn_module2.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.6.ffn_module2.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.6.ffn_module2.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.6.ffn_module2.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.6.ffn_module2.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.6.norm1.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.6.norm1.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.6.norm2.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.6.norm2.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.7.mha_layer.in_proj_weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([2304, 768]).
	size mismatch for 1.transformer.encoder.layers.7.mha_layer.pos_bias_u: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.7.mha_layer.pos_bias_v: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.7.mha_layer.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.7.mha_layer.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.7.mha_layer.linear_pos.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.7.convolution_module.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.7.convolution_module.layer_norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.7.convolution_module.bottleneck.0.weight: copying a param with shape torch.Size([1024, 512, 1]) from checkpoint, the shape in current model is torch.Size([1536, 768, 1]).
	size mismatch for 1.transformer.encoder.layers.7.convolution_module.bottleneck.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([1536]).
	size mismatch for 1.transformer.encoder.layers.7.convolution_module.conv.weight: copying a param with shape torch.Size([512, 1, 31]) from checkpoint, the shape in current model is torch.Size([768, 1, 31]).
	size mismatch for 1.transformer.encoder.layers.7.convolution_module.conv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.7.convolution_module.after_conv.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.7.convolution_module.after_conv.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.7.convolution_module.after_conv.2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.7.convolution_module.after_conv.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.7.ffn_module1.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.7.ffn_module1.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.7.ffn_module1.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.7.ffn_module1.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.7.ffn_module1.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.7.ffn_module2.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.7.ffn_module2.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.7.ffn_module2.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.7.ffn_module2.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.7.ffn_module2.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.7.norm1.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.7.norm1.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.7.norm2.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.7.norm2.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.8.mha_layer.in_proj_weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([2304, 768]).
	size mismatch for 1.transformer.encoder.layers.8.mha_layer.pos_bias_u: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.8.mha_layer.pos_bias_v: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.8.mha_layer.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.8.mha_layer.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.8.mha_layer.linear_pos.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.8.convolution_module.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.8.convolution_module.layer_norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.8.convolution_module.bottleneck.0.weight: copying a param with shape torch.Size([1024, 512, 1]) from checkpoint, the shape in current model is torch.Size([1536, 768, 1]).
	size mismatch for 1.transformer.encoder.layers.8.convolution_module.bottleneck.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([1536]).
	size mismatch for 1.transformer.encoder.layers.8.convolution_module.conv.weight: copying a param with shape torch.Size([512, 1, 31]) from checkpoint, the shape in current model is torch.Size([768, 1, 31]).
	size mismatch for 1.transformer.encoder.layers.8.convolution_module.conv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.8.convolution_module.after_conv.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.8.convolution_module.after_conv.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.8.convolution_module.after_conv.2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.8.convolution_module.after_conv.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.8.ffn_module1.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.8.ffn_module1.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.8.ffn_module1.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.8.ffn_module1.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.8.ffn_module1.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.8.ffn_module2.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.8.ffn_module2.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.8.ffn_module2.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.8.ffn_module2.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.8.ffn_module2.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.8.norm1.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.8.norm1.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.8.norm2.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.8.norm2.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.9.mha_layer.in_proj_weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([2304, 768]).
	size mismatch for 1.transformer.encoder.layers.9.mha_layer.pos_bias_u: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.9.mha_layer.pos_bias_v: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.9.mha_layer.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.9.mha_layer.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.9.mha_layer.linear_pos.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.9.convolution_module.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.9.convolution_module.layer_norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.9.convolution_module.bottleneck.0.weight: copying a param with shape torch.Size([1024, 512, 1]) from checkpoint, the shape in current model is torch.Size([1536, 768, 1]).
	size mismatch for 1.transformer.encoder.layers.9.convolution_module.bottleneck.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([1536]).
	size mismatch for 1.transformer.encoder.layers.9.convolution_module.conv.weight: copying a param with shape torch.Size([512, 1, 31]) from checkpoint, the shape in current model is torch.Size([768, 1, 31]).
	size mismatch for 1.transformer.encoder.layers.9.convolution_module.conv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.9.convolution_module.after_conv.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.9.convolution_module.after_conv.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.9.convolution_module.after_conv.2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.9.convolution_module.after_conv.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.9.ffn_module1.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.9.ffn_module1.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.9.ffn_module1.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.9.ffn_module1.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.9.ffn_module1.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.9.ffn_module2.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.9.ffn_module2.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.9.ffn_module2.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.9.ffn_module2.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.9.ffn_module2.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.9.norm1.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.9.norm1.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.9.norm2.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.9.norm2.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.10.mha_layer.in_proj_weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([2304, 768]).
	size mismatch for 1.transformer.encoder.layers.10.mha_layer.pos_bias_u: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.10.mha_layer.pos_bias_v: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.10.mha_layer.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.10.mha_layer.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.10.mha_layer.linear_pos.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.10.convolution_module.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.10.convolution_module.layer_norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.10.convolution_module.bottleneck.0.weight: copying a param with shape torch.Size([1024, 512, 1]) from checkpoint, the shape in current model is torch.Size([1536, 768, 1]).
	size mismatch for 1.transformer.encoder.layers.10.convolution_module.bottleneck.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([1536]).
	size mismatch for 1.transformer.encoder.layers.10.convolution_module.conv.weight: copying a param with shape torch.Size([512, 1, 31]) from checkpoint, the shape in current model is torch.Size([768, 1, 31]).
	size mismatch for 1.transformer.encoder.layers.10.convolution_module.conv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.10.convolution_module.after_conv.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.10.convolution_module.after_conv.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.10.convolution_module.after_conv.2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.10.convolution_module.after_conv.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.10.ffn_module1.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.10.ffn_module1.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.10.ffn_module1.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.10.ffn_module1.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.10.ffn_module1.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.10.ffn_module2.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.10.ffn_module2.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.10.ffn_module2.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.10.ffn_module2.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.10.ffn_module2.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.10.norm1.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.10.norm1.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.10.norm2.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.10.norm2.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.11.mha_layer.in_proj_weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([2304, 768]).
	size mismatch for 1.transformer.encoder.layers.11.mha_layer.pos_bias_u: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.11.mha_layer.pos_bias_v: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.11.mha_layer.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.11.mha_layer.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.11.mha_layer.linear_pos.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.11.convolution_module.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.11.convolution_module.layer_norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.11.convolution_module.bottleneck.0.weight: copying a param with shape torch.Size([1024, 512, 1]) from checkpoint, the shape in current model is torch.Size([1536, 768, 1]).
	size mismatch for 1.transformer.encoder.layers.11.convolution_module.bottleneck.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([1536]).
	size mismatch for 1.transformer.encoder.layers.11.convolution_module.conv.weight: copying a param with shape torch.Size([512, 1, 31]) from checkpoint, the shape in current model is torch.Size([768, 1, 31]).
	size mismatch for 1.transformer.encoder.layers.11.convolution_module.conv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.11.convolution_module.after_conv.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.11.convolution_module.after_conv.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.11.convolution_module.after_conv.2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.11.convolution_module.after_conv.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.11.ffn_module1.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.11.ffn_module1.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.11.ffn_module1.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.11.ffn_module1.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.11.ffn_module1.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.11.ffn_module2.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.11.ffn_module2.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.11.ffn_module2.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.11.ffn_module2.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.11.ffn_module2.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.11.norm1.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.11.norm1.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.11.norm2.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.11.norm2.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.norm.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.norm.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.custom_src_module.layers.0.w.weight: copying a param with shape torch.Size([512, 640]) from checkpoint, the shape in current model is torch.Size([768, 640]).
	size mismatch for 1.transformer.custom_src_module.layers.0.w.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.custom_tgt_module.layers.0.emb.Embedding.weight: copying a param with shape torch.Size([5000, 512]) from checkpoint, the shape in current model is torch.Size([5000, 768]).
	size mismatch for 3.w.weight: copying a param with shape torch.Size([8192, 512]) from checkpoint, the shape in current model is torch.Size([8192, 768]).
2023-12-07 12:18:54,857 - speechbrain.core - INFO - Beginning experiment!
2023-12-07 12:18:54,858 - speechbrain.core - INFO - Experiment folder: results/LibriSpeech/brqb/1001
2023-12-07 12:18:55,627 - speechbrain.utils.superpowers - DEBUG - appdirs==1.4.4
attrs==23.1.0
black==19.10b0
certifi==2023.11.17
cfgv==3.4.0
charset-normalizer==3.3.2
click==8.0.4
cmake==3.27.7
distlib==0.3.7
entrypoints==0.3
filelock==3.13.1
flake8==3.7.9
fsspec==2023.10.0
huggingface-hub==0.19.4
HyperPyYAML==1.2.2
hypothesis==6.91.0
identify==2.5.32
idna==3.6
iniconfig==2.0.0
Jinja2==3.1.2
joblib==1.3.2
kenlm==0.2.0
lit==17.0.6
MarkupSafe==2.1.3
mccabe==0.6.1
mpmath==1.3.0
networkx==3.2.1
nodeenv==1.8.0
numpy==1.26.2
nvidia-cublas-cu11==11.10.3.66
nvidia-cublas-cu12==12.1.3.1
nvidia-cuda-cupti-cu11==11.7.101
nvidia-cuda-cupti-cu12==12.1.105
nvidia-cuda-nvrtc-cu11==11.7.99
nvidia-cuda-nvrtc-cu12==12.1.105
nvidia-cuda-runtime-cu11==11.7.99
nvidia-cuda-runtime-cu12==12.1.105
nvidia-cudnn-cu11==8.5.0.96
nvidia-cudnn-cu12==8.9.2.26
nvidia-cufft-cu11==10.9.0.58
nvidia-cufft-cu12==11.0.2.54
nvidia-curand-cu11==10.2.10.91
nvidia-curand-cu12==10.3.2.106
nvidia-cusolver-cu11==11.4.0.1
nvidia-cusolver-cu12==11.4.5.107
nvidia-cusparse-cu11==11.7.4.91
nvidia-cusparse-cu12==12.1.0.106
nvidia-nccl-cu11==2.14.3
nvidia-nccl-cu12==2.18.1
nvidia-nvjitlink-cu12==12.3.101
nvidia-nvtx-cu11==11.7.91
nvidia-nvtx-cu12==12.1.105
packaging==23.2
pandas==2.1.3
pathspec==0.11.2
platformdirs==4.0.0
pluggy==1.3.0
pre-commit==3.5.0
pycodestyle==2.5.0
pyctcdecode==0.5.0
pyflakes==2.1.1
pygtrie==2.5.0
pytest==7.4.0
python-dateutil==2.8.2
pytz==2023.3.post1
PyYAML==6.0.1
regex==2023.10.3
requests==2.31.0
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
safetensors==0.4.1
scipy==1.11.4
sentencepiece==0.1.99
six==1.16.0
sortedcontainers==2.4.0
-e git+https://github.com/whettenr/speechbrain.git@b545d914c7e1677cae6d8fe030a00f7594b78475#egg=speechbrain
sympy==1.12
tokenizers==0.15.0
toml==0.10.2
torch==2.0.1
torchaudio==2.0.2
tqdm==4.66.1
transformers==4.35.2
triton==2.0.0
typed-ast==1.5.5
typing_extensions==4.8.0
tzdata==2023.3
urllib3==2.1.0
virtualenv==20.24.7
yamllint==1.23.0


2023-12-07 12:18:55,643 - speechbrain.utils.superpowers - DEBUG - a97c3dd


2023-12-07 12:18:56,035 - speechbrain.dataio.encoder - DEBUG - Loaded categorical encoding from results/LibriSpeech/brqb/1001/save/label_encoder.txt
2023-12-07 12:18:56,036 - speechbrain.dataio.encoder - INFO - Load called, but CTCTextEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.
2023-12-07 12:18:56,036 - speechbrain.dataio.encoder - DEBUG - Loaded categorical encoding from results/LibriSpeech/brqb/1001/save/label_encoder.txt
2023-12-07 12:18:56,037 - speechbrain.core - INFO - Info: auto_mix_prec arg from hparam file is used
2023-12-07 12:18:56,037 - speechbrain.core - INFO - Info: ckpt_interval_minutes arg from hparam file is used
2023-12-07 12:18:59,307 - speechbrain.core - INFO - 46.3M trainable parameters in ASR
2023-12-07 12:18:59,308 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results/LibriSpeech/brqb/1001/save.
2023-12-07 12:18:59,308 - speechbrain.pretrained.fetching - INFO - Fetch model.ckpt: Using existing file/symlink in results/LibriSpeech/brqb/1001/save/weighted_ssl_model.ckpt.
2023-12-07 12:18:59,309 - speechbrain.pretrained.fetching - INFO - Fetch normalizer.ckpt: Using existing file/symlink in results/LibriSpeech/brqb/1001/save/normalize.ckpt.
2023-12-07 12:18:59,310 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: weighted_ssl_model, normalize
2023-12-07 12:19:00,757 - speechbrain.core - ERROR - Exception:
Traceback (most recent call last):
  File "/gpfswork/rech/nkp/uaj64gk/bestrqexp/benchmark/benchmarks/MP3S/LibriSpeech/LSTM/train_brq.py", line 371, in <module>
    hparams["pretrainer"].load_collected(asr_brain.device)
  File "/gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain/speechbrain/utils/parameter_transfer.py", line 314, in load_collected
    self._call_load_hooks(paramfiles, device)
  File "/gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain/speechbrain/utils/parameter_transfer.py", line 331, in _call_load_hooks
    default_hook(obj, loadpath, device=device)
  File "/gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain/speechbrain/utils/checkpoints.py", line 150, in torch_parameter_transfer
    incompatible_keys = obj.load_state_dict(
                        ^^^^^^^^^^^^^^^^^^^^
  File "/linkhome/rech/genzjn01/uaj64gk/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2041, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for ModuleList:
	size mismatch for 1.transformer.positional_encoding.inv_freq: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for 1.transformer.positional_encoding_decoder.pe: copying a param with shape torch.Size([1, 2500, 512]) from checkpoint, the shape in current model is torch.Size([1, 2500, 768]).
	size mismatch for 1.transformer.encoder.layers.0.mha_layer.in_proj_weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([2304, 768]).
	size mismatch for 1.transformer.encoder.layers.0.mha_layer.pos_bias_u: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.0.mha_layer.pos_bias_v: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.0.mha_layer.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.0.mha_layer.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.0.mha_layer.linear_pos.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.0.convolution_module.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.0.convolution_module.layer_norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.0.convolution_module.bottleneck.0.weight: copying a param with shape torch.Size([1024, 512, 1]) from checkpoint, the shape in current model is torch.Size([1536, 768, 1]).
	size mismatch for 1.transformer.encoder.layers.0.convolution_module.bottleneck.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([1536]).
	size mismatch for 1.transformer.encoder.layers.0.convolution_module.conv.weight: copying a param with shape torch.Size([512, 1, 31]) from checkpoint, the shape in current model is torch.Size([768, 1, 31]).
	size mismatch for 1.transformer.encoder.layers.0.convolution_module.conv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.0.convolution_module.after_conv.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.0.convolution_module.after_conv.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.0.convolution_module.after_conv.2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.0.convolution_module.after_conv.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.0.ffn_module1.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.0.ffn_module1.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.0.ffn_module1.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.0.ffn_module1.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.0.ffn_module1.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.0.ffn_module2.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.0.ffn_module2.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.0.ffn_module2.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.0.ffn_module2.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.0.ffn_module2.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.0.norm1.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.0.norm1.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.0.norm2.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.0.norm2.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.1.mha_layer.in_proj_weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([2304, 768]).
	size mismatch for 1.transformer.encoder.layers.1.mha_layer.pos_bias_u: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.1.mha_layer.pos_bias_v: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.1.mha_layer.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.1.mha_layer.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.1.mha_layer.linear_pos.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.1.convolution_module.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.1.convolution_module.layer_norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.1.convolution_module.bottleneck.0.weight: copying a param with shape torch.Size([1024, 512, 1]) from checkpoint, the shape in current model is torch.Size([1536, 768, 1]).
	size mismatch for 1.transformer.encoder.layers.1.convolution_module.bottleneck.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([1536]).
	size mismatch for 1.transformer.encoder.layers.1.convolution_module.conv.weight: copying a param with shape torch.Size([512, 1, 31]) from checkpoint, the shape in current model is torch.Size([768, 1, 31]).
	size mismatch for 1.transformer.encoder.layers.1.convolution_module.conv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.1.convolution_module.after_conv.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.1.convolution_module.after_conv.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.1.convolution_module.after_conv.2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.1.convolution_module.after_conv.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.1.ffn_module1.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.1.ffn_module1.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.1.ffn_module1.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.1.ffn_module1.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.1.ffn_module1.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.1.ffn_module2.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.1.ffn_module2.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.1.ffn_module2.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.1.ffn_module2.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.1.ffn_module2.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.1.norm1.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.1.norm1.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.1.norm2.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.1.norm2.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.2.mha_layer.in_proj_weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([2304, 768]).
	size mismatch for 1.transformer.encoder.layers.2.mha_layer.pos_bias_u: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.2.mha_layer.pos_bias_v: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.2.mha_layer.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.2.mha_layer.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.2.mha_layer.linear_pos.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.2.convolution_module.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.2.convolution_module.layer_norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.2.convolution_module.bottleneck.0.weight: copying a param with shape torch.Size([1024, 512, 1]) from checkpoint, the shape in current model is torch.Size([1536, 768, 1]).
	size mismatch for 1.transformer.encoder.layers.2.convolution_module.bottleneck.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([1536]).
	size mismatch for 1.transformer.encoder.layers.2.convolution_module.conv.weight: copying a param with shape torch.Size([512, 1, 31]) from checkpoint, the shape in current model is torch.Size([768, 1, 31]).
	size mismatch for 1.transformer.encoder.layers.2.convolution_module.conv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.2.convolution_module.after_conv.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.2.convolution_module.after_conv.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.2.convolution_module.after_conv.2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.2.convolution_module.after_conv.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.2.ffn_module1.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.2.ffn_module1.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.2.ffn_module1.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.2.ffn_module1.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.2.ffn_module1.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.2.ffn_module2.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.2.ffn_module2.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.2.ffn_module2.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.2.ffn_module2.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.2.ffn_module2.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.2.norm1.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.2.norm1.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.2.norm2.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.2.norm2.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.3.mha_layer.in_proj_weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([2304, 768]).
	size mismatch for 1.transformer.encoder.layers.3.mha_layer.pos_bias_u: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.3.mha_layer.pos_bias_v: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.3.mha_layer.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.3.mha_layer.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.3.mha_layer.linear_pos.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.3.convolution_module.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.3.convolution_module.layer_norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.3.convolution_module.bottleneck.0.weight: copying a param with shape torch.Size([1024, 512, 1]) from checkpoint, the shape in current model is torch.Size([1536, 768, 1]).
	size mismatch for 1.transformer.encoder.layers.3.convolution_module.bottleneck.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([1536]).
	size mismatch for 1.transformer.encoder.layers.3.convolution_module.conv.weight: copying a param with shape torch.Size([512, 1, 31]) from checkpoint, the shape in current model is torch.Size([768, 1, 31]).
	size mismatch for 1.transformer.encoder.layers.3.convolution_module.conv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.3.convolution_module.after_conv.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.3.convolution_module.after_conv.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.3.convolution_module.after_conv.2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.3.convolution_module.after_conv.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.3.ffn_module1.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.3.ffn_module1.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.3.ffn_module1.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.3.ffn_module1.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.3.ffn_module1.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.3.ffn_module2.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.3.ffn_module2.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.3.ffn_module2.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.3.ffn_module2.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.3.ffn_module2.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.3.norm1.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.3.norm1.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.3.norm2.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.3.norm2.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.4.mha_layer.in_proj_weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([2304, 768]).
	size mismatch for 1.transformer.encoder.layers.4.mha_layer.pos_bias_u: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.4.mha_layer.pos_bias_v: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.4.mha_layer.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.4.mha_layer.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.4.mha_layer.linear_pos.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.4.convolution_module.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.4.convolution_module.layer_norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.4.convolution_module.bottleneck.0.weight: copying a param with shape torch.Size([1024, 512, 1]) from checkpoint, the shape in current model is torch.Size([1536, 768, 1]).
	size mismatch for 1.transformer.encoder.layers.4.convolution_module.bottleneck.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([1536]).
	size mismatch for 1.transformer.encoder.layers.4.convolution_module.conv.weight: copying a param with shape torch.Size([512, 1, 31]) from checkpoint, the shape in current model is torch.Size([768, 1, 31]).
	size mismatch for 1.transformer.encoder.layers.4.convolution_module.conv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.4.convolution_module.after_conv.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.4.convolution_module.after_conv.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.4.convolution_module.after_conv.2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.4.convolution_module.after_conv.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.4.ffn_module1.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.4.ffn_module1.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.4.ffn_module1.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.4.ffn_module1.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.4.ffn_module1.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.4.ffn_module2.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.4.ffn_module2.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.4.ffn_module2.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.4.ffn_module2.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.4.ffn_module2.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.4.norm1.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.4.norm1.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.4.norm2.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.4.norm2.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.5.mha_layer.in_proj_weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([2304, 768]).
	size mismatch for 1.transformer.encoder.layers.5.mha_layer.pos_bias_u: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.5.mha_layer.pos_bias_v: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.5.mha_layer.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.5.mha_layer.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.5.mha_layer.linear_pos.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.5.convolution_module.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.5.convolution_module.layer_norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.5.convolution_module.bottleneck.0.weight: copying a param with shape torch.Size([1024, 512, 1]) from checkpoint, the shape in current model is torch.Size([1536, 768, 1]).
	size mismatch for 1.transformer.encoder.layers.5.convolution_module.bottleneck.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([1536]).
	size mismatch for 1.transformer.encoder.layers.5.convolution_module.conv.weight: copying a param with shape torch.Size([512, 1, 31]) from checkpoint, the shape in current model is torch.Size([768, 1, 31]).
	size mismatch for 1.transformer.encoder.layers.5.convolution_module.conv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.5.convolution_module.after_conv.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.5.convolution_module.after_conv.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.5.convolution_module.after_conv.2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.5.convolution_module.after_conv.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.5.ffn_module1.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.5.ffn_module1.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.5.ffn_module1.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.5.ffn_module1.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.5.ffn_module1.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.5.ffn_module2.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.5.ffn_module2.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.5.ffn_module2.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.5.ffn_module2.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.5.ffn_module2.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.5.norm1.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.5.norm1.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.5.norm2.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.5.norm2.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.6.mha_layer.in_proj_weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([2304, 768]).
	size mismatch for 1.transformer.encoder.layers.6.mha_layer.pos_bias_u: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.6.mha_layer.pos_bias_v: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.6.mha_layer.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.6.mha_layer.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.6.mha_layer.linear_pos.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.6.convolution_module.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.6.convolution_module.layer_norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.6.convolution_module.bottleneck.0.weight: copying a param with shape torch.Size([1024, 512, 1]) from checkpoint, the shape in current model is torch.Size([1536, 768, 1]).
	size mismatch for 1.transformer.encoder.layers.6.convolution_module.bottleneck.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([1536]).
	size mismatch for 1.transformer.encoder.layers.6.convolution_module.conv.weight: copying a param with shape torch.Size([512, 1, 31]) from checkpoint, the shape in current model is torch.Size([768, 1, 31]).
	size mismatch for 1.transformer.encoder.layers.6.convolution_module.conv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.6.convolution_module.after_conv.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.6.convolution_module.after_conv.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.6.convolution_module.after_conv.2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.6.convolution_module.after_conv.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.6.ffn_module1.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.6.ffn_module1.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.6.ffn_module1.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.6.ffn_module1.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.6.ffn_module1.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.6.ffn_module2.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.6.ffn_module2.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.6.ffn_module2.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.6.ffn_module2.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.6.ffn_module2.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.6.norm1.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.6.norm1.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.6.norm2.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.6.norm2.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.7.mha_layer.in_proj_weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([2304, 768]).
	size mismatch for 1.transformer.encoder.layers.7.mha_layer.pos_bias_u: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.7.mha_layer.pos_bias_v: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.7.mha_layer.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.7.mha_layer.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.7.mha_layer.linear_pos.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.7.convolution_module.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.7.convolution_module.layer_norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.7.convolution_module.bottleneck.0.weight: copying a param with shape torch.Size([1024, 512, 1]) from checkpoint, the shape in current model is torch.Size([1536, 768, 1]).
	size mismatch for 1.transformer.encoder.layers.7.convolution_module.bottleneck.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([1536]).
	size mismatch for 1.transformer.encoder.layers.7.convolution_module.conv.weight: copying a param with shape torch.Size([512, 1, 31]) from checkpoint, the shape in current model is torch.Size([768, 1, 31]).
	size mismatch for 1.transformer.encoder.layers.7.convolution_module.conv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.7.convolution_module.after_conv.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.7.convolution_module.after_conv.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.7.convolution_module.after_conv.2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.7.convolution_module.after_conv.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.7.ffn_module1.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.7.ffn_module1.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.7.ffn_module1.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.7.ffn_module1.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.7.ffn_module1.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.7.ffn_module2.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.7.ffn_module2.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.7.ffn_module2.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.7.ffn_module2.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.7.ffn_module2.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.7.norm1.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.7.norm1.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.7.norm2.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.7.norm2.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.8.mha_layer.in_proj_weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([2304, 768]).
	size mismatch for 1.transformer.encoder.layers.8.mha_layer.pos_bias_u: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.8.mha_layer.pos_bias_v: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.8.mha_layer.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.8.mha_layer.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.8.mha_layer.linear_pos.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.8.convolution_module.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.8.convolution_module.layer_norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.8.convolution_module.bottleneck.0.weight: copying a param with shape torch.Size([1024, 512, 1]) from checkpoint, the shape in current model is torch.Size([1536, 768, 1]).
	size mismatch for 1.transformer.encoder.layers.8.convolution_module.bottleneck.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([1536]).
	size mismatch for 1.transformer.encoder.layers.8.convolution_module.conv.weight: copying a param with shape torch.Size([512, 1, 31]) from checkpoint, the shape in current model is torch.Size([768, 1, 31]).
	size mismatch for 1.transformer.encoder.layers.8.convolution_module.conv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.8.convolution_module.after_conv.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.8.convolution_module.after_conv.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.8.convolution_module.after_conv.2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.8.convolution_module.after_conv.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.8.ffn_module1.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.8.ffn_module1.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.8.ffn_module1.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.8.ffn_module1.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.8.ffn_module1.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.8.ffn_module2.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.8.ffn_module2.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.8.ffn_module2.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.8.ffn_module2.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.8.ffn_module2.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.8.norm1.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.8.norm1.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.8.norm2.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.8.norm2.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.9.mha_layer.in_proj_weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([2304, 768]).
	size mismatch for 1.transformer.encoder.layers.9.mha_layer.pos_bias_u: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.9.mha_layer.pos_bias_v: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.9.mha_layer.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.9.mha_layer.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.9.mha_layer.linear_pos.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.9.convolution_module.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.9.convolution_module.layer_norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.9.convolution_module.bottleneck.0.weight: copying a param with shape torch.Size([1024, 512, 1]) from checkpoint, the shape in current model is torch.Size([1536, 768, 1]).
	size mismatch for 1.transformer.encoder.layers.9.convolution_module.bottleneck.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([1536]).
	size mismatch for 1.transformer.encoder.layers.9.convolution_module.conv.weight: copying a param with shape torch.Size([512, 1, 31]) from checkpoint, the shape in current model is torch.Size([768, 1, 31]).
	size mismatch for 1.transformer.encoder.layers.9.convolution_module.conv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.9.convolution_module.after_conv.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.9.convolution_module.after_conv.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.9.convolution_module.after_conv.2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.9.convolution_module.after_conv.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.9.ffn_module1.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.9.ffn_module1.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.9.ffn_module1.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.9.ffn_module1.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.9.ffn_module1.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.9.ffn_module2.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.9.ffn_module2.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.9.ffn_module2.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.9.ffn_module2.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.9.ffn_module2.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.9.norm1.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.9.norm1.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.9.norm2.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.9.norm2.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.10.mha_layer.in_proj_weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([2304, 768]).
	size mismatch for 1.transformer.encoder.layers.10.mha_layer.pos_bias_u: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.10.mha_layer.pos_bias_v: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.10.mha_layer.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.10.mha_layer.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.10.mha_layer.linear_pos.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.10.convolution_module.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.10.convolution_module.layer_norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.10.convolution_module.bottleneck.0.weight: copying a param with shape torch.Size([1024, 512, 1]) from checkpoint, the shape in current model is torch.Size([1536, 768, 1]).
	size mismatch for 1.transformer.encoder.layers.10.convolution_module.bottleneck.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([1536]).
	size mismatch for 1.transformer.encoder.layers.10.convolution_module.conv.weight: copying a param with shape torch.Size([512, 1, 31]) from checkpoint, the shape in current model is torch.Size([768, 1, 31]).
	size mismatch for 1.transformer.encoder.layers.10.convolution_module.conv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.10.convolution_module.after_conv.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.10.convolution_module.after_conv.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.10.convolution_module.after_conv.2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.10.convolution_module.after_conv.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.10.ffn_module1.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.10.ffn_module1.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.10.ffn_module1.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.10.ffn_module1.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.10.ffn_module1.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.10.ffn_module2.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.10.ffn_module2.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.10.ffn_module2.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.10.ffn_module2.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.10.ffn_module2.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.10.norm1.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.10.norm1.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.10.norm2.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.10.norm2.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.11.mha_layer.in_proj_weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([2304, 768]).
	size mismatch for 1.transformer.encoder.layers.11.mha_layer.pos_bias_u: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.11.mha_layer.pos_bias_v: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([96, 8]).
	size mismatch for 1.transformer.encoder.layers.11.mha_layer.out_proj.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.11.mha_layer.out_proj.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.11.mha_layer.linear_pos.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.11.convolution_module.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.11.convolution_module.layer_norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.11.convolution_module.bottleneck.0.weight: copying a param with shape torch.Size([1024, 512, 1]) from checkpoint, the shape in current model is torch.Size([1536, 768, 1]).
	size mismatch for 1.transformer.encoder.layers.11.convolution_module.bottleneck.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([1536]).
	size mismatch for 1.transformer.encoder.layers.11.convolution_module.conv.weight: copying a param with shape torch.Size([512, 1, 31]) from checkpoint, the shape in current model is torch.Size([768, 1, 31]).
	size mismatch for 1.transformer.encoder.layers.11.convolution_module.conv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.11.convolution_module.after_conv.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.11.convolution_module.after_conv.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.11.convolution_module.after_conv.2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).
	size mismatch for 1.transformer.encoder.layers.11.convolution_module.after_conv.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.11.ffn_module1.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.11.ffn_module1.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.11.ffn_module1.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.11.ffn_module1.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.11.ffn_module1.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.11.ffn_module2.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.11.ffn_module2.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.11.ffn_module2.1.ffn.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).
	size mismatch for 1.transformer.encoder.layers.11.ffn_module2.1.ffn.3.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).
	size mismatch for 1.transformer.encoder.layers.11.ffn_module2.1.ffn.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.11.norm1.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.11.norm1.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.11.norm2.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.layers.11.norm2.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.norm.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.encoder.norm.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.custom_src_module.layers.0.w.weight: copying a param with shape torch.Size([512, 640]) from checkpoint, the shape in current model is torch.Size([768, 640]).
	size mismatch for 1.transformer.custom_src_module.layers.0.w.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for 1.transformer.custom_tgt_module.layers.0.emb.Embedding.weight: copying a param with shape torch.Size([5000, 512]) from checkpoint, the shape in current model is torch.Size([5000, 768]).
	size mismatch for 3.w.weight: copying a param with shape torch.Size([8192, 512]) from checkpoint, the shape in current model is torch.Size([8192, 768]).
