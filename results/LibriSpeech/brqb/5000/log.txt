2023-12-07 12:23:35,190 - speechbrain.core - INFO - Beginning experiment!
2023-12-07 12:23:35,190 - speechbrain.core - INFO - Experiment folder: results/LibriSpeech/brqb/5000
2023-12-07 12:23:35,938 - speechbrain.utils.superpowers - DEBUG - appdirs==1.4.4
attrs==23.1.0
black==19.10b0
certifi==2023.11.17
cfgv==3.4.0
charset-normalizer==3.3.2
click==8.0.4
cmake==3.27.7
distlib==0.3.7
entrypoints==0.3
filelock==3.13.1
flake8==3.7.9
fsspec==2023.10.0
huggingface-hub==0.19.4
HyperPyYAML==1.2.2
hypothesis==6.91.0
identify==2.5.32
idna==3.6
iniconfig==2.0.0
Jinja2==3.1.2
joblib==1.3.2
kenlm==0.2.0
lit==17.0.6
MarkupSafe==2.1.3
mccabe==0.6.1
mpmath==1.3.0
networkx==3.2.1
nodeenv==1.8.0
numpy==1.26.2
nvidia-cublas-cu11==11.10.3.66
nvidia-cublas-cu12==12.1.3.1
nvidia-cuda-cupti-cu11==11.7.101
nvidia-cuda-cupti-cu12==12.1.105
nvidia-cuda-nvrtc-cu11==11.7.99
nvidia-cuda-nvrtc-cu12==12.1.105
nvidia-cuda-runtime-cu11==11.7.99
nvidia-cuda-runtime-cu12==12.1.105
nvidia-cudnn-cu11==8.5.0.96
nvidia-cudnn-cu12==8.9.2.26
nvidia-cufft-cu11==10.9.0.58
nvidia-cufft-cu12==11.0.2.54
nvidia-curand-cu11==10.2.10.91
nvidia-curand-cu12==10.3.2.106
nvidia-cusolver-cu11==11.4.0.1
nvidia-cusolver-cu12==11.4.5.107
nvidia-cusparse-cu11==11.7.4.91
nvidia-cusparse-cu12==12.1.0.106
nvidia-nccl-cu11==2.14.3
nvidia-nccl-cu12==2.18.1
nvidia-nvjitlink-cu12==12.3.101
nvidia-nvtx-cu11==11.7.91
nvidia-nvtx-cu12==12.1.105
packaging==23.2
pandas==2.1.3
pathspec==0.11.2
platformdirs==4.0.0
pluggy==1.3.0
pre-commit==3.5.0
pycodestyle==2.5.0
pyctcdecode==0.5.0
pyflakes==2.1.1
pygtrie==2.5.0
pytest==7.4.0
python-dateutil==2.8.2
pytz==2023.3.post1
PyYAML==6.0.1
regex==2023.10.3
requests==2.31.0
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
safetensors==0.4.1
scipy==1.11.4
sentencepiece==0.1.99
six==1.16.0
sortedcontainers==2.4.0
-e git+https://github.com/whettenr/speechbrain.git@b545d914c7e1677cae6d8fe030a00f7594b78475#egg=speechbrain
sympy==1.12
tokenizers==0.15.0
toml==0.10.2
torch==2.0.1
torchaudio==2.0.2
tqdm==4.66.1
transformers==4.35.2
triton==2.0.0
typed-ast==1.5.5
typing_extensions==4.8.0
tzdata==2023.3
urllib3==2.1.0
virtualenv==20.24.7
yamllint==1.23.0


2023-12-07 12:23:35,955 - speechbrain.utils.superpowers - DEBUG - a97c3dd


2023-12-07 12:23:36,340 - speechbrain.dataio.encoder - DEBUG - Would load categorical encoding from results/LibriSpeech/brqb/5000/save/label_encoder.txt, but file doesn't exist yet.
2023-12-07 12:23:36,861 - speechbrain.dataio.encoder - INFO - Moving label 'T' from index 0, because '<blank>' was put at its place.
2023-12-07 12:23:36,869 - speechbrain.dataio.encoder - INFO - Load called, but CTCTextEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.
2023-12-07 12:23:36,875 - speechbrain.dataio.encoder - DEBUG - Loaded categorical encoding from results/LibriSpeech/brqb/5000/save/label_encoder.txt
2023-12-07 12:23:36,875 - speechbrain.core - INFO - Info: auto_mix_prec arg from hparam file is used
2023-12-07 12:23:36,875 - speechbrain.core - INFO - Info: ckpt_interval_minutes arg from hparam file is used
2023-12-07 12:23:40,053 - speechbrain.core - INFO - 46.3M trainable parameters in ASR
2023-12-07 12:23:40,053 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results/LibriSpeech/brqb/5000/save.
2023-12-07 12:23:40,054 - speechbrain.pretrained.fetching - INFO - Destination model.ckpt: local file in /gpfswork/rech/nkp/uaj64gk/bestrqexp/jz/brqwide/model.ckpt.
2023-12-07 12:23:40,055 - speechbrain.utils.parameter_transfer - INFO - Set local path in self.paths[weighted_ssl_model] = results/LibriSpeech/brqb/5000/save/weighted_ssl_model.ckpt
2023-12-07 12:23:40,056 - speechbrain.pretrained.fetching - INFO - Destination normalizer.ckpt: local file in /gpfswork/rech/nkp/uaj64gk/bestrqexp/jz/brqwide/normalizer.ckpt.
2023-12-07 12:23:40,057 - speechbrain.utils.parameter_transfer - INFO - Set local path in self.paths[normalize] = results/LibriSpeech/brqb/5000/save/normalize.ckpt
2023-12-07 12:23:40,066 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: weighted_ssl_model, normalize
2023-12-07 12:23:40,066 - speechbrain.utils.parameter_transfer - INFO - Redirecting (loading from local path): results/LibriSpeech/brqb/5000/save/weighted_ssl_model.ckpt -> results/LibriSpeech/brqb/5000/save/weighted_ssl_model.ckpt
2023-12-07 12:23:40,066 - speechbrain.utils.parameter_transfer - INFO - Redirecting (loading from local path): results/LibriSpeech/brqb/5000/save/normalize.ckpt -> results/LibriSpeech/brqb/5000/save/normalize.ckpt
2023-12-07 12:23:42,413 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to ModuleList(
  (0): ConvolutionFrontEnd(
    (convblock_0): ConvBlock(
      (convs): Sequential(
        (conv_0): Conv2d(
          (conv): Conv2d(1, 128, kernel_size=(3, 3), stride=(2, 2))
        )
        (norm_0): LayerNorm(
          (norm): LayerNorm((40, 128), eps=1e-05, elementwise_affine=True)
        )
        (act_0): LeakyReLU(negative_slope=0.01)
        (dropout_0): Dropout(p=0.1, inplace=False)
      )
    )
    (convblock_1): ConvBlock(
      (convs): Sequential(
        (conv_0): Conv2d(
          (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(2, 2))
        )
        (norm_0): LayerNorm(
          (norm): LayerNorm((20, 32), eps=1e-05, elementwise_affine=True)
        )
        (act_0): LeakyReLU(negative_slope=0.01)
        (dropout_0): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (1): WeightedEncoderWrapper(
    (transformer): TransformerASR(
      (positional_encoding): RelPosEncXL()
      (positional_encoding_decoder): PositionalEncoding()
      (encoder): ConformerEncoder(
        (layers): ModuleList(
          (0-11): 12 x ConformerEncoderLayer(
            (mha_layer): RelPosMHAXL(
              (dropout_att): Dropout(p=0.1, inplace=False)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
              (linear_pos): Linear(in_features=768, out_features=768, bias=False)
            )
            (convolution_module): ConvolutionModule(
              (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (bottleneck): Sequential(
                (0): Conv1d(768, 1536, kernel_size=(1,), stride=(1,))
                (1): GLU(dim=1)
              )
              (conv): Conv1d(768, 768, kernel_size=(31,), stride=(1,), padding=(15,), groups=768)
              (after_conv): Sequential(
                (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (1): Swish(
                  (sigmoid): Sigmoid()
                )
                (2): Linear(in_features=768, out_features=768, bias=True)
                (3): Dropout(p=0.1, inplace=False)
              )
            )
            (ffn_module1): Sequential(
              (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (1): PositionalwiseFeedForward(
                (ffn): Sequential(
                  (0): Linear(in_features=768, out_features=2048, bias=True)
                  (1): Swish(
                    (sigmoid): Sigmoid()
                  )
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=2048, out_features=768, bias=True)
                )
              )
              (2): Dropout(p=0.1, inplace=False)
            )
            (ffn_module2): Sequential(
              (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (1): PositionalwiseFeedForward(
                (ffn): Sequential(
                  (0): Linear(in_features=768, out_features=2048, bias=True)
                  (1): Swish(
                    (sigmoid): Sigmoid()
                  )
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=2048, out_features=768, bias=True)
                )
              )
              (2): Dropout(p=0.1, inplace=False)
            )
            (norm1): LayerNorm(
              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (norm2): LayerNorm(
              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (drop): Dropout(p=0.1, inplace=False)
          )
        )
        (norm): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
      )
      (custom_src_module): ModuleList(
        (layers): ModuleList(
          (0): Linear(
            (w): Linear(in_features=640, out_features=768, bias=True)
          )
          (1): Dropout(p=0.1, inplace=False)
        )
      )
      (custom_tgt_module): ModuleList(
        (layers): ModuleList(
          (0): NormalizedEmbedding(
            (emb): Embedding(
              (Embedding): Embedding(5000, 768)
            )
          )
        )
      )
    )
  )
  (2): RandomProjectionQuantizer()
  (3): Linear(
    (w): Linear(in_features=768, out_features=8192, bias=True)
  )
) loading from results/LibriSpeech/brqb/5000/save/weighted_ssl_model.ckpt, the transferred parameters did not have parameters for the key: 1.weights
2023-12-07 12:23:42,417 - speechbrain.utils.checkpoints - INFO - Would load a checkpoint here, but none found yet.
2023-12-07 12:23:42,417 - speechbrain.utils.epoch_loop - INFO - Going into epoch 1
2023-12-07 12:29:39,288 - speechbrain.core - ERROR - Exception:
Traceback (most recent call last):
  File "/gpfswork/rech/nkp/uaj64gk/bestrqexp/benchmark/benchmarks/MP3S/LibriSpeech/LSTM/train_brq.py", line 376, in <module>
    asr_brain.fit(
  File "/gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain/speechbrain/core.py", line 1366, in fit
    self._fit_train(train_set=train_set, epoch=epoch, enable=enable)
  File "/gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain/speechbrain/core.py", line 1193, in _fit_train
    loss = self.fit_batch(batch)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfswork/rech/nkp/uaj64gk/bestrqexp/benchmark/benchmarks/MP3S/LibriSpeech/LSTM/train_brq.py", line 93, in fit_batch
    if self.check_gradients(loss):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain/speechbrain/core.py", line 1131, in check_gradients
    torch.nn.utils.clip_grad_norm_(
  File "/linkhome/rech/genzjn01/uaj64gk/.local/lib/python3.11/site-packages/torch/nn/utils/clip_grad.py", line 12, in clip_grad_norm_
    def clip_grad_norm_(
    
KeyboardInterrupt
2023-12-07 12:37:05,635 - speechbrain.core - INFO - Beginning experiment!
2023-12-07 12:37:05,649 - speechbrain.core - INFO - Experiment folder: results/LibriSpeech/brqb/5000
2023-12-07 12:37:08,706 - speechbrain.utils.superpowers - DEBUG - appdirs==1.4.4
attrs==23.1.0
black==19.10b0
certifi==2023.11.17
cfgv==3.4.0
charset-normalizer==3.3.2
click==8.0.4
cmake==3.27.7
distlib==0.3.7
entrypoints==0.3
filelock==3.13.1
flake8==3.7.9
fsspec==2023.10.0
huggingface-hub==0.19.4
HyperPyYAML==1.2.2
hypothesis==6.91.0
identify==2.5.32
idna==3.6
iniconfig==2.0.0
Jinja2==3.1.2
joblib==1.3.2
kenlm==0.2.0
lit==17.0.6
MarkupSafe==2.1.3
mccabe==0.6.1
mpmath==1.3.0
networkx==3.2.1
nodeenv==1.8.0
numpy==1.26.2
nvidia-cublas-cu11==11.10.3.66
nvidia-cublas-cu12==12.1.3.1
nvidia-cuda-cupti-cu11==11.7.101
nvidia-cuda-cupti-cu12==12.1.105
nvidia-cuda-nvrtc-cu11==11.7.99
nvidia-cuda-nvrtc-cu12==12.1.105
nvidia-cuda-runtime-cu11==11.7.99
nvidia-cuda-runtime-cu12==12.1.105
nvidia-cudnn-cu11==8.5.0.96
nvidia-cudnn-cu12==8.9.2.26
nvidia-cufft-cu11==10.9.0.58
nvidia-cufft-cu12==11.0.2.54
nvidia-curand-cu11==10.2.10.91
nvidia-curand-cu12==10.3.2.106
nvidia-cusolver-cu11==11.4.0.1
nvidia-cusolver-cu12==11.4.5.107
nvidia-cusparse-cu11==11.7.4.91
nvidia-cusparse-cu12==12.1.0.106
nvidia-nccl-cu11==2.14.3
nvidia-nccl-cu12==2.18.1
nvidia-nvjitlink-cu12==12.3.101
nvidia-nvtx-cu11==11.7.91
nvidia-nvtx-cu12==12.1.105
packaging==23.2
pandas==2.1.3
pathspec==0.11.2
platformdirs==4.0.0
pluggy==1.3.0
pre-commit==3.5.0
pycodestyle==2.5.0
pyctcdecode==0.5.0
pyflakes==2.1.1
pygtrie==2.5.0
pytest==7.4.0
python-dateutil==2.8.2
pytz==2023.3.post1
PyYAML==6.0.1
regex==2023.10.3
requests==2.31.0
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
safetensors==0.4.1
scipy==1.11.4
sentencepiece==0.1.99
six==1.16.0
sortedcontainers==2.4.0
-e /gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain
sympy==1.12
tokenizers==0.15.0
toml==0.10.2
torch==2.0.1
torchaudio==2.0.2
tqdm==4.66.1
transformers==4.35.2
triton==2.0.0
typed-ast==1.5.5
typing_extensions==4.8.0
tzdata==2023.3
urllib3==2.1.0
virtualenv==20.24.7
yamllint==1.23.0

ERROR: Error [Errno 2] No such file or directory: 'git' while executing command git config --get-regexp 'remote\..*\.url'
WARNING: cannot determine version of editable source in /gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain (git command not found in path)

2023-12-07 12:37:09,370 - speechbrain.dataio.encoder - DEBUG - Loaded categorical encoding from results/LibriSpeech/brqb/5000/save/label_encoder.txt
2023-12-07 12:37:09,370 - speechbrain.dataio.encoder - INFO - Load called, but CTCTextEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.
2023-12-07 12:37:09,371 - speechbrain.dataio.encoder - DEBUG - Loaded categorical encoding from results/LibriSpeech/brqb/5000/save/label_encoder.txt
2023-12-07 12:37:09,371 - speechbrain.core - INFO - Info: auto_mix_prec arg from hparam file is used
2023-12-07 12:37:09,371 - speechbrain.core - INFO - Info: ckpt_interval_minutes arg from hparam file is used
2023-12-07 12:37:15,514 - speechbrain.core - INFO - 46.3M trainable parameters in ASR
2023-12-07 12:37:15,515 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results/LibriSpeech/brqb/5000/save.
2023-12-07 12:37:15,539 - speechbrain.pretrained.fetching - INFO - Fetch model.ckpt: Using existing file/symlink in results/LibriSpeech/brqb/5000/save/weighted_ssl_model.ckpt.
2023-12-07 12:37:15,540 - speechbrain.utils.parameter_transfer - INFO - Set local path in self.paths[weighted_ssl_model] = results/LibriSpeech/brqb/5000/save/weighted_ssl_model.ckpt
2023-12-07 12:37:15,562 - speechbrain.pretrained.fetching - INFO - Fetch normalizer.ckpt: Using existing file/symlink in results/LibriSpeech/brqb/5000/save/normalize.ckpt.
2023-12-07 12:37:15,563 - speechbrain.utils.parameter_transfer - INFO - Set local path in self.paths[normalize] = results/LibriSpeech/brqb/5000/save/normalize.ckpt
2023-12-07 12:37:15,563 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: weighted_ssl_model, normalize
2023-12-07 12:37:15,563 - speechbrain.utils.parameter_transfer - INFO - Redirecting (loading from local path): results/LibriSpeech/brqb/5000/save/weighted_ssl_model.ckpt -> results/LibriSpeech/brqb/5000/save/weighted_ssl_model.ckpt
2023-12-07 12:37:15,563 - speechbrain.utils.parameter_transfer - INFO - Redirecting (loading from local path): results/LibriSpeech/brqb/5000/save/normalize.ckpt -> results/LibriSpeech/brqb/5000/save/normalize.ckpt
2023-12-07 12:37:17,407 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to ModuleList(
  (0): ConvolutionFrontEnd(
    (convblock_0): ConvBlock(
      (convs): Sequential(
        (conv_0): Conv2d(
          (conv): Conv2d(1, 128, kernel_size=(3, 3), stride=(2, 2))
        )
        (norm_0): LayerNorm(
          (norm): LayerNorm((40, 128), eps=1e-05, elementwise_affine=True)
        )
        (act_0): LeakyReLU(negative_slope=0.01)
        (dropout_0): Dropout(p=0.1, inplace=False)
      )
    )
    (convblock_1): ConvBlock(
      (convs): Sequential(
        (conv_0): Conv2d(
          (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(2, 2))
        )
        (norm_0): LayerNorm(
          (norm): LayerNorm((20, 32), eps=1e-05, elementwise_affine=True)
        )
        (act_0): LeakyReLU(negative_slope=0.01)
        (dropout_0): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (1): WeightedEncoderWrapper(
    (transformer): TransformerASR(
      (positional_encoding): RelPosEncXL()
      (positional_encoding_decoder): PositionalEncoding()
      (encoder): ConformerEncoder(
        (layers): ModuleList(
          (0-11): 12 x ConformerEncoderLayer(
            (mha_layer): RelPosMHAXL(
              (dropout_att): Dropout(p=0.1, inplace=False)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
              (linear_pos): Linear(in_features=768, out_features=768, bias=False)
            )
            (convolution_module): ConvolutionModule(
              (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (bottleneck): Sequential(
                (0): Conv1d(768, 1536, kernel_size=(1,), stride=(1,))
                (1): GLU(dim=1)
              )
              (conv): Conv1d(768, 768, kernel_size=(31,), stride=(1,), padding=(15,), groups=768)
              (after_conv): Sequential(
                (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (1): Swish(
                  (sigmoid): Sigmoid()
                )
                (2): Linear(in_features=768, out_features=768, bias=True)
                (3): Dropout(p=0.1, inplace=False)
              )
            )
            (ffn_module1): Sequential(
              (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (1): PositionalwiseFeedForward(
                (ffn): Sequential(
                  (0): Linear(in_features=768, out_features=2048, bias=True)
                  (1): Swish(
                    (sigmoid): Sigmoid()
                  )
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=2048, out_features=768, bias=True)
                )
              )
              (2): Dropout(p=0.1, inplace=False)
            )
            (ffn_module2): Sequential(
              (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (1): PositionalwiseFeedForward(
                (ffn): Sequential(
                  (0): Linear(in_features=768, out_features=2048, bias=True)
                  (1): Swish(
                    (sigmoid): Sigmoid()
                  )
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=2048, out_features=768, bias=True)
                )
              )
              (2): Dropout(p=0.1, inplace=False)
            )
            (norm1): LayerNorm(
              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (norm2): LayerNorm(
              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (drop): Dropout(p=0.1, inplace=False)
          )
        )
        (norm): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
      )
      (custom_src_module): ModuleList(
        (layers): ModuleList(
          (0): Linear(
            (w): Linear(in_features=640, out_features=768, bias=True)
          )
          (1): Dropout(p=0.1, inplace=False)
        )
      )
      (custom_tgt_module): ModuleList(
        (layers): ModuleList(
          (0): NormalizedEmbedding(
            (emb): Embedding(
              (Embedding): Embedding(5000, 768)
            )
          )
        )
      )
    )
  )
  (2): RandomProjectionQuantizer()
  (3): Linear(
    (w): Linear(in_features=768, out_features=8192, bias=True)
  )
) loading from results/LibriSpeech/brqb/5000/save/weighted_ssl_model.ckpt, the transferred parameters did not have parameters for the key: 1.weights
2023-12-07 12:37:17,477 - speechbrain.utils.checkpoints - INFO - Would load a checkpoint here, but none found yet.
2023-12-07 12:37:17,477 - speechbrain.utils.epoch_loop - INFO - Going into epoch 1
2023-12-07 13:02:04,048 - speechbrain.utils.train_logger - INFO - epoch: 1, lr_model: 2.00e-04 - train loss: 5.26e-01 - valid loss: 3.20e-01, valid CER: 7.54, valid WER: 26.27
2023-12-07 13:02:05,795 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+13-02-04+00
2023-12-07 13:02:05,798 - speechbrain.utils.epoch_loop - INFO - Going into epoch 2
2023-12-07 13:26:10,537 - speechbrain.utils.train_logger - INFO - epoch: 2, lr_model: 2.00e-04 - train loss: 2.66e-01 - valid loss: 2.77e-01, valid CER: 6.25, valid WER: 22.03
2023-12-07 13:26:11,770 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+13-26-10+00
2023-12-07 13:26:11,814 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+13-02-04+00
2023-12-07 13:26:11,814 - speechbrain.utils.epoch_loop - INFO - Going into epoch 3
2023-12-07 13:50:20,205 - speechbrain.utils.train_logger - INFO - epoch: 3, lr_model: 2.00e-04 - train loss: 2.09e-01 - valid loss: 2.53e-01, valid CER: 5.95, valid WER: 20.82
2023-12-07 13:50:21,845 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+13-50-20+00
2023-12-07 13:50:21,860 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+13-26-10+00
2023-12-07 13:50:21,860 - speechbrain.utils.epoch_loop - INFO - Going into epoch 4
2023-12-07 14:14:22,495 - speechbrain.utils.train_logger - INFO - epoch: 4, lr_model: 2.00e-04 - train loss: 1.77e-01 - valid loss: 2.51e-01, valid CER: 5.64, valid WER: 19.78
2023-12-07 14:14:58,808 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+14-14-22+00
2023-12-07 14:14:58,828 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+13-50-20+00
2023-12-07 14:14:58,828 - speechbrain.utils.epoch_loop - INFO - Going into epoch 5
2023-12-07 14:38:59,506 - speechbrain.utils.train_logger - INFO - epoch: 5, lr_model: 2.00e-04 - train loss: 1.54e-01 - valid loss: 2.48e-01, valid CER: 5.61, valid WER: 19.39
2023-12-07 14:40:56,804 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+14-38-59+00
2023-12-07 14:40:56,850 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+14-14-22+00
2023-12-07 14:40:56,850 - speechbrain.utils.epoch_loop - INFO - Going into epoch 6
2023-12-07 15:04:58,758 - speechbrain.utils.train_logger - INFO - epoch: 6, lr_model: 2.00e-04 - train loss: 1.37e-01 - valid loss: 2.44e-01, valid CER: 5.47, valid WER: 19.08
2023-12-07 15:06:11,726 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+15-04-58+00
2023-12-07 15:06:11,750 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+14-38-59+00
2023-12-07 15:06:11,750 - speechbrain.utils.epoch_loop - INFO - Going into epoch 7
2023-12-07 15:30:32,632 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.0002 to 0.00016
2023-12-07 15:30:32,654 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.01 to 0.009
2023-12-07 15:30:32,655 - speechbrain.utils.train_logger - INFO - epoch: 7, lr_model: 2.00e-04 - train loss: 1.23e-01 - valid loss: 2.50e-01, valid CER: 5.87, valid WER: 20.34
2023-12-07 15:30:42,009 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+15-30-32+00
2023-12-07 15:30:42,030 - speechbrain.utils.epoch_loop - INFO - Going into epoch 8
2023-12-07 15:54:51,580 - speechbrain.utils.train_logger - INFO - epoch: 8, lr_model: 1.60e-04 - train loss: 1.02e-01 - valid loss: 2.43e-01, valid CER: 5.19, valid WER: 18.03
2023-12-07 15:54:59,046 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+15-54-51+00
2023-12-07 15:54:59,097 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+15-04-58+00
2023-12-07 15:54:59,103 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+15-30-32+00
2023-12-07 15:54:59,104 - speechbrain.utils.epoch_loop - INFO - Going into epoch 9
2023-12-07 16:19:02,890 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.00016 to 0.00013
2023-12-07 16:19:02,902 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.009 to 0.0081
2023-12-07 16:19:02,902 - speechbrain.utils.train_logger - INFO - epoch: 9, lr_model: 1.60e-04 - train loss: 8.98e-02 - valid loss: 2.48e-01, valid CER: 5.18, valid WER: 17.98
2023-12-07 16:19:55,628 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+16-19-02+00
2023-12-07 16:19:55,705 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+15-54-51+00
2023-12-07 16:19:55,705 - speechbrain.utils.epoch_loop - INFO - Going into epoch 10
2023-12-07 16:43:55,097 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.00013 to 0.0001
2023-12-07 16:43:55,114 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.0081 to 0.0073
2023-12-07 16:43:55,156 - speechbrain.utils.train_logger - INFO - epoch: 10, lr_model: 1.28e-04 - train loss: 7.47e-02 - valid loss: 2.54e-01, valid CER: 5.03, valid WER: 17.39
2023-12-07 16:43:57,007 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+16-43-55+00
2023-12-07 16:43:57,059 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+16-19-02+00
2023-12-07 16:43:57,059 - speechbrain.utils.epoch_loop - INFO - Going into epoch 11
2023-12-07 17:07:46,977 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.0001 to 8.2e-05
2023-12-07 17:07:46,977 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.0073 to 0.0066
2023-12-07 17:07:46,979 - speechbrain.utils.train_logger - INFO - epoch: 11, lr_model: 1.02e-04 - train loss: 6.08e-02 - valid loss: 2.56e-01, valid CER: 4.81, valid WER: 16.59
2023-12-07 17:07:50,796 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+17-07-46+00
2023-12-07 17:07:50,840 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+16-43-55+00
2023-12-07 17:07:50,841 - speechbrain.utils.epoch_loop - INFO - Going into epoch 12
2023-12-07 17:31:49,382 - speechbrain.nnet.schedulers - INFO - Changing lr from 8.2e-05 to 6.6e-05
2023-12-07 17:31:49,423 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.0066 to 0.0059
2023-12-07 17:31:49,423 - speechbrain.utils.train_logger - INFO - epoch: 12, lr_model: 8.19e-05 - train loss: 4.98e-02 - valid loss: 2.64e-01, valid CER: 4.72, valid WER: 16.33
2023-12-07 17:33:13,550 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+17-31-49+00
2023-12-07 17:33:13,586 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+17-07-46+00
2023-12-07 17:33:13,587 - speechbrain.utils.epoch_loop - INFO - Going into epoch 13
2023-12-07 17:57:06,392 - speechbrain.nnet.schedulers - INFO - Changing lr from 6.6e-05 to 5.2e-05
2023-12-07 17:57:06,392 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.0059 to 0.0053
2023-12-07 17:57:06,392 - speechbrain.utils.train_logger - INFO - epoch: 13, lr_model: 6.55e-05 - train loss: 4.10e-02 - valid loss: 2.66e-01, valid CER: 4.73, valid WER: 16.22
2023-12-07 17:57:57,771 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+17-57-06+00
2023-12-07 17:57:57,816 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+17-31-49+00
2023-12-07 17:57:57,817 - speechbrain.utils.epoch_loop - INFO - Going into epoch 14
2023-12-07 18:21:48,847 - speechbrain.nnet.schedulers - INFO - Changing lr from 5.2e-05 to 4.2e-05
2023-12-07 18:21:48,847 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.0053 to 0.0048
2023-12-07 18:21:48,847 - speechbrain.utils.train_logger - INFO - epoch: 14, lr_model: 5.24e-05 - train loss: 3.36e-02 - valid loss: 2.70e-01, valid CER: 4.66, valid WER: 16.02
2023-12-07 18:32:27,900 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+18-21-48+00
2023-12-07 18:32:27,965 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+17-57-06+00
2023-12-07 18:32:27,965 - speechbrain.utils.epoch_loop - INFO - Going into epoch 15
2023-12-07 18:56:18,251 - speechbrain.nnet.schedulers - INFO - Changing lr from 4.2e-05 to 3.4e-05
2023-12-07 18:56:18,273 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.0048 to 0.0043
2023-12-07 18:56:18,273 - speechbrain.utils.train_logger - INFO - epoch: 15, lr_model: 4.19e-05 - train loss: 2.83e-02 - valid loss: 2.76e-01, valid CER: 4.62, valid WER: 15.75
2023-12-07 19:03:30,487 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+18-56-18+00
2023-12-07 19:03:30,560 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+18-21-48+00
2023-12-07 19:03:30,560 - speechbrain.utils.epoch_loop - INFO - Going into epoch 16
2023-12-07 19:27:23,316 - speechbrain.nnet.schedulers - INFO - Changing lr from 3.4e-05 to 2.7e-05
2023-12-07 19:27:23,336 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.0043 to 0.0039
2023-12-07 19:27:23,337 - speechbrain.utils.train_logger - INFO - epoch: 16, lr_model: 3.36e-05 - train loss: 2.49e-02 - valid loss: 2.78e-01, valid CER: 4.59, valid WER: 15.70
2023-12-07 19:29:29,903 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+19-27-23+00
2023-12-07 19:29:29,980 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+18-56-18+00
2023-12-07 19:29:29,981 - speechbrain.utils.epoch_loop - INFO - Going into epoch 17
2023-12-07 19:53:22,241 - speechbrain.nnet.schedulers - INFO - Changing lr from 2.7e-05 to 2.1e-05
2023-12-07 19:53:22,249 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.0039 to 0.0035
2023-12-07 19:53:22,249 - speechbrain.utils.train_logger - INFO - epoch: 17, lr_model: 2.68e-05 - train loss: 2.18e-02 - valid loss: 2.79e-01, valid CER: 4.57, valid WER: 15.46
2023-12-07 19:54:08,538 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+19-53-22+00
2023-12-07 19:54:08,589 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+19-27-23+00
2023-12-07 19:54:08,589 - speechbrain.utils.epoch_loop - INFO - Going into epoch 18
2023-12-07 20:18:01,572 - speechbrain.nnet.schedulers - INFO - Changing lr from 2.1e-05 to 1.7e-05
2023-12-07 20:18:01,589 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.0035 to 0.0031
2023-12-07 20:18:01,589 - speechbrain.utils.train_logger - INFO - epoch: 18, lr_model: 2.15e-05 - train loss: 1.97e-02 - valid loss: 2.88e-01, valid CER: 4.56, valid WER: 15.66
2023-12-07 20:18:11,055 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+20-18-01+00
2023-12-07 20:18:11,100 - speechbrain.utils.epoch_loop - INFO - Going into epoch 19
2023-12-07 20:42:04,568 - speechbrain.nnet.schedulers - INFO - Changing lr from 1.7e-05 to 1.4e-05
2023-12-07 20:42:04,583 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.0031 to 0.0028
2023-12-07 20:42:04,583 - speechbrain.utils.train_logger - INFO - epoch: 19, lr_model: 1.72e-05 - train loss: 1.75e-02 - valid loss: 2.88e-01, valid CER: 4.50, valid WER: 15.41
2023-12-07 20:43:44,452 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+20-42-04+00
2023-12-07 20:43:44,555 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+20-18-01+00
2023-12-07 20:43:44,558 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+19-53-22+00
2023-12-07 20:43:44,559 - speechbrain.utils.epoch_loop - INFO - Going into epoch 20
2023-12-07 21:07:35,994 - speechbrain.nnet.schedulers - INFO - Changing lr from 1.4e-05 to 1.1e-05
2023-12-07 21:07:36,013 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.0028 to 0.0025
2023-12-07 21:07:36,013 - speechbrain.utils.train_logger - INFO - epoch: 20, lr_model: 1.37e-05 - train loss: 1.62e-02 - valid loss: 2.91e-01, valid CER: 4.52, valid WER: 15.46
2023-12-07 21:08:24,194 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+21-07-36+00
2023-12-07 21:08:24,263 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+21-07-36+00
2023-12-07 21:08:25,150 - speechbrain.dataio.encoder - DEBUG - Loaded categorical encoding from results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+21-07-36+00/tokenizer.ckpt
2023-12-07 21:08:25,151 - root - DEBUG - SaveableDataLoader was requested to load a checkpoint, but the DataLoader has already been iterated. The DataLoader file will be ignored. This is normal in evaluation, when a checkpoint is loaded just to retrieve the best model.
2023-12-07 21:09:33,725 - speechbrain.utils.train_logger - INFO - Epoch loaded: 20 - test loss: 3.05e-01, test CER: 4.43, test WER: 15.14
2023-12-07 21:09:44,667 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+21-07-36+00
2023-12-07 21:09:45,544 - speechbrain.dataio.encoder - DEBUG - Loaded categorical encoding from results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+21-07-36+00/tokenizer.ckpt
2023-12-07 21:09:45,545 - root - DEBUG - SaveableDataLoader was requested to load a checkpoint, but the DataLoader has already been iterated. The DataLoader file will be ignored. This is normal in evaluation, when a checkpoint is loaded just to retrieve the best model.
2023-12-07 21:10:53,131 - speechbrain.utils.train_logger - INFO - Epoch loaded: 20 - test loss: 8.80e-01, test CER: 12.01, test WER: 33.13
2023-12-08 12:30:32,592 - speechbrain.core - INFO - Beginning experiment!
2023-12-08 12:30:32,608 - speechbrain.core - INFO - Experiment folder: results/LibriSpeech/brqb/5000
2023-12-08 12:30:38,365 - speechbrain.utils.superpowers - DEBUG - appdirs==1.4.4
attrs==23.1.0
black==19.10b0
certifi==2023.11.17
cfgv==3.4.0
charset-normalizer==3.3.2
click==8.0.4
cmake==3.27.7
distlib==0.3.7
entrypoints==0.3
filelock==3.13.1
flake8==3.7.9
fsspec==2023.10.0
huggingface-hub==0.19.4
HyperPyYAML==1.2.2
hypothesis==6.91.0
identify==2.5.32
idna==3.6
iniconfig==2.0.0
Jinja2==3.1.2
joblib==1.3.2
kenlm==0.2.0
lit==17.0.6
MarkupSafe==2.1.3
mccabe==0.6.1
mpmath==1.3.0
networkx==3.2.1
nodeenv==1.8.0
numpy==1.26.2
nvidia-cublas-cu11==11.10.3.66
nvidia-cublas-cu12==12.1.3.1
nvidia-cuda-cupti-cu11==11.7.101
nvidia-cuda-cupti-cu12==12.1.105
nvidia-cuda-nvrtc-cu11==11.7.99
nvidia-cuda-nvrtc-cu12==12.1.105
nvidia-cuda-runtime-cu11==11.7.99
nvidia-cuda-runtime-cu12==12.1.105
nvidia-cudnn-cu11==8.5.0.96
nvidia-cudnn-cu12==8.9.2.26
nvidia-cufft-cu11==10.9.0.58
nvidia-cufft-cu12==11.0.2.54
nvidia-curand-cu11==10.2.10.91
nvidia-curand-cu12==10.3.2.106
nvidia-cusolver-cu11==11.4.0.1
nvidia-cusolver-cu12==11.4.5.107
nvidia-cusparse-cu11==11.7.4.91
nvidia-cusparse-cu12==12.1.0.106
nvidia-nccl-cu11==2.14.3
nvidia-nccl-cu12==2.18.1
nvidia-nvjitlink-cu12==12.3.101
nvidia-nvtx-cu11==11.7.91
nvidia-nvtx-cu12==12.1.105
packaging==23.2
pandas==2.1.3
pathspec==0.11.2
platformdirs==4.0.0
pluggy==1.3.0
pre-commit==3.5.0
pycodestyle==2.5.0
pyctcdecode==0.5.0
pyflakes==2.1.1
pygtrie==2.5.0
pytest==7.4.0
python-dateutil==2.8.2
pytz==2023.3.post1
PyYAML==6.0.1
regex==2023.10.3
requests==2.31.0
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
safetensors==0.4.1
scipy==1.11.4
sentencepiece==0.1.99
six==1.16.0
sortedcontainers==2.4.0
-e git+https://github.com/whettenr/speechbrain.git@b545d914c7e1677cae6d8fe030a00f7594b78475#egg=speechbrain
sympy==1.12
tokenizers==0.15.0
toml==0.10.2
torch==2.0.1
torchaudio==2.0.2
tqdm==4.66.1
transformers==4.35.2
triton==2.0.0
typed-ast==1.5.5
typing_extensions==4.8.0
tzdata==2023.3
urllib3==2.1.0
virtualenv==20.24.7
yamllint==1.23.0


2023-12-08 12:30:38,491 - speechbrain.utils.superpowers - DEBUG - 3a645a0


2023-12-08 12:30:39,085 - speechbrain.dataio.encoder - DEBUG - Loaded categorical encoding from results/LibriSpeech/brqb/5000/save/label_encoder.txt
2023-12-08 12:30:39,085 - speechbrain.dataio.encoder - INFO - Load called, but CTCTextEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.
2023-12-08 12:30:39,085 - speechbrain.dataio.encoder - DEBUG - Loaded categorical encoding from results/LibriSpeech/brqb/5000/save/label_encoder.txt
2023-12-08 12:32:01,324 - pyctcdecode.decoder - WARNING - Unigrams not provided and cannot be automatically determined from LM file (only arpa format). Decoding accuracy might be reduced.
2023-12-08 12:32:01,343 - pyctcdecode.alphabet - INFO - Alphabet determined to be of regular style.
2023-12-08 12:32:01,344 - pyctcdecode.language_model - WARNING - No known unigrams provided, decoding results might be a lot worse.
2023-12-08 12:32:01,344 - speechbrain.core - INFO - Info: auto_mix_prec arg from hparam file is used
2023-12-08 12:32:01,344 - speechbrain.core - INFO - Info: ckpt_interval_minutes arg from hparam file is used
2023-12-08 12:32:09,529 - speechbrain.core - INFO - 46.3M trainable parameters in ASR
2023-12-08 12:32:09,529 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results/LibriSpeech/brqb/5000/save.
2023-12-08 12:32:09,585 - speechbrain.pretrained.fetching - INFO - Fetch model.ckpt: Using existing file/symlink in results/LibriSpeech/brqb/5000/save/weighted_ssl_model.ckpt.
2023-12-08 12:32:09,586 - speechbrain.utils.parameter_transfer - INFO - Set local path in self.paths[weighted_ssl_model] = results/LibriSpeech/brqb/5000/save/weighted_ssl_model.ckpt
2023-12-08 12:32:09,623 - speechbrain.pretrained.fetching - INFO - Fetch normalizer.ckpt: Using existing file/symlink in results/LibriSpeech/brqb/5000/save/normalize.ckpt.
2023-12-08 12:32:09,624 - speechbrain.utils.parameter_transfer - INFO - Set local path in self.paths[normalize] = results/LibriSpeech/brqb/5000/save/normalize.ckpt
2023-12-08 12:32:09,624 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: weighted_ssl_model, normalize
2023-12-08 12:32:09,624 - speechbrain.utils.parameter_transfer - INFO - Redirecting (loading from local path): results/LibriSpeech/brqb/5000/save/weighted_ssl_model.ckpt -> results/LibriSpeech/brqb/5000/save/weighted_ssl_model.ckpt
2023-12-08 12:32:09,624 - speechbrain.utils.parameter_transfer - INFO - Redirecting (loading from local path): results/LibriSpeech/brqb/5000/save/normalize.ckpt -> results/LibriSpeech/brqb/5000/save/normalize.ckpt
2023-12-08 12:32:12,547 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to ModuleList(
  (0): ConvolutionFrontEnd(
    (convblock_0): ConvBlock(
      (convs): Sequential(
        (conv_0): Conv2d(
          (conv): Conv2d(1, 128, kernel_size=(3, 3), stride=(2, 2))
        )
        (norm_0): LayerNorm(
          (norm): LayerNorm((40, 128), eps=1e-05, elementwise_affine=True)
        )
        (act_0): LeakyReLU(negative_slope=0.01)
        (dropout_0): Dropout(p=0.1, inplace=False)
      )
    )
    (convblock_1): ConvBlock(
      (convs): Sequential(
        (conv_0): Conv2d(
          (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(2, 2))
        )
        (norm_0): LayerNorm(
          (norm): LayerNorm((20, 32), eps=1e-05, elementwise_affine=True)
        )
        (act_0): LeakyReLU(negative_slope=0.01)
        (dropout_0): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (1): WeightedEncoderWrapper(
    (transformer): TransformerASR(
      (positional_encoding): RelPosEncXL()
      (positional_encoding_decoder): PositionalEncoding()
      (encoder): ConformerEncoder(
        (layers): ModuleList(
          (0-11): 12 x ConformerEncoderLayer(
            (mha_layer): RelPosMHAXL(
              (dropout_att): Dropout(p=0.1, inplace=False)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
              (linear_pos): Linear(in_features=768, out_features=768, bias=False)
            )
            (convolution_module): ConvolutionModule(
              (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (bottleneck): Sequential(
                (0): Conv1d(768, 1536, kernel_size=(1,), stride=(1,))
                (1): GLU(dim=1)
              )
              (conv): Conv1d(768, 768, kernel_size=(31,), stride=(1,), padding=(15,), groups=768)
              (after_conv): Sequential(
                (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (1): Swish(
                  (sigmoid): Sigmoid()
                )
                (2): Linear(in_features=768, out_features=768, bias=True)
                (3): Dropout(p=0.1, inplace=False)
              )
            )
            (ffn_module1): Sequential(
              (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (1): PositionalwiseFeedForward(
                (ffn): Sequential(
                  (0): Linear(in_features=768, out_features=2048, bias=True)
                  (1): Swish(
                    (sigmoid): Sigmoid()
                  )
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=2048, out_features=768, bias=True)
                )
              )
              (2): Dropout(p=0.1, inplace=False)
            )
            (ffn_module2): Sequential(
              (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (1): PositionalwiseFeedForward(
                (ffn): Sequential(
                  (0): Linear(in_features=768, out_features=2048, bias=True)
                  (1): Swish(
                    (sigmoid): Sigmoid()
                  )
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=2048, out_features=768, bias=True)
                )
              )
              (2): Dropout(p=0.1, inplace=False)
            )
            (norm1): LayerNorm(
              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (norm2): LayerNorm(
              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (drop): Dropout(p=0.1, inplace=False)
          )
        )
        (norm): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
      )
      (custom_src_module): ModuleList(
        (layers): ModuleList(
          (0): Linear(
            (w): Linear(in_features=640, out_features=768, bias=True)
          )
          (1): Dropout(p=0.1, inplace=False)
        )
      )
      (custom_tgt_module): ModuleList(
        (layers): ModuleList(
          (0): NormalizedEmbedding(
            (emb): Embedding(
              (Embedding): Embedding(5000, 768)
            )
          )
        )
      )
    )
  )
  (2): RandomProjectionQuantizer()
  (3): Linear(
    (w): Linear(in_features=768, out_features=8192, bias=True)
  )
) loading from results/LibriSpeech/brqb/5000/save/weighted_ssl_model.ckpt, the transferred parameters did not have parameters for the key: 1.weights
2023-12-08 12:32:12,549 - speechbrain.core - INFO - Test only mode, skipping training and validation stages.
2023-12-08 12:32:12,599 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+21-07-36+00
2023-12-08 12:32:15,264 - speechbrain.dataio.encoder - DEBUG - Loaded categorical encoding from results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+21-07-36+00/tokenizer.ckpt
2023-12-08 12:33:41,359 - speechbrain.utils.train_logger - INFO - Epoch loaded: 20 - test loss: 3.05e-01, test CER: 3.34, test WER: 9.67
2023-12-08 12:33:41,472 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+21-07-36+00
2023-12-08 12:33:43,925 - speechbrain.dataio.encoder - DEBUG - Loaded categorical encoding from results/LibriSpeech/brqb/5000/save/CKPT+2023-12-07+21-07-36+00/tokenizer.ckpt
2023-12-08 12:35:32,835 - speechbrain.utils.train_logger - INFO - Epoch loaded: 20 - test loss: 8.80e-01, test CER: 10.28, test WER: 24.16
