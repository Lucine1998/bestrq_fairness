2023-12-01 23:13:54,678 - speechbrain.core - INFO - Beginning experiment!
2023-12-01 23:13:54,684 - speechbrain.core - INFO - Experiment folder: results/LibriSpeech/brq_hyperconf/2000
2023-12-01 23:13:55,428 - speechbrain.utils.superpowers - DEBUG - appdirs==1.4.4
attrs==23.1.0
black==19.10b0
certifi==2023.11.17
cfgv==3.4.0
charset-normalizer==3.3.2
click==8.0.4
cmake==3.27.7
distlib==0.3.7
entrypoints==0.3
filelock==3.13.1
flake8==3.7.9
fsspec==2023.10.0
huggingface-hub==0.19.4
HyperPyYAML==1.2.2
identify==2.5.32
idna==3.6
iniconfig==2.0.0
Jinja2==3.1.2
joblib==1.3.2
lit==17.0.6
MarkupSafe==2.1.3
mccabe==0.6.1
mpmath==1.3.0
networkx==3.2.1
nodeenv==1.8.0
numpy==1.26.2
nvidia-cublas-cu11==11.10.3.66
nvidia-cublas-cu12==12.1.3.1
nvidia-cuda-cupti-cu11==11.7.101
nvidia-cuda-cupti-cu12==12.1.105
nvidia-cuda-nvrtc-cu11==11.7.99
nvidia-cuda-nvrtc-cu12==12.1.105
nvidia-cuda-runtime-cu11==11.7.99
nvidia-cuda-runtime-cu12==12.1.105
nvidia-cudnn-cu11==8.5.0.96
nvidia-cudnn-cu12==8.9.2.26
nvidia-cufft-cu11==10.9.0.58
nvidia-cufft-cu12==11.0.2.54
nvidia-curand-cu11==10.2.10.91
nvidia-curand-cu12==10.3.2.106
nvidia-cusolver-cu11==11.4.0.1
nvidia-cusolver-cu12==11.4.5.107
nvidia-cusparse-cu11==11.7.4.91
nvidia-cusparse-cu12==12.1.0.106
nvidia-nccl-cu11==2.14.3
nvidia-nccl-cu12==2.18.1
nvidia-nvjitlink-cu12==12.3.101
nvidia-nvtx-cu11==11.7.91
nvidia-nvtx-cu12==12.1.105
packaging==23.2
pandas==2.1.3
pathspec==0.11.2
platformdirs==4.0.0
pluggy==1.3.0
pre-commit==3.5.0
pycodestyle==2.5.0
pyflakes==2.1.1
pytest==7.4.0
python-dateutil==2.8.2
pytz==2023.3.post1
PyYAML==6.0.1
regex==2023.10.3
requests==2.31.0
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
safetensors==0.4.1
scipy==1.11.4
sentencepiece==0.1.99
six==1.16.0
-e git+https://github.com/whettenr/speechbrain.git@b545d914c7e1677cae6d8fe030a00f7594b78475#egg=speechbrain
sympy==1.12
tokenizers==0.15.0
toml==0.10.2
torch==2.0.1
torchaudio==2.0.2
tqdm==4.66.1
transformers==4.35.2
triton==2.0.0
typed-ast==1.5.5
typing_extensions==4.8.0
tzdata==2023.3
urllib3==2.1.0
virtualenv==20.24.7
yamllint==1.23.0


2023-12-01 23:13:55,443 - speechbrain.utils.superpowers - DEBUG - b938680


2023-12-01 23:13:55,876 - speechbrain.dataio.encoder - DEBUG - Would load categorical encoding from results/LibriSpeech/brq_hyperconf/2000/save/label_encoder.txt, but file doesn't exist yet.
2023-12-01 23:13:56,366 - speechbrain.dataio.encoder - INFO - Moving label 'T' from index 0, because '<blank>' was put at its place.
2023-12-01 23:13:56,368 - speechbrain.dataio.encoder - INFO - Load called, but CTCTextEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.
2023-12-01 23:13:56,369 - speechbrain.dataio.encoder - DEBUG - Loaded categorical encoding from results/LibriSpeech/brq_hyperconf/2000/save/label_encoder.txt
2023-12-01 23:13:56,370 - speechbrain.core - INFO - Info: auto_mix_prec arg from hparam file is used
2023-12-01 23:13:56,370 - speechbrain.core - INFO - Info: ckpt_interval_minutes arg from hparam file is used
2023-12-01 23:13:58,553 - speechbrain.core - INFO - 42.1M trainable parameters in ASR
2023-12-01 23:13:58,557 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results/LibriSpeech/brq_hyperconf/2000/save.
2023-12-01 23:13:58,590 - speechbrain.pretrained.fetching - INFO - Destination model.ckpt: local file in /gpfswork/rech/nkp/uaj64gk/bestrqexp/jz/brqhc/CKPT+2023-12-01+15-51-27+00/model.ckpt.
2023-12-01 23:13:58,593 - speechbrain.utils.parameter_transfer - INFO - Set local path in self.paths[weighted_ssl_model] = results/LibriSpeech/brq_hyperconf/2000/save/weighted_ssl_model.ckpt
2023-12-01 23:13:58,594 - speechbrain.pretrained.fetching - INFO - Destination normalizer.ckpt: local file in /gpfswork/rech/nkp/uaj64gk/bestrqexp/jz/brqhc/CKPT+2023-12-01+15-51-27+00/normalizer.ckpt.
2023-12-01 23:13:58,598 - speechbrain.utils.parameter_transfer - INFO - Set local path in self.paths[normalize] = results/LibriSpeech/brq_hyperconf/2000/save/normalize.ckpt
2023-12-01 23:13:58,602 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: weighted_ssl_model, normalize
2023-12-01 23:13:58,603 - speechbrain.utils.parameter_transfer - INFO - Redirecting (loading from local path): results/LibriSpeech/brq_hyperconf/2000/save/weighted_ssl_model.ckpt -> results/LibriSpeech/brq_hyperconf/2000/save/weighted_ssl_model.ckpt
2023-12-01 23:13:58,603 - speechbrain.utils.parameter_transfer - INFO - Redirecting (loading from local path): results/LibriSpeech/brq_hyperconf/2000/save/normalize.ckpt -> results/LibriSpeech/brq_hyperconf/2000/save/normalize.ckpt
2023-12-01 23:14:00,402 - speechbrain.core - ERROR - Exception:
Traceback (most recent call last):
  File "/gpfswork/rech/nkp/uaj64gk/bestrqexp/benchmark/benchmarks/MP3S/LibriSpeech/LSTM/train_brq.py", line 371, in <module>
    hparams["pretrainer"].load_collected(asr_brain.device)
  File "/gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain/speechbrain/utils/parameter_transfer.py", line 314, in load_collected
    self._call_load_hooks(paramfiles, device)
  File "/gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain/speechbrain/utils/parameter_transfer.py", line 331, in _call_load_hooks
    default_hook(obj, loadpath, device=device)
  File "/gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain/speechbrain/utils/checkpoints.py", line 150, in torch_parameter_transfer
    incompatible_keys = obj.load_state_dict(
                        ^^^^^^^^^^^^^^^^^^^^
  File "/linkhome/rech/genzjn01/uaj64gk/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2041, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for ModuleList:
	size mismatch for 1.transformer.custom_tgt_module.layers.0.emb.Embedding.weight: copying a param with shape torch.Size([5000, 512]) from checkpoint, the shape in current model is torch.Size([30, 512]).
2023-12-01 23:19:30,944 - speechbrain.core - INFO - Beginning experiment!
2023-12-01 23:19:30,944 - speechbrain.core - INFO - Experiment folder: results/LibriSpeech/brq_hyperconf/2000
2023-12-01 23:19:31,567 - speechbrain.utils.superpowers - DEBUG - appdirs==1.4.4
attrs==23.1.0
black==19.10b0
certifi==2023.11.17
cfgv==3.4.0
charset-normalizer==3.3.2
click==8.0.4
cmake==3.27.7
distlib==0.3.7
entrypoints==0.3
filelock==3.13.1
flake8==3.7.9
fsspec==2023.10.0
huggingface-hub==0.19.4
HyperPyYAML==1.2.2
identify==2.5.32
idna==3.6
iniconfig==2.0.0
Jinja2==3.1.2
joblib==1.3.2
lit==17.0.6
MarkupSafe==2.1.3
mccabe==0.6.1
mpmath==1.3.0
networkx==3.2.1
nodeenv==1.8.0
numpy==1.26.2
nvidia-cublas-cu11==11.10.3.66
nvidia-cublas-cu12==12.1.3.1
nvidia-cuda-cupti-cu11==11.7.101
nvidia-cuda-cupti-cu12==12.1.105
nvidia-cuda-nvrtc-cu11==11.7.99
nvidia-cuda-nvrtc-cu12==12.1.105
nvidia-cuda-runtime-cu11==11.7.99
nvidia-cuda-runtime-cu12==12.1.105
nvidia-cudnn-cu11==8.5.0.96
nvidia-cudnn-cu12==8.9.2.26
nvidia-cufft-cu11==10.9.0.58
nvidia-cufft-cu12==11.0.2.54
nvidia-curand-cu11==10.2.10.91
nvidia-curand-cu12==10.3.2.106
nvidia-cusolver-cu11==11.4.0.1
nvidia-cusolver-cu12==11.4.5.107
nvidia-cusparse-cu11==11.7.4.91
nvidia-cusparse-cu12==12.1.0.106
nvidia-nccl-cu11==2.14.3
nvidia-nccl-cu12==2.18.1
nvidia-nvjitlink-cu12==12.3.101
nvidia-nvtx-cu11==11.7.91
nvidia-nvtx-cu12==12.1.105
packaging==23.2
pandas==2.1.3
pathspec==0.11.2
platformdirs==4.0.0
pluggy==1.3.0
pre-commit==3.5.0
pycodestyle==2.5.0
pyflakes==2.1.1
pytest==7.4.0
python-dateutil==2.8.2
pytz==2023.3.post1
PyYAML==6.0.1
regex==2023.10.3
requests==2.31.0
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
safetensors==0.4.1
scipy==1.11.4
sentencepiece==0.1.99
six==1.16.0
-e git+https://github.com/whettenr/speechbrain.git@b545d914c7e1677cae6d8fe030a00f7594b78475#egg=speechbrain
sympy==1.12
tokenizers==0.15.0
toml==0.10.2
torch==2.0.1
torchaudio==2.0.2
tqdm==4.66.1
transformers==4.35.2
triton==2.0.0
typed-ast==1.5.5
typing_extensions==4.8.0
tzdata==2023.3
urllib3==2.1.0
virtualenv==20.24.7
yamllint==1.23.0


2023-12-01 23:19:31,581 - speechbrain.utils.superpowers - DEBUG - b938680


2023-12-01 23:19:31,989 - speechbrain.dataio.encoder - DEBUG - Loaded categorical encoding from results/LibriSpeech/brq_hyperconf/2000/save/label_encoder.txt
2023-12-01 23:19:31,989 - speechbrain.dataio.encoder - INFO - Load called, but CTCTextEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.
2023-12-01 23:19:31,990 - speechbrain.dataio.encoder - DEBUG - Loaded categorical encoding from results/LibriSpeech/brq_hyperconf/2000/save/label_encoder.txt
2023-12-01 23:19:31,990 - speechbrain.core - INFO - Info: auto_mix_prec arg from hparam file is used
2023-12-01 23:19:31,990 - speechbrain.core - INFO - Info: ckpt_interval_minutes arg from hparam file is used
2023-12-01 23:19:34,055 - speechbrain.core - INFO - 42.1M trainable parameters in ASR
2023-12-01 23:19:34,056 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results/LibriSpeech/brq_hyperconf/2000/save.
2023-12-01 23:19:34,056 - speechbrain.pretrained.fetching - INFO - Fetch model.ckpt: Using existing file/symlink in results/LibriSpeech/brq_hyperconf/2000/save/weighted_ssl_model.ckpt.
2023-12-01 23:19:34,057 - speechbrain.utils.parameter_transfer - INFO - Set local path in self.paths[weighted_ssl_model] = results/LibriSpeech/brq_hyperconf/2000/save/weighted_ssl_model.ckpt
2023-12-01 23:19:34,057 - speechbrain.pretrained.fetching - INFO - Fetch normalizer.ckpt: Using existing file/symlink in results/LibriSpeech/brq_hyperconf/2000/save/normalize.ckpt.
2023-12-01 23:19:34,057 - speechbrain.utils.parameter_transfer - INFO - Set local path in self.paths[normalize] = results/LibriSpeech/brq_hyperconf/2000/save/normalize.ckpt
2023-12-01 23:19:34,057 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: weighted_ssl_model, normalize
2023-12-01 23:19:34,058 - speechbrain.utils.parameter_transfer - INFO - Redirecting (loading from local path): results/LibriSpeech/brq_hyperconf/2000/save/weighted_ssl_model.ckpt -> results/LibriSpeech/brq_hyperconf/2000/save/weighted_ssl_model.ckpt
2023-12-01 23:19:34,058 - speechbrain.utils.parameter_transfer - INFO - Redirecting (loading from local path): results/LibriSpeech/brq_hyperconf/2000/save/normalize.ckpt -> results/LibriSpeech/brq_hyperconf/2000/save/normalize.ckpt
2023-12-01 23:19:36,125 - speechbrain.core - ERROR - Exception:
Traceback (most recent call last):
  File "/gpfswork/rech/nkp/uaj64gk/bestrqexp/benchmark/benchmarks/MP3S/LibriSpeech/LSTM/train_brq.py", line 371, in <module>
    hparams["pretrainer"].load_collected(asr_brain.device)
  File "/gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain/speechbrain/utils/parameter_transfer.py", line 314, in load_collected
    self._call_load_hooks(paramfiles, device)
  File "/gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain/speechbrain/utils/parameter_transfer.py", line 331, in _call_load_hooks
    default_hook(obj, loadpath, device=device)
  File "/gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain/speechbrain/utils/checkpoints.py", line 150, in torch_parameter_transfer
    incompatible_keys = obj.load_state_dict(
                        ^^^^^^^^^^^^^^^^^^^^
  File "/linkhome/rech/genzjn01/uaj64gk/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2041, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for ModuleList:
	size mismatch for 1.transformer.custom_tgt_module.layers.0.emb.Embedding.weight: copying a param with shape torch.Size([5000, 512]) from checkpoint, the shape in current model is torch.Size([30, 512]).
2023-12-01 23:29:09,644 - speechbrain.core - INFO - Beginning experiment!
2023-12-01 23:29:09,645 - speechbrain.core - INFO - Experiment folder: results/LibriSpeech/brq_hyperconf/2000
2023-12-01 23:29:10,291 - speechbrain.utils.superpowers - DEBUG - appdirs==1.4.4
attrs==23.1.0
black==19.10b0
certifi==2023.11.17
cfgv==3.4.0
charset-normalizer==3.3.2
click==8.0.4
cmake==3.27.7
distlib==0.3.7
entrypoints==0.3
filelock==3.13.1
flake8==3.7.9
fsspec==2023.10.0
huggingface-hub==0.19.4
HyperPyYAML==1.2.2
identify==2.5.32
idna==3.6
iniconfig==2.0.0
Jinja2==3.1.2
joblib==1.3.2
lit==17.0.6
MarkupSafe==2.1.3
mccabe==0.6.1
mpmath==1.3.0
networkx==3.2.1
nodeenv==1.8.0
numpy==1.26.2
nvidia-cublas-cu11==11.10.3.66
nvidia-cublas-cu12==12.1.3.1
nvidia-cuda-cupti-cu11==11.7.101
nvidia-cuda-cupti-cu12==12.1.105
nvidia-cuda-nvrtc-cu11==11.7.99
nvidia-cuda-nvrtc-cu12==12.1.105
nvidia-cuda-runtime-cu11==11.7.99
nvidia-cuda-runtime-cu12==12.1.105
nvidia-cudnn-cu11==8.5.0.96
nvidia-cudnn-cu12==8.9.2.26
nvidia-cufft-cu11==10.9.0.58
nvidia-cufft-cu12==11.0.2.54
nvidia-curand-cu11==10.2.10.91
nvidia-curand-cu12==10.3.2.106
nvidia-cusolver-cu11==11.4.0.1
nvidia-cusolver-cu12==11.4.5.107
nvidia-cusparse-cu11==11.7.4.91
nvidia-cusparse-cu12==12.1.0.106
nvidia-nccl-cu11==2.14.3
nvidia-nccl-cu12==2.18.1
nvidia-nvjitlink-cu12==12.3.101
nvidia-nvtx-cu11==11.7.91
nvidia-nvtx-cu12==12.1.105
packaging==23.2
pandas==2.1.3
pathspec==0.11.2
platformdirs==4.0.0
pluggy==1.3.0
pre-commit==3.5.0
pycodestyle==2.5.0
pyflakes==2.1.1
pytest==7.4.0
python-dateutil==2.8.2
pytz==2023.3.post1
PyYAML==6.0.1
regex==2023.10.3
requests==2.31.0
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
safetensors==0.4.1
scipy==1.11.4
sentencepiece==0.1.99
six==1.16.0
-e git+https://github.com/whettenr/speechbrain.git@b545d914c7e1677cae6d8fe030a00f7594b78475#egg=speechbrain
sympy==1.12
tokenizers==0.15.0
toml==0.10.2
torch==2.0.1
torchaudio==2.0.2
tqdm==4.66.1
transformers==4.35.2
triton==2.0.0
typed-ast==1.5.5
typing_extensions==4.8.0
tzdata==2023.3
urllib3==2.1.0
virtualenv==20.24.7
yamllint==1.23.0


2023-12-01 23:29:10,305 - speechbrain.utils.superpowers - DEBUG - b938680


2023-12-01 23:29:10,693 - speechbrain.dataio.encoder - DEBUG - Loaded categorical encoding from results/LibriSpeech/brq_hyperconf/2000/save/label_encoder.txt
2023-12-01 23:29:10,693 - speechbrain.dataio.encoder - INFO - Load called, but CTCTextEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.
2023-12-01 23:29:10,694 - speechbrain.dataio.encoder - DEBUG - Loaded categorical encoding from results/LibriSpeech/brq_hyperconf/2000/save/label_encoder.txt
2023-12-01 23:29:10,694 - speechbrain.core - INFO - Info: auto_mix_prec arg from hparam file is used
2023-12-01 23:29:10,694 - speechbrain.core - INFO - Info: ckpt_interval_minutes arg from hparam file is used
2023-12-01 23:29:12,866 - speechbrain.core - INFO - 42.1M trainable parameters in ASR
2023-12-01 23:29:12,866 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results/LibriSpeech/brq_hyperconf/2000/save.
2023-12-01 23:29:12,866 - speechbrain.pretrained.fetching - INFO - Fetch model.ckpt: Using existing file/symlink in results/LibriSpeech/brq_hyperconf/2000/save/weighted_ssl_model.ckpt.
2023-12-01 23:29:12,867 - speechbrain.utils.parameter_transfer - INFO - Set local path in self.paths[weighted_ssl_model] = results/LibriSpeech/brq_hyperconf/2000/save/weighted_ssl_model.ckpt
2023-12-01 23:29:12,867 - speechbrain.pretrained.fetching - INFO - Fetch normalizer.ckpt: Using existing file/symlink in results/LibriSpeech/brq_hyperconf/2000/save/normalize.ckpt.
2023-12-01 23:29:12,868 - speechbrain.utils.parameter_transfer - INFO - Set local path in self.paths[normalize] = results/LibriSpeech/brq_hyperconf/2000/save/normalize.ckpt
2023-12-01 23:29:12,868 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: weighted_ssl_model, normalize
2023-12-01 23:29:12,868 - speechbrain.utils.parameter_transfer - INFO - Redirecting (loading from local path): results/LibriSpeech/brq_hyperconf/2000/save/weighted_ssl_model.ckpt -> results/LibriSpeech/brq_hyperconf/2000/save/weighted_ssl_model.ckpt
2023-12-01 23:29:12,868 - speechbrain.utils.parameter_transfer - INFO - Redirecting (loading from local path): results/LibriSpeech/brq_hyperconf/2000/save/normalize.ckpt -> results/LibriSpeech/brq_hyperconf/2000/save/normalize.ckpt
2023-12-01 23:29:14,842 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to ModuleList(
  (0): ConvolutionFrontEnd(
    (convblock_0): ConvBlock(
      (convs): Sequential(
        (conv_0): Conv2d(
          (conv): Conv2d(1, 128, kernel_size=(3, 3), stride=(2, 2))
        )
        (norm_0): LayerNorm(
          (norm): LayerNorm((40, 128), eps=1e-05, elementwise_affine=True)
        )
        (act_0): LeakyReLU(negative_slope=0.01)
        (dropout_0): Dropout(p=0.1, inplace=False)
      )
    )
    (convblock_1): ConvBlock(
      (convs): Sequential(
        (conv_0): Conv2d(
          (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(2, 2))
        )
        (norm_0): LayerNorm(
          (norm): LayerNorm((20, 32), eps=1e-05, elementwise_affine=True)
        )
        (act_0): LeakyReLU(negative_slope=0.01)
        (dropout_0): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (1): WeightedEncoderWrapper(
    (transformer): TransformerASR(
      (positional_encoding): PositionalEncoding()
      (encoder): ConformerEncoder(
        (layers): ModuleList(
          (0-11): 12 x ConformerEncoderLayer(
            (mha_layer): HyperMixing(
              (hyper): HyperNetwork(
                (w1_gen): ParallelMLPs(
                  (activation): GELU(approximate='none')
                )
                (w2_gen): ParallelMLPs(
                  (activation): GELU(approximate='none')
                )
              )
              (activation): GELU(approximate='none')
              (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (positional_encoding): PositionalEncoding()
            )
            (convolution_module): ConvolutionModule(
              (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (bottleneck): Sequential(
                (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
                (1): GLU(dim=1)
              )
              (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
              (after_conv): Sequential(
                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (1): Swish(
                  (sigmoid): Sigmoid()
                )
                (2): Linear(in_features=512, out_features=512, bias=True)
                (3): Dropout(p=0.1, inplace=False)
              )
            )
            (ffn_module1): Sequential(
              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (1): PositionalwiseFeedForward(
                (ffn): Sequential(
                  (0): Linear(in_features=512, out_features=2048, bias=True)
                  (1): Swish(
                    (sigmoid): Sigmoid()
                  )
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (2): Dropout(p=0.1, inplace=False)
            )
            (ffn_module2): Sequential(
              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (1): PositionalwiseFeedForward(
                (ffn): Sequential(
                  (0): Linear(in_features=512, out_features=2048, bias=True)
                  (1): Swish(
                    (sigmoid): Sigmoid()
                  )
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (2): Dropout(p=0.1, inplace=False)
            )
            (norm1): LayerNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
            (norm2): LayerNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
            (drop): Dropout(p=0.1, inplace=False)
          )
        )
        (norm): LayerNorm(
          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        )
      )
      (custom_src_module): ModuleList(
        (layers): ModuleList(
          (0): Linear(
            (w): Linear(in_features=640, out_features=512, bias=True)
          )
          (1): Dropout(p=0.1, inplace=False)
        )
      )
      (custom_tgt_module): ModuleList(
        (layers): ModuleList(
          (0): NormalizedEmbedding(
            (emb): Embedding(
              (Embedding): Embedding(5000, 512)
            )
          )
        )
      )
    )
  )
  (2): RandomProjectionQuantizer()
  (3): Linear(
    (w): Linear(in_features=512, out_features=8192, bias=True)
  )
) loading from results/LibriSpeech/brq_hyperconf/2000/save/weighted_ssl_model.ckpt, the transferred parameters did not have parameters for the key: 1.weights
2023-12-01 23:29:14,853 - speechbrain.utils.checkpoints - INFO - Would load a checkpoint here, but none found yet.
2023-12-01 23:29:14,855 - speechbrain.utils.epoch_loop - INFO - Going into epoch 1
2023-12-01 23:29:41,998 - speechbrain.core - ERROR - Exception:
Traceback (most recent call last):
  File "/gpfswork/rech/nkp/uaj64gk/bestrqexp/benchmark/benchmarks/MP3S/LibriSpeech/LSTM/train_brq.py", line 376, in <module>
    asr_brain.fit(
  File "/gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain/speechbrain/core.py", line 1366, in fit
    self._fit_train(train_set=train_set, epoch=epoch, enable=enable)
  File "/gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain/speechbrain/core.py", line 1193, in _fit_train
    loss = self.fit_batch(batch)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfswork/rech/nkp/uaj64gk/bestrqexp/benchmark/benchmarks/MP3S/LibriSpeech/LSTM/train_brq.py", line 91, in fit_batch
    loss = self.compute_objectives(predictions, batch, sb.Stage.TRAIN)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfswork/rech/nkp/uaj64gk/bestrqexp/benchmark/benchmarks/MP3S/LibriSpeech/LSTM/train_brq.py", line 57, in compute_objectives
    loss_ctc = self.hparams.ctc_cost(p_ctc, tokens, wav_lens, tokens_lens)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain/speechbrain/nnet/losses.py", line 280, in ctc_loss
    loss = torch.nn.functional.ctc_loss(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/linkhome/rech/genzjn01/uaj64gk/.local/lib/python3.11/site-packages/torch/nn/functional.py", line 2631, in ctc_loss
    return torch.ctc_loss(
           ^^^^^^^^^^^^^^^
KeyboardInterrupt
2023-12-02 09:03:21,142 - speechbrain.core - INFO - Beginning experiment!
2023-12-02 09:03:21,154 - speechbrain.core - INFO - Experiment folder: results/LibriSpeech/brq_hyperconf/2000
2023-12-02 09:03:22,583 - speechbrain.utils.superpowers - DEBUG - appdirs==1.4.4
attrs==23.1.0
black==19.10b0
certifi==2023.11.17
cfgv==3.4.0
charset-normalizer==3.3.2
click==8.0.4
cmake==3.27.7
distlib==0.3.7
entrypoints==0.3
filelock==3.13.1
flake8==3.7.9
fsspec==2023.10.0
huggingface-hub==0.19.4
HyperPyYAML==1.2.2
identify==2.5.32
idna==3.6
iniconfig==2.0.0
Jinja2==3.1.2
joblib==1.3.2
lit==17.0.6
MarkupSafe==2.1.3
mccabe==0.6.1
mpmath==1.3.0
networkx==3.2.1
nodeenv==1.8.0
numpy==1.26.2
nvidia-cublas-cu11==11.10.3.66
nvidia-cublas-cu12==12.1.3.1
nvidia-cuda-cupti-cu11==11.7.101
nvidia-cuda-cupti-cu12==12.1.105
nvidia-cuda-nvrtc-cu11==11.7.99
nvidia-cuda-nvrtc-cu12==12.1.105
nvidia-cuda-runtime-cu11==11.7.99
nvidia-cuda-runtime-cu12==12.1.105
nvidia-cudnn-cu11==8.5.0.96
nvidia-cudnn-cu12==8.9.2.26
nvidia-cufft-cu11==10.9.0.58
nvidia-cufft-cu12==11.0.2.54
nvidia-curand-cu11==10.2.10.91
nvidia-curand-cu12==10.3.2.106
nvidia-cusolver-cu11==11.4.0.1
nvidia-cusolver-cu12==11.4.5.107
nvidia-cusparse-cu11==11.7.4.91
nvidia-cusparse-cu12==12.1.0.106
nvidia-nccl-cu11==2.14.3
nvidia-nccl-cu12==2.18.1
nvidia-nvjitlink-cu12==12.3.101
nvidia-nvtx-cu11==11.7.91
nvidia-nvtx-cu12==12.1.105
packaging==23.2
pandas==2.1.3
pathspec==0.11.2
platformdirs==4.0.0
pluggy==1.3.0
pre-commit==3.5.0
pycodestyle==2.5.0
pyflakes==2.1.1
pytest==7.4.0
python-dateutil==2.8.2
pytz==2023.3.post1
PyYAML==6.0.1
regex==2023.10.3
requests==2.31.0
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
safetensors==0.4.1
scipy==1.11.4
sentencepiece==0.1.99
six==1.16.0
-e git+https://github.com/whettenr/speechbrain.git@b545d914c7e1677cae6d8fe030a00f7594b78475#egg=speechbrain
sympy==1.12
tokenizers==0.15.0
toml==0.10.2
torch==2.0.1
torchaudio==2.0.2
tqdm==4.66.1
transformers==4.35.2
triton==2.0.0
typed-ast==1.5.5
typing_extensions==4.8.0
tzdata==2023.3
urllib3==2.1.0
virtualenv==20.24.7
yamllint==1.23.0


2023-12-02 09:03:22,657 - speechbrain.utils.superpowers - DEBUG - 8b94a5a


2023-12-02 09:03:23,208 - speechbrain.dataio.encoder - DEBUG - Loaded categorical encoding from results/LibriSpeech/brq_hyperconf/2000/save/label_encoder.txt
2023-12-02 09:03:23,208 - speechbrain.dataio.encoder - INFO - Load called, but CTCTextEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.
2023-12-02 09:03:23,209 - speechbrain.dataio.encoder - DEBUG - Loaded categorical encoding from results/LibriSpeech/brq_hyperconf/2000/save/label_encoder.txt
2023-12-02 09:03:23,209 - speechbrain.core - INFO - Info: auto_mix_prec arg from hparam file is used
2023-12-02 09:03:23,209 - speechbrain.core - INFO - Info: ckpt_interval_minutes arg from hparam file is used
2023-12-02 09:03:25,651 - speechbrain.core - INFO - 42.1M trainable parameters in ASR
2023-12-02 09:03:25,651 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results/LibriSpeech/brq_hyperconf/2000/save.
2023-12-02 09:03:25,652 - speechbrain.pretrained.fetching - INFO - Fetch model.ckpt: Using existing file/symlink in results/LibriSpeech/brq_hyperconf/2000/save/weighted_ssl_model.ckpt.
2023-12-02 09:03:25,652 - speechbrain.utils.parameter_transfer - INFO - Set local path in self.paths[weighted_ssl_model] = results/LibriSpeech/brq_hyperconf/2000/save/weighted_ssl_model.ckpt
2023-12-02 09:03:25,652 - speechbrain.pretrained.fetching - INFO - Fetch normalizer.ckpt: Using existing file/symlink in results/LibriSpeech/brq_hyperconf/2000/save/normalize.ckpt.
2023-12-02 09:03:25,653 - speechbrain.utils.parameter_transfer - INFO - Set local path in self.paths[normalize] = results/LibriSpeech/brq_hyperconf/2000/save/normalize.ckpt
2023-12-02 09:03:25,653 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: weighted_ssl_model, normalize
2023-12-02 09:03:25,653 - speechbrain.utils.parameter_transfer - INFO - Redirecting (loading from local path): results/LibriSpeech/brq_hyperconf/2000/save/weighted_ssl_model.ckpt -> results/LibriSpeech/brq_hyperconf/2000/save/weighted_ssl_model.ckpt
2023-12-02 09:03:25,653 - speechbrain.utils.parameter_transfer - INFO - Redirecting (loading from local path): results/LibriSpeech/brq_hyperconf/2000/save/normalize.ckpt -> results/LibriSpeech/brq_hyperconf/2000/save/normalize.ckpt
2023-12-02 09:03:27,457 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to ModuleList(
  (0): ConvolutionFrontEnd(
    (convblock_0): ConvBlock(
      (convs): Sequential(
        (conv_0): Conv2d(
          (conv): Conv2d(1, 128, kernel_size=(3, 3), stride=(2, 2))
        )
        (norm_0): LayerNorm(
          (norm): LayerNorm((40, 128), eps=1e-05, elementwise_affine=True)
        )
        (act_0): LeakyReLU(negative_slope=0.01)
        (dropout_0): Dropout(p=0.1, inplace=False)
      )
    )
    (convblock_1): ConvBlock(
      (convs): Sequential(
        (conv_0): Conv2d(
          (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(2, 2))
        )
        (norm_0): LayerNorm(
          (norm): LayerNorm((20, 32), eps=1e-05, elementwise_affine=True)
        )
        (act_0): LeakyReLU(negative_slope=0.01)
        (dropout_0): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (1): WeightedEncoderWrapper(
    (transformer): TransformerASR(
      (positional_encoding): PositionalEncoding()
      (encoder): ConformerEncoder(
        (layers): ModuleList(
          (0-11): 12 x ConformerEncoderLayer(
            (mha_layer): HyperMixing(
              (hyper): HyperNetwork(
                (w1_gen): ParallelMLPs(
                  (activation): GELU(approximate='none')
                )
                (w2_gen): ParallelMLPs(
                  (activation): GELU(approximate='none')
                )
              )
              (activation): GELU(approximate='none')
              (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (positional_encoding): PositionalEncoding()
            )
            (convolution_module): ConvolutionModule(
              (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (bottleneck): Sequential(
                (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
                (1): GLU(dim=1)
              )
              (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
              (after_conv): Sequential(
                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (1): Swish(
                  (sigmoid): Sigmoid()
                )
                (2): Linear(in_features=512, out_features=512, bias=True)
                (3): Dropout(p=0.1, inplace=False)
              )
            )
            (ffn_module1): Sequential(
              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (1): PositionalwiseFeedForward(
                (ffn): Sequential(
                  (0): Linear(in_features=512, out_features=2048, bias=True)
                  (1): Swish(
                    (sigmoid): Sigmoid()
                  )
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (2): Dropout(p=0.1, inplace=False)
            )
            (ffn_module2): Sequential(
              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (1): PositionalwiseFeedForward(
                (ffn): Sequential(
                  (0): Linear(in_features=512, out_features=2048, bias=True)
                  (1): Swish(
                    (sigmoid): Sigmoid()
                  )
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (2): Dropout(p=0.1, inplace=False)
            )
            (norm1): LayerNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
            (norm2): LayerNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
            (drop): Dropout(p=0.1, inplace=False)
          )
        )
        (norm): LayerNorm(
          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        )
      )
      (custom_src_module): ModuleList(
        (layers): ModuleList(
          (0): Linear(
            (w): Linear(in_features=640, out_features=512, bias=True)
          )
          (1): Dropout(p=0.1, inplace=False)
        )
      )
      (custom_tgt_module): ModuleList(
        (layers): ModuleList(
          (0): NormalizedEmbedding(
            (emb): Embedding(
              (Embedding): Embedding(5000, 512)
            )
          )
        )
      )
    )
  )
  (2): RandomProjectionQuantizer()
  (3): Linear(
    (w): Linear(in_features=512, out_features=8192, bias=True)
  )
) loading from results/LibriSpeech/brq_hyperconf/2000/save/weighted_ssl_model.ckpt, the transferred parameters did not have parameters for the key: 1.weights
2023-12-02 09:03:27,460 - speechbrain.utils.checkpoints - INFO - Would load a checkpoint here, but none found yet.
2023-12-02 09:03:27,460 - speechbrain.utils.epoch_loop - INFO - Going into epoch 1
2023-12-02 09:04:41,641 - speechbrain.core - ERROR - Exception:
Traceback (most recent call last):
  File "/gpfswork/rech/nkp/uaj64gk/bestrqexp/benchmark/benchmarks/MP3S/LibriSpeech/LSTM/train_brq.py", line 376, in <module>
    asr_brain.fit(
  File "/gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain/speechbrain/core.py", line 1366, in fit
    self._fit_train(train_set=train_set, epoch=epoch, enable=enable)
  File "/gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain/speechbrain/core.py", line 1193, in _fit_train
    loss = self.fit_batch(batch)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfswork/rech/nkp/uaj64gk/bestrqexp/benchmark/benchmarks/MP3S/LibriSpeech/LSTM/train_brq.py", line 90, in fit_batch
    predictions = self.compute_forward(batch, sb.Stage.TRAIN)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfswork/rech/nkp/uaj64gk/bestrqexp/benchmark/benchmarks/MP3S/LibriSpeech/LSTM/train_brq.py", line 36, in compute_forward
    feats = self.modules.weighted_ssl_model[1](feats, wav_lens)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/linkhome/rech/genzjn01/uaj64gk/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain/speechbrain/lobes/models/transformer/TransformerASR.py", line 438, in forward
    _, hidden_states = self.transformer.encode(x, wav_lens, pad_idx)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain/speechbrain/lobes/models/transformer/TransformerASR.py", line 337, in encode
    encoder_out, _, hidden_state_lst = self.encoder(
                                       ^^^^^^^^^^^^^
  File "/linkhome/rech/genzjn01/uaj64gk/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain/speechbrain/lobes/models/transformer/Conformer.py", line 415, in forward
    output, attention = enc_layer(
                        ^^^^^^^^^^
  File "/linkhome/rech/genzjn01/uaj64gk/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain/speechbrain/lobes/models/transformer/Conformer.py", line 269, in forward
    x = self.norm2(x + 0.5 * self.ffn_module2(x))
                             ^^^^^^^^^^^^^^^^^^^
  File "/linkhome/rech/genzjn01/uaj64gk/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/linkhome/rech/genzjn01/uaj64gk/.local/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/linkhome/rech/genzjn01/uaj64gk/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1495, in _call_impl
    forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)
                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
2023-12-02 09:06:54,714 - speechbrain.core - INFO - Beginning experiment!
2023-12-02 09:06:54,715 - speechbrain.core - INFO - Experiment folder: results/LibriSpeech/brq_hyperconf/2000
2023-12-02 09:06:56,112 - speechbrain.utils.superpowers - DEBUG - appdirs==1.4.4
attrs==23.1.0
black==19.10b0
certifi==2023.11.17
cfgv==3.4.0
charset-normalizer==3.3.2
click==8.0.4
cmake==3.27.7
distlib==0.3.7
entrypoints==0.3
filelock==3.13.1
flake8==3.7.9
fsspec==2023.10.0
huggingface-hub==0.19.4
HyperPyYAML==1.2.2
identify==2.5.32
idna==3.6
iniconfig==2.0.0
Jinja2==3.1.2
joblib==1.3.2
lit==17.0.6
MarkupSafe==2.1.3
mccabe==0.6.1
mpmath==1.3.0
networkx==3.2.1
nodeenv==1.8.0
numpy==1.26.2
nvidia-cublas-cu11==11.10.3.66
nvidia-cublas-cu12==12.1.3.1
nvidia-cuda-cupti-cu11==11.7.101
nvidia-cuda-cupti-cu12==12.1.105
nvidia-cuda-nvrtc-cu11==11.7.99
nvidia-cuda-nvrtc-cu12==12.1.105
nvidia-cuda-runtime-cu11==11.7.99
nvidia-cuda-runtime-cu12==12.1.105
nvidia-cudnn-cu11==8.5.0.96
nvidia-cudnn-cu12==8.9.2.26
nvidia-cufft-cu11==10.9.0.58
nvidia-cufft-cu12==11.0.2.54
nvidia-curand-cu11==10.2.10.91
nvidia-curand-cu12==10.3.2.106
nvidia-cusolver-cu11==11.4.0.1
nvidia-cusolver-cu12==11.4.5.107
nvidia-cusparse-cu11==11.7.4.91
nvidia-cusparse-cu12==12.1.0.106
nvidia-nccl-cu11==2.14.3
nvidia-nccl-cu12==2.18.1
nvidia-nvjitlink-cu12==12.3.101
nvidia-nvtx-cu11==11.7.91
nvidia-nvtx-cu12==12.1.105
packaging==23.2
pandas==2.1.3
pathspec==0.11.2
platformdirs==4.0.0
pluggy==1.3.0
pre-commit==3.5.0
pycodestyle==2.5.0
pyflakes==2.1.1
pytest==7.4.0
python-dateutil==2.8.2
pytz==2023.3.post1
PyYAML==6.0.1
regex==2023.10.3
requests==2.31.0
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
safetensors==0.4.1
scipy==1.11.4
sentencepiece==0.1.99
six==1.16.0
-e git+https://github.com/whettenr/speechbrain.git@b545d914c7e1677cae6d8fe030a00f7594b78475#egg=speechbrain
sympy==1.12
tokenizers==0.15.0
toml==0.10.2
torch==2.0.1
torchaudio==2.0.2
tqdm==4.66.1
transformers==4.35.2
triton==2.0.0
typed-ast==1.5.5
typing_extensions==4.8.0
tzdata==2023.3
urllib3==2.1.0
virtualenv==20.24.7
yamllint==1.23.0


2023-12-02 09:06:56,173 - speechbrain.utils.superpowers - DEBUG - 51e9a7c


2023-12-02 09:06:56,712 - speechbrain.dataio.encoder - DEBUG - Loaded categorical encoding from results/LibriSpeech/brq_hyperconf/2000/save/label_encoder.txt
2023-12-02 09:06:56,712 - speechbrain.dataio.encoder - INFO - Load called, but CTCTextEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.
2023-12-02 09:06:56,712 - speechbrain.dataio.encoder - DEBUG - Loaded categorical encoding from results/LibriSpeech/brq_hyperconf/2000/save/label_encoder.txt
2023-12-02 09:06:56,713 - speechbrain.core - INFO - Info: auto_mix_prec arg from hparam file is used
2023-12-02 09:06:56,713 - speechbrain.core - INFO - Info: ckpt_interval_minutes arg from hparam file is used
2023-12-02 09:06:59,129 - speechbrain.core - INFO - 42.1M trainable parameters in ASR
2023-12-02 09:06:59,129 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results/LibriSpeech/brq_hyperconf/2000/save.
2023-12-02 09:06:59,130 - speechbrain.pretrained.fetching - INFO - Fetch model.ckpt: Using existing file/symlink in results/LibriSpeech/brq_hyperconf/2000/save/weighted_ssl_model.ckpt.
2023-12-02 09:06:59,130 - speechbrain.utils.parameter_transfer - INFO - Set local path in self.paths[weighted_ssl_model] = results/LibriSpeech/brq_hyperconf/2000/save/weighted_ssl_model.ckpt
2023-12-02 09:06:59,130 - speechbrain.pretrained.fetching - INFO - Fetch normalizer.ckpt: Using existing file/symlink in results/LibriSpeech/brq_hyperconf/2000/save/normalize.ckpt.
2023-12-02 09:06:59,131 - speechbrain.utils.parameter_transfer - INFO - Set local path in self.paths[normalize] = results/LibriSpeech/brq_hyperconf/2000/save/normalize.ckpt
2023-12-02 09:06:59,131 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: weighted_ssl_model, normalize
2023-12-02 09:06:59,131 - speechbrain.utils.parameter_transfer - INFO - Redirecting (loading from local path): results/LibriSpeech/brq_hyperconf/2000/save/weighted_ssl_model.ckpt -> results/LibriSpeech/brq_hyperconf/2000/save/weighted_ssl_model.ckpt
2023-12-02 09:06:59,131 - speechbrain.utils.parameter_transfer - INFO - Redirecting (loading from local path): results/LibriSpeech/brq_hyperconf/2000/save/normalize.ckpt -> results/LibriSpeech/brq_hyperconf/2000/save/normalize.ckpt
2023-12-02 09:07:00,871 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to ModuleList(
  (0): ConvolutionFrontEnd(
    (convblock_0): ConvBlock(
      (convs): Sequential(
        (conv_0): Conv2d(
          (conv): Conv2d(1, 128, kernel_size=(3, 3), stride=(2, 2))
        )
        (norm_0): LayerNorm(
          (norm): LayerNorm((40, 128), eps=1e-05, elementwise_affine=True)
        )
        (act_0): LeakyReLU(negative_slope=0.01)
        (dropout_0): Dropout(p=0.1, inplace=False)
      )
    )
    (convblock_1): ConvBlock(
      (convs): Sequential(
        (conv_0): Conv2d(
          (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(2, 2))
        )
        (norm_0): LayerNorm(
          (norm): LayerNorm((20, 32), eps=1e-05, elementwise_affine=True)
        )
        (act_0): LeakyReLU(negative_slope=0.01)
        (dropout_0): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (1): WeightedEncoderWrapper(
    (transformer): TransformerASR(
      (positional_encoding): PositionalEncoding()
      (encoder): ConformerEncoder(
        (layers): ModuleList(
          (0-11): 12 x ConformerEncoderLayer(
            (mha_layer): HyperMixing(
              (hyper): HyperNetwork(
                (w1_gen): ParallelMLPs(
                  (activation): GELU(approximate='none')
                )
                (w2_gen): ParallelMLPs(
                  (activation): GELU(approximate='none')
                )
              )
              (activation): GELU(approximate='none')
              (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (positional_encoding): PositionalEncoding()
            )
            (convolution_module): ConvolutionModule(
              (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (bottleneck): Sequential(
                (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
                (1): GLU(dim=1)
              )
              (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
              (after_conv): Sequential(
                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (1): Swish(
                  (sigmoid): Sigmoid()
                )
                (2): Linear(in_features=512, out_features=512, bias=True)
                (3): Dropout(p=0.1, inplace=False)
              )
            )
            (ffn_module1): Sequential(
              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (1): PositionalwiseFeedForward(
                (ffn): Sequential(
                  (0): Linear(in_features=512, out_features=2048, bias=True)
                  (1): Swish(
                    (sigmoid): Sigmoid()
                  )
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (2): Dropout(p=0.1, inplace=False)
            )
            (ffn_module2): Sequential(
              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (1): PositionalwiseFeedForward(
                (ffn): Sequential(
                  (0): Linear(in_features=512, out_features=2048, bias=True)
                  (1): Swish(
                    (sigmoid): Sigmoid()
                  )
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (2): Dropout(p=0.1, inplace=False)
            )
            (norm1): LayerNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
            (norm2): LayerNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
            (drop): Dropout(p=0.1, inplace=False)
          )
        )
        (norm): LayerNorm(
          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        )
      )
      (custom_src_module): ModuleList(
        (layers): ModuleList(
          (0): Linear(
            (w): Linear(in_features=640, out_features=512, bias=True)
          )
          (1): Dropout(p=0.1, inplace=False)
        )
      )
      (custom_tgt_module): ModuleList(
        (layers): ModuleList(
          (0): NormalizedEmbedding(
            (emb): Embedding(
              (Embedding): Embedding(5000, 512)
            )
          )
        )
      )
    )
  )
  (2): RandomProjectionQuantizer()
  (3): Linear(
    (w): Linear(in_features=512, out_features=8192, bias=True)
  )
) loading from results/LibriSpeech/brq_hyperconf/2000/save/weighted_ssl_model.ckpt, the transferred parameters did not have parameters for the key: 1.weights
2023-12-02 09:07:00,874 - speechbrain.utils.checkpoints - INFO - Would load a checkpoint here, but none found yet.
2023-12-02 09:07:00,874 - speechbrain.utils.epoch_loop - INFO - Going into epoch 1
2023-12-02 09:07:03,609 - speechbrain.core - INFO - Beginning experiment!
2023-12-02 09:07:03,611 - speechbrain.core - INFO - Experiment folder: results/LibriSpeech/brq_hyperconf/2000
2023-12-02 09:07:06,003 - speechbrain.utils.superpowers - DEBUG - appdirs==1.4.4
attrs==23.1.0
black==19.10b0
certifi==2023.11.17
cfgv==3.4.0
charset-normalizer==3.3.2
click==8.0.4
cmake==3.27.7
distlib==0.3.7
entrypoints==0.3
filelock==3.13.1
flake8==3.7.9
fsspec==2023.10.0
huggingface-hub==0.19.4
HyperPyYAML==1.2.2
identify==2.5.32
idna==3.6
iniconfig==2.0.0
Jinja2==3.1.2
joblib==1.3.2
lit==17.0.6
MarkupSafe==2.1.3
mccabe==0.6.1
mpmath==1.3.0
networkx==3.2.1
nodeenv==1.8.0
numpy==1.26.2
nvidia-cublas-cu11==11.10.3.66
nvidia-cublas-cu12==12.1.3.1
nvidia-cuda-cupti-cu11==11.7.101
nvidia-cuda-cupti-cu12==12.1.105
nvidia-cuda-nvrtc-cu11==11.7.99
nvidia-cuda-nvrtc-cu12==12.1.105
nvidia-cuda-runtime-cu11==11.7.99
nvidia-cuda-runtime-cu12==12.1.105
nvidia-cudnn-cu11==8.5.0.96
nvidia-cudnn-cu12==8.9.2.26
nvidia-cufft-cu11==10.9.0.58
nvidia-cufft-cu12==11.0.2.54
nvidia-curand-cu11==10.2.10.91
nvidia-curand-cu12==10.3.2.106
nvidia-cusolver-cu11==11.4.0.1
nvidia-cusolver-cu12==11.4.5.107
nvidia-cusparse-cu11==11.7.4.91
nvidia-cusparse-cu12==12.1.0.106
nvidia-nccl-cu11==2.14.3
nvidia-nccl-cu12==2.18.1
nvidia-nvjitlink-cu12==12.3.101
nvidia-nvtx-cu11==11.7.91
nvidia-nvtx-cu12==12.1.105
packaging==23.2
pandas==2.1.3
pathspec==0.11.2
platformdirs==4.0.0
pluggy==1.3.0
pre-commit==3.5.0
pycodestyle==2.5.0
pyflakes==2.1.1
pytest==7.4.0
python-dateutil==2.8.2
pytz==2023.3.post1
PyYAML==6.0.1
regex==2023.10.3
requests==2.31.0
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
safetensors==0.4.1
scipy==1.11.4
sentencepiece==0.1.99
six==1.16.0
-e /gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain
sympy==1.12
tokenizers==0.15.0
toml==0.10.2
torch==2.0.1
torchaudio==2.0.2
tqdm==4.66.1
transformers==4.35.2
triton==2.0.0
typed-ast==1.5.5
typing_extensions==4.8.0
tzdata==2023.3
urllib3==2.1.0
virtualenv==20.24.7
yamllint==1.23.0

ERROR: Error [Errno 2] No such file or directory: 'git' while executing command git config --get-regexp 'remote\..*\.url'
WARNING: cannot determine version of editable source in /gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain (git command not found in path)

2023-12-02 09:07:06,563 - speechbrain.dataio.encoder - DEBUG - Loaded categorical encoding from results/LibriSpeech/brq_hyperconf/2000/save/label_encoder.txt
2023-12-02 09:07:06,563 - speechbrain.dataio.encoder - INFO - Load called, but CTCTextEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.
2023-12-02 09:07:06,563 - speechbrain.dataio.encoder - DEBUG - Loaded categorical encoding from results/LibriSpeech/brq_hyperconf/2000/save/label_encoder.txt
2023-12-02 09:07:06,564 - speechbrain.core - INFO - Info: auto_mix_prec arg from hparam file is used
2023-12-02 09:07:06,564 - speechbrain.core - INFO - Info: ckpt_interval_minutes arg from hparam file is used
2023-12-02 09:07:12,479 - speechbrain.core - INFO - 42.1M trainable parameters in ASR
2023-12-02 09:07:12,479 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results/LibriSpeech/brq_hyperconf/2000/save.
2023-12-02 09:07:12,518 - speechbrain.pretrained.fetching - INFO - Fetch model.ckpt: Using existing file/symlink in results/LibriSpeech/brq_hyperconf/2000/save/weighted_ssl_model.ckpt.
2023-12-02 09:07:12,519 - speechbrain.utils.parameter_transfer - INFO - Set local path in self.paths[weighted_ssl_model] = results/LibriSpeech/brq_hyperconf/2000/save/weighted_ssl_model.ckpt
2023-12-02 09:07:12,522 - speechbrain.pretrained.fetching - INFO - Fetch normalizer.ckpt: Using existing file/symlink in results/LibriSpeech/brq_hyperconf/2000/save/normalize.ckpt.
2023-12-02 09:07:12,523 - speechbrain.utils.parameter_transfer - INFO - Set local path in self.paths[normalize] = results/LibriSpeech/brq_hyperconf/2000/save/normalize.ckpt
2023-12-02 09:07:12,621 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: weighted_ssl_model, normalize
2023-12-02 09:07:12,621 - speechbrain.utils.parameter_transfer - INFO - Redirecting (loading from local path): results/LibriSpeech/brq_hyperconf/2000/save/weighted_ssl_model.ckpt -> results/LibriSpeech/brq_hyperconf/2000/save/weighted_ssl_model.ckpt
2023-12-02 09:07:12,621 - speechbrain.utils.parameter_transfer - INFO - Redirecting (loading from local path): results/LibriSpeech/brq_hyperconf/2000/save/normalize.ckpt -> results/LibriSpeech/brq_hyperconf/2000/save/normalize.ckpt
2023-12-02 09:07:13,213 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to ModuleList(
  (0): ConvolutionFrontEnd(
    (convblock_0): ConvBlock(
      (convs): Sequential(
        (conv_0): Conv2d(
          (conv): Conv2d(1, 128, kernel_size=(3, 3), stride=(2, 2))
        )
        (norm_0): LayerNorm(
          (norm): LayerNorm((40, 128), eps=1e-05, elementwise_affine=True)
        )
        (act_0): LeakyReLU(negative_slope=0.01)
        (dropout_0): Dropout(p=0.1, inplace=False)
      )
    )
    (convblock_1): ConvBlock(
      (convs): Sequential(
        (conv_0): Conv2d(
          (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(2, 2))
        )
        (norm_0): LayerNorm(
          (norm): LayerNorm((20, 32), eps=1e-05, elementwise_affine=True)
        )
        (act_0): LeakyReLU(negative_slope=0.01)
        (dropout_0): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (1): WeightedEncoderWrapper(
    (transformer): TransformerASR(
      (positional_encoding): PositionalEncoding()
      (encoder): ConformerEncoder(
        (layers): ModuleList(
          (0-11): 12 x ConformerEncoderLayer(
            (mha_layer): HyperMixing(
              (hyper): HyperNetwork(
                (w1_gen): ParallelMLPs(
                  (activation): GELU(approximate='none')
                )
                (w2_gen): ParallelMLPs(
                  (activation): GELU(approximate='none')
                )
              )
              (activation): GELU(approximate='none')
              (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (positional_encoding): PositionalEncoding()
            )
            (convolution_module): ConvolutionModule(
              (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (bottleneck): Sequential(
                (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
                (1): GLU(dim=1)
              )
              (conv): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
              (after_conv): Sequential(
                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (1): Swish(
                  (sigmoid): Sigmoid()
                )
                (2): Linear(in_features=512, out_features=512, bias=True)
                (3): Dropout(p=0.1, inplace=False)
              )
            )
            (ffn_module1): Sequential(
              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (1): PositionalwiseFeedForward(
                (ffn): Sequential(
                  (0): Linear(in_features=512, out_features=2048, bias=True)
                  (1): Swish(
                    (sigmoid): Sigmoid()
                  )
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (2): Dropout(p=0.1, inplace=False)
            )
            (ffn_module2): Sequential(
              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (1): PositionalwiseFeedForward(
                (ffn): Sequential(
                  (0): Linear(in_features=512, out_features=2048, bias=True)
                  (1): Swish(
                    (sigmoid): Sigmoid()
                  )
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (2): Dropout(p=0.1, inplace=False)
            )
            (norm1): LayerNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
            (norm2): LayerNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
            (drop): Dropout(p=0.1, inplace=False)
          )
        )
        (norm): LayerNorm(
          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        )
      )
      (custom_src_module): ModuleList(
        (layers): ModuleList(
          (0): Linear(
            (w): Linear(in_features=640, out_features=512, bias=True)
          )
          (1): Dropout(p=0.1, inplace=False)
        )
      )
      (custom_tgt_module): ModuleList(
        (layers): ModuleList(
          (0): NormalizedEmbedding(
            (emb): Embedding(
              (Embedding): Embedding(5000, 512)
            )
          )
        )
      )
    )
  )
  (2): RandomProjectionQuantizer()
  (3): Linear(
    (w): Linear(in_features=512, out_features=8192, bias=True)
  )
) loading from results/LibriSpeech/brq_hyperconf/2000/save/weighted_ssl_model.ckpt, the transferred parameters did not have parameters for the key: 1.weights
2023-12-02 09:07:13,222 - speechbrain.utils.checkpoints - INFO - Would load a checkpoint here, but none found yet.
2023-12-02 09:07:13,222 - speechbrain.utils.epoch_loop - INFO - Going into epoch 1
2023-12-02 09:08:26,585 - speechbrain.core - ERROR - Exception:
Traceback (most recent call last):
  File "/gpfswork/rech/nkp/uaj64gk/bestrqexp/benchmark/benchmarks/MP3S/LibriSpeech/LSTM/train_brq.py", line 376, in <module>
    asr_brain.fit(
  File "/gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain/speechbrain/core.py", line 1366, in fit
    self._fit_train(train_set=train_set, epoch=epoch, enable=enable)
  File "/gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain/speechbrain/core.py", line 1193, in _fit_train
    loss = self.fit_batch(batch)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfswork/rech/nkp/uaj64gk/bestrqexp/benchmark/benchmarks/MP3S/LibriSpeech/LSTM/train_brq.py", line 90, in fit_batch
    predictions = self.compute_forward(batch, sb.Stage.TRAIN)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfswork/rech/nkp/uaj64gk/bestrqexp/benchmark/benchmarks/MP3S/LibriSpeech/LSTM/train_brq.py", line 36, in compute_forward
    feats = self.modules.weighted_ssl_model[1](feats, wav_lens)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/linkhome/rech/genzjn01/uaj64gk/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain/speechbrain/lobes/models/transformer/TransformerASR.py", line 438, in forward
    _, hidden_states = self.transformer.encode(x, wav_lens, pad_idx)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain/speechbrain/lobes/models/transformer/TransformerASR.py", line 337, in encode
    encoder_out, _, hidden_state_lst = self.encoder(
                                       ^^^^^^^^^^^^^
  File "/linkhome/rech/genzjn01/uaj64gk/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain/speechbrain/lobes/models/transformer/Conformer.py", line 415, in forward
    output, attention = enc_layer(
                        ^^^^^^^^^^
  File "/linkhome/rech/genzjn01/uaj64gk/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/speechbrain/speechbrain/lobes/models/transformer/Conformer.py", line 269, in forward
    x = self.norm2(x + 0.5 * self.ffn_module2(x))
                             ^^^^^^^^^^^^^^^^^^^
  File "/linkhome/rech/genzjn01/uaj64gk/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/linkhome/rech/genzjn01/uaj64gk/.local/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/linkhome/rech/genzjn01/uaj64gk/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/linkhome/rech/genzjn01/uaj64gk/.local/lib/python3.11/site-packages/torch/nn/modules/normalization.py", line 190, in forward
    return F.layer_norm(
           ^^^^^^^^^^^^^
  File "/linkhome/rech/genzjn01/uaj64gk/.local/lib/python3.11/site-packages/torch/nn/functional.py", line 2515, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
2023-12-02 09:29:48,597 - speechbrain.utils.train_logger - INFO - epoch: 1, lr_model: 2.00e-04 - train loss: 6.38e-01 - valid loss: 4.09e-01, valid CER: 9.95, valid WER: 33.12
2023-12-02 09:29:50,160 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brq_hyperconf/2000/save/CKPT+2023-12-02+09-29-48+00
2023-12-02 09:29:50,184 - speechbrain.utils.epoch_loop - INFO - Going into epoch 2
2023-12-02 09:52:03,374 - speechbrain.utils.train_logger - INFO - epoch: 2, lr_model: 2.00e-04 - train loss: 3.49e-01 - valid loss: 3.42e-01, valid CER: 8.38, valid WER: 28.26
2023-12-02 10:05:40,018 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brq_hyperconf/2000/save/CKPT+2023-12-02+09-52-03+00
2023-12-02 10:05:40,190 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brq_hyperconf/2000/save/CKPT+2023-12-02+09-29-48+00
2023-12-02 10:05:40,191 - speechbrain.utils.epoch_loop - INFO - Going into epoch 3
2023-12-02 10:27:33,423 - speechbrain.utils.train_logger - INFO - epoch: 3, lr_model: 2.00e-04 - train loss: 2.74e-01 - valid loss: 3.19e-01, valid CER: 7.64, valid WER: 25.80
2023-12-02 10:27:43,631 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brq_hyperconf/2000/save/CKPT+2023-12-02+10-27-33+00
2023-12-02 10:27:43,674 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brq_hyperconf/2000/save/CKPT+2023-12-02+09-52-03+00
2023-12-02 10:27:43,674 - speechbrain.utils.epoch_loop - INFO - Going into epoch 4
2023-12-02 10:49:37,067 - speechbrain.utils.train_logger - INFO - epoch: 4, lr_model: 2.00e-04 - train loss: 2.27e-01 - valid loss: 3.09e-01, valid CER: 7.22, valid WER: 24.58
2023-12-02 10:49:38,344 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brq_hyperconf/2000/save/CKPT+2023-12-02+10-49-37+00
2023-12-02 10:49:38,359 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brq_hyperconf/2000/save/CKPT+2023-12-02+10-27-33+00
2023-12-02 10:49:38,360 - speechbrain.utils.epoch_loop - INFO - Going into epoch 5
2023-12-02 11:11:29,149 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.0002 to 0.00016
2023-12-02 11:11:29,171 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.01 to 0.009
2023-12-02 11:11:29,171 - speechbrain.utils.train_logger - INFO - epoch: 5, lr_model: 2.00e-04 - train loss: 1.96e-01 - valid loss: 3.10e-01, valid CER: 6.93, valid WER: 23.40
2023-12-02 11:11:30,409 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brq_hyperconf/2000/save/CKPT+2023-12-02+11-11-29+00
2023-12-02 11:11:30,465 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brq_hyperconf/2000/save/CKPT+2023-12-02+10-49-37+00
2023-12-02 11:11:30,466 - speechbrain.utils.epoch_loop - INFO - Going into epoch 6
2023-12-02 11:33:22,480 - speechbrain.utils.train_logger - INFO - epoch: 6, lr_model: 1.60e-04 - train loss: 1.57e-01 - valid loss: 3.07e-01, valid CER: 6.66, valid WER: 22.49
2023-12-02 11:33:23,575 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brq_hyperconf/2000/save/CKPT+2023-12-02+11-33-22+00
2023-12-02 11:33:23,613 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brq_hyperconf/2000/save/CKPT+2023-12-02+11-11-29+00
2023-12-02 11:33:23,613 - speechbrain.utils.epoch_loop - INFO - Going into epoch 7
2023-12-02 11:55:13,096 - speechbrain.utils.train_logger - INFO - epoch: 7, lr_model: 1.60e-04 - train loss: 1.37e-01 - valid loss: 3.05e-01, valid CER: 6.56, valid WER: 21.99
2023-12-02 11:55:14,126 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brq_hyperconf/2000/save/CKPT+2023-12-02+11-55-13+00
2023-12-02 11:55:14,173 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brq_hyperconf/2000/save/CKPT+2023-12-02+11-33-22+00
2023-12-02 11:55:14,174 - speechbrain.utils.epoch_loop - INFO - Going into epoch 8
2023-12-02 12:17:02,745 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.00016 to 0.00013
2023-12-02 12:17:02,758 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.009 to 0.0081
2023-12-02 12:17:02,758 - speechbrain.utils.train_logger - INFO - epoch: 8, lr_model: 1.60e-04 - train loss: 1.23e-01 - valid loss: 3.10e-01, valid CER: 6.55, valid WER: 21.87
2023-12-02 12:17:03,699 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brq_hyperconf/2000/save/CKPT+2023-12-02+12-17-02+00
2023-12-02 12:17:03,723 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brq_hyperconf/2000/save/CKPT+2023-12-02+11-55-13+00
2023-12-02 12:17:03,724 - speechbrain.utils.epoch_loop - INFO - Going into epoch 9
2023-12-02 12:38:50,219 - speechbrain.utils.train_logger - INFO - epoch: 9, lr_model: 1.28e-04 - train loss: 1.01e-01 - valid loss: 3.07e-01, valid CER: 6.44, valid WER: 21.56
2023-12-02 12:38:51,216 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brq_hyperconf/2000/save/CKPT+2023-12-02+12-38-50+00
2023-12-02 12:38:51,243 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brq_hyperconf/2000/save/CKPT+2023-12-02+12-17-02+00
2023-12-02 12:38:51,243 - speechbrain.utils.epoch_loop - INFO - Going into epoch 10
2023-12-02 13:00:40,058 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.00013 to 0.0001
2023-12-02 13:00:40,078 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.0081 to 0.0073
2023-12-02 13:00:40,078 - speechbrain.utils.train_logger - INFO - epoch: 10, lr_model: 1.28e-04 - train loss: 8.96e-02 - valid loss: 3.18e-01, valid CER: 6.39, valid WER: 21.45
2023-12-02 13:00:41,150 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brq_hyperconf/2000/save/CKPT+2023-12-02+13-00-40+00
2023-12-02 13:00:41,199 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brq_hyperconf/2000/save/CKPT+2023-12-02+12-38-50+00
2023-12-02 13:00:41,199 - speechbrain.utils.epoch_loop - INFO - Going into epoch 11
2023-12-02 13:22:32,182 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.0001 to 8.2e-05
2023-12-02 13:22:32,195 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.0073 to 0.0066
2023-12-02 13:22:32,195 - speechbrain.utils.train_logger - INFO - epoch: 11, lr_model: 1.02e-04 - train loss: 7.41e-02 - valid loss: 3.24e-01, valid CER: 6.22, valid WER: 20.71
2023-12-02 13:22:33,228 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brq_hyperconf/2000/save/CKPT+2023-12-02+13-22-32+00
2023-12-02 13:22:33,273 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brq_hyperconf/2000/save/CKPT+2023-12-02+13-00-40+00
2023-12-02 13:22:33,273 - speechbrain.utils.epoch_loop - INFO - Going into epoch 12
2023-12-02 13:44:23,418 - speechbrain.nnet.schedulers - INFO - Changing lr from 8.2e-05 to 6.6e-05
2023-12-02 13:44:23,437 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.0066 to 0.0059
2023-12-02 13:44:23,438 - speechbrain.utils.train_logger - INFO - epoch: 12, lr_model: 8.19e-05 - train loss: 6.12e-02 - valid loss: 3.30e-01, valid CER: 6.12, valid WER: 20.39
2023-12-02 13:44:24,546 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brq_hyperconf/2000/save/CKPT+2023-12-02+13-44-23+00
2023-12-02 13:44:24,585 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brq_hyperconf/2000/save/CKPT+2023-12-02+13-22-32+00
2023-12-02 13:44:24,585 - speechbrain.utils.epoch_loop - INFO - Going into epoch 13
2023-12-02 14:06:11,832 - speechbrain.nnet.schedulers - INFO - Changing lr from 6.6e-05 to 5.2e-05
2023-12-02 14:06:11,856 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.0059 to 0.0053
2023-12-02 14:06:11,856 - speechbrain.utils.train_logger - INFO - epoch: 13, lr_model: 6.55e-05 - train loss: 5.04e-02 - valid loss: 3.33e-01, valid CER: 6.05, valid WER: 20.19
2023-12-02 14:06:12,955 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brq_hyperconf/2000/save/CKPT+2023-12-02+14-06-11+00
2023-12-02 14:06:13,009 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brq_hyperconf/2000/save/CKPT+2023-12-02+13-44-23+00
2023-12-02 14:06:13,009 - speechbrain.utils.epoch_loop - INFO - Going into epoch 14
2023-12-02 14:28:00,843 - speechbrain.nnet.schedulers - INFO - Changing lr from 5.2e-05 to 4.2e-05
2023-12-02 14:28:00,857 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.0053 to 0.0048
2023-12-02 14:28:00,858 - speechbrain.utils.train_logger - INFO - epoch: 14, lr_model: 5.24e-05 - train loss: 4.23e-02 - valid loss: 3.37e-01, valid CER: 5.97, valid WER: 19.80
2023-12-02 14:28:01,892 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brq_hyperconf/2000/save/CKPT+2023-12-02+14-28-00+00
2023-12-02 14:28:01,947 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brq_hyperconf/2000/save/CKPT+2023-12-02+14-06-11+00
2023-12-02 14:28:01,948 - speechbrain.utils.epoch_loop - INFO - Going into epoch 15
2023-12-02 14:49:51,692 - speechbrain.nnet.schedulers - INFO - Changing lr from 4.2e-05 to 3.4e-05
2023-12-02 14:49:51,708 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.0048 to 0.0043
2023-12-02 14:49:51,709 - speechbrain.utils.train_logger - INFO - epoch: 15, lr_model: 4.19e-05 - train loss: 3.65e-02 - valid loss: 3.48e-01, valid CER: 5.92, valid WER: 19.72
2023-12-02 14:49:52,819 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brq_hyperconf/2000/save/CKPT+2023-12-02+14-49-51+00
2023-12-02 14:49:52,880 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brq_hyperconf/2000/save/CKPT+2023-12-02+14-28-00+00
2023-12-02 14:49:52,881 - speechbrain.utils.epoch_loop - INFO - Going into epoch 16
2023-12-02 15:11:39,628 - speechbrain.nnet.schedulers - INFO - Changing lr from 3.4e-05 to 2.7e-05
2023-12-02 15:11:39,652 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.0043 to 0.0039
2023-12-02 15:11:39,652 - speechbrain.utils.train_logger - INFO - epoch: 16, lr_model: 3.36e-05 - train loss: 3.12e-02 - valid loss: 3.52e-01, valid CER: 5.87, valid WER: 19.46
2023-12-02 15:11:40,643 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brq_hyperconf/2000/save/CKPT+2023-12-02+15-11-39+00
2023-12-02 15:11:40,686 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brq_hyperconf/2000/save/CKPT+2023-12-02+14-49-51+00
2023-12-02 15:11:40,687 - speechbrain.utils.epoch_loop - INFO - Going into epoch 17
2023-12-02 15:33:28,392 - speechbrain.nnet.schedulers - INFO - Changing lr from 2.7e-05 to 2.1e-05
2023-12-02 15:33:28,403 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.0039 to 0.0035
2023-12-02 15:33:28,404 - speechbrain.utils.train_logger - INFO - epoch: 17, lr_model: 2.68e-05 - train loss: 2.81e-02 - valid loss: 3.54e-01, valid CER: 5.85, valid WER: 19.50
2023-12-02 15:33:29,480 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brq_hyperconf/2000/save/CKPT+2023-12-02+15-33-28+00
2023-12-02 15:33:29,524 - speechbrain.utils.epoch_loop - INFO - Going into epoch 18
2023-12-02 15:55:15,679 - speechbrain.nnet.schedulers - INFO - Changing lr from 2.1e-05 to 1.7e-05
2023-12-02 15:55:15,697 - speechbrain.nnet.schedulers - INFO - Changing lr from 0.0035 to 0.0031
2023-12-02 15:55:15,707 - speechbrain.utils.train_logger - INFO - epoch: 18, lr_model: 2.15e-05 - train loss: 2.51e-02 - valid loss: 3.60e-01, valid CER: 5.80, valid WER: 19.30
2023-12-02 15:55:16,860 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/LibriSpeech/brq_hyperconf/2000/save/CKPT+2023-12-02+15-55-15+00
2023-12-02 15:55:16,932 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brq_hyperconf/2000/save/CKPT+2023-12-02+15-33-28+00
2023-12-02 15:55:16,934 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/LibriSpeech/brq_hyperconf/2000/save/CKPT+2023-12-02+15-11-39+00
2023-12-02 15:55:16,935 - speechbrain.utils.epoch_loop - INFO - Going into epoch 19
