# modified 2024-08-22
# Generated 2023-12-05 from:
# /gpfsdswork/projects/rech/nkp/uaj64gk/bestrqexp/bestrq/hparams/best_rq_base.yaml
# yamllint disable
# ############################################################################
# Model: SSL: Implementation of Best-RQ
# Encoder: Conformer Encoder w/Random Projection Quantizer
# Training: Librispeech 960h
# Authors:  Ryan, Pan LIU
# ############################################################################
# Seed needs to be set at top of yaml, before objects with parameters are made

seed: 1000
__set_seed: !apply:torch.manual_seed [1000]

project_name: bestrq
experiment_name: base
#data_folder: /gpfsscratch/rech/nkp/uaj64gk/corpus/LibriSpeech/
data_folder: /kaggle/input/librispeech/LibriSpeech
output_folder: results/bestrq/1000

ckpt_interval_min: 15
save_folder: results/bestrq/1000/save
# Logging file for every N optimizer steps (many lines)
train_steps_log: results/bestrq/1000/train_steps_log_F.txt
# Logging file per epoch
train_stage_log: results/bestrq/1000/train_stage_log_F.txt

#train_splits: [train-clean-100, train-clean-360, train-other-500]
train_splits: [train-clean-100]
dev_splits: [dev-clean]
test_splits: [test-clean]
# train_csv: results/bestrq/csv/train.csv
# valid_csv: results/bestrq/csv/dev-clean.csv
# skip_prep: true

#train_csv: /kaggle/input/librispeech-prepared/librispeech_prepared/train-clean-100.csv
train_csv: /kaggle/working/train-clean-F.csv
valid_csv: /kaggle/input/librispeech-prepared/librispeech_prepared/dev-clean.csv
skip_prep: True

avoid_if_longer_than: 60.0
avoid_if_shorter_than: 2.0
log_interval: 500 # Logging every N optimizer steps
auto_mix_prec: false
bfloat16_mix_prec: false
max_grad_norm: 1.


# The training will either stops at number_of_epochs or optimizer_step_limit
# I.e. the first that is reached.
# number_of_epochs: 42
number_of_epochs: 10
optimizer_step_limit: 400000
# grad_accumulation_factor: 2

seconds_per_batch: 100 # Fits in a 11GB GPUs
train_num_buckets: 50

train_dataloader_options:
  num_workers: 4

test_dataloader_options:
  batch_size: 8   # DynamicBatching not used at testing time
  num_workers: 4

# Training parameters (based on Section 4.1.1)
lr: 0.0008
warmup: 25000

# Feature parameters
sample_rate: 16000
n_fft: 400
n_mels: 80
hop_length: 10
pad_to_divisible_by: 4
# this is because the convolutions reduction dim by 4 
# so the input to quantizer also needs to reduce dim by 4

####################### Model parameters ###########################
# Transformer
d_model: 512
nhead: 8 # table 1 https://arxiv.org/pdf/2010.10504.pdf
num_encoder_layers: 12 # section 4.1.1
num_decoder_layers: 0
d_ffn: 2048
transformer_dropout: 0.1
activation: &id001 !name:torch.nn.GELU
output_neurons: 5000
encoder_layerdrop: 0.05

# 15% masking
mask_length: 4 # for 400ms
mask_prob: 0.15
noise_mean: 0
noise_std: 0.1

# quantizer parameter
p_input: 320
cb_dim: 16
cb_vocab: 8192


############################## models ################################

CNN: &id003 !new:speechbrain.lobes.models.convolution.ConvolutionFrontEnd
  input_shape: (8, 10, 80)
  num_blocks: 2
  num_layers_per_block: 1
  out_channels: (128, 32)
  kernel_sizes: (3, 3)
  strides: (2, 2)
  residuals: (False, False)

Transformer: &id002 !new:speechbrain.lobes.models.transformer.TransformerASR.TransformerASR


  input_size: 640
  tgt_vocab: 5000
  d_model: 512
  nhead: 8
  num_encoder_layers: 12
  num_decoder_layers: 0
  d_ffn: 2048
  dropout: 0.1
  activation: *id001
  encoder_module: conformer
  attention_type: RelPosMHAXL
  normalize_before: true
  causal: false
  layerdrop_prob: 0.05

# We must call an encoder wrapper so the decoder isn't run (we don't have any)
wrapper: &id004 !new:speechbrain.lobes.models.transformer.TransformerASR.EncoderWrapper
  transformer: *id002
Quantizer: &id005 !new:.quantiser.RandomProjectionQuantizer
    # projection
  input_dim: 320
    # codebook
  cb_dim: 16
  cb_vocab: 8192

linear: &id006 !new:speechbrain.nnet.linear.Linear

  input_size: 512
  n_neurons: 8192

modules:
  CNN: *id003
  wrapper: *id004
  Quantizer: *id005
  normalize: &id007 !new:speechbrain.processing.features.InputNormalization
    norm_type: global
    update_until_epoch: 4


############################## running ################################

  linear: *id006
model: &id008 !new:torch.nn.ModuleList
- [*id003, *id004, *id005, *id006]
optimizer: !name:torch.optim.Adam
  lr: 0.0008
  betas: (0.9, 0.98)
  eps: 0.000000001

compute_features: !new:speechbrain.lobes.features.Fbank
  sample_rate: 16000
  n_fft: 400
  n_mels: 80

normalize: *id007
epoch_counter: &id010 !new:speechbrain.utils.epoch_loop.EpochCounter
  limit: 42


train_steps_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
  save_file: results/bestrq/1000/train_steps_log.txt

train_stage_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
  save_file: results/bestrq/1000/train_stage_log.txt

noam_annealing: &id009 !new:speechbrain.nnet.schedulers.NoamScheduler
  lr_initial: 0.0008
  n_warmup_steps: 25000

checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
  checkpoints_dir: results/bestrq/1000/save
  recoverables:
    model: *id008
    noam_scheduler: *id009
    normalizer: *id007
    counter: *id010